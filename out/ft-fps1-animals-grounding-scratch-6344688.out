============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[2024-05-23 04:21:07,713] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:16,268] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-05-23 04:21:16,269] [INFO] [runner.py:555:main] cmd = /home/scur0405/.conda/envs/llamavid/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llamavid/train/train_mem.py --deepspeed ./scripts/zero2_offload.json --model_name_or_path ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 --version imgsp_v1 --data_path ./data/LLaMA-VID-Finetune/animal-grounding/train_grounding_animals.json --image_folder ./data/LLaMA-VID-Finetune --video_folder ./data/LLaMA-VID-Finetune --vision_tower ./model_zoo/LAVIS/eva_vit_g.pth --image_processor ./llamavid/processor/clip-patch14-224 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length False --video_fps 1 --video_token 2 --bert_type qformer_pretrain_freeze_all --num_query 32 --compress_type mean --bf16 True --output_dir ./work_dirs/finetuning-grounding-animals-scratch-fps-1-ckpt --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 200 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 65536 --gradient_checkpointing True --dataloader_num_workers 1 --lazy_preprocess True --report_to wandb
[2024-05-23 04:21:19,741] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:24,775] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-23 04:21:24,776] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-23 04:21:24,776] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-23 04:21:24,776] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-23 04:21:24,776] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-23 04:21:32,287] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:32,399] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:32,411] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:32,443] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
no smdistributed.modelparallel.torchno smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch

[2024-05-23 04:21:36,137] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:36,137] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:36,137] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:36,137] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:36,138] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:36,139] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:36,139] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:36,139] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:36,139] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.81s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_key_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.86s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_query', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_val_projector.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vlm_att_ln.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.05s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vlm_att_query', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vlm_att_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_val_projector.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_key_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Loading pretrained weights...
Loading pretrained weights...
Loading pretrained weights...
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Formatting inputs...Skip in lazy mode
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.281270980834961 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.3007490634918213 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.311661720275879 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.3306233882904053 seconds
Rank: 0 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 2 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 3 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 1 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
wandb: Currently logged in as: antonios-tragoudaras (tonytragoudaras). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /gpfs/home4/scur0405/LLaMA-VID/wandb/run-20240523_042607-5s5xihry
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-thunder-26
wandb: ⭐️ View project at https://wandb.ai/tonytragoudaras/huggingface
wandb: 🚀 View run at https://wandb.ai/tonytragoudaras/huggingface/runs/5s5xihry
  0%|          | 0/436 [00:00<?, ?it/s]  0%|          | 1/436 [00:28<3:29:02, 28.83s/it]                                                 {'loss': 3.9453, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.0}
  0%|          | 1/436 [00:28<3:29:02, 28.83s/it]  0%|          | 2/436 [00:36<1:58:51, 16.43s/it]                                                 {'loss': 3.4883, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.0}
  0%|          | 2/436 [00:36<1:58:51, 16.43s/it]  1%|          | 3/436 [00:44<1:29:13, 12.36s/it]                                                 {'loss': 3.457, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.01}
  1%|          | 3/436 [00:44<1:29:13, 12.36s/it]  1%|          | 4/436 [00:51<1:16:18, 10.60s/it]                                                 {'loss': 3.1484, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.01}
  1%|          | 4/436 [00:52<1:16:18, 10.60s/it]  1%|          | 5/436 [00:59<1:08:15,  9.50s/it]                                                 {'loss': 3.2891, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.01}
  1%|          | 5/436 [00:59<1:08:15,  9.50s/it]  1%|▏         | 6/436 [01:07<1:03:37,  8.88s/it]                                                 {'loss': 2.7725, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.01}
  1%|▏         | 6/436 [01:07<1:03:37,  8.88s/it]  2%|▏         | 7/436 [01:14<1:00:53,  8.52s/it]                                                 {'loss': 2.4014, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|▏         | 7/436 [01:14<1:00:53,  8.52s/it]  2%|▏         | 8/436 [01:22<59:30,  8.34s/it]                                                 {'loss': 1.8877, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.02}
  2%|▏         | 8/436 [01:22<59:30,  8.34s/it]  2%|▏         | 9/436 [01:30<58:13,  8.18s/it]                                               {'loss': 1.6768, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.02}
  2%|▏         | 9/436 [01:30<58:13,  8.18s/it]  2%|▏         | 10/436 [01:38<57:17,  8.07s/it]                                                {'loss': 1.7905, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.02}
  2%|▏         | 10/436 [01:38<57:17,  8.07s/it]  3%|▎         | 11/436 [01:46<56:04,  7.92s/it]                                                {'loss': 1.668, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.03}
  3%|▎         | 11/436 [01:46<56:04,  7.92s/it]  3%|▎         | 12/436 [01:54<57:00,  8.07s/it]                                                {'loss': 1.3467, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.03}
  3%|▎         | 12/436 [01:54<57:00,  8.07s/it]  3%|▎         | 13/436 [02:02<55:46,  7.91s/it]                                                {'loss': 1.3237, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.03}
  3%|▎         | 13/436 [02:02<55:46,  7.91s/it]  3%|▎         | 14/436 [02:09<55:09,  7.84s/it]                                                {'loss': 1.0884, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|▎         | 14/436 [02:09<55:09,  7.84s/it]  3%|▎         | 15/436 [02:17<53:59,  7.69s/it]                                                {'loss': 1.3589, 'learning_rate': 1.9999722895969904e-05, 'epoch': 0.03}
  3%|▎         | 15/436 [02:17<53:59,  7.69s/it]  4%|▎         | 16/436 [02:25<54:29,  7.79s/it]                                                {'loss': 1.1558, 'learning_rate': 1.999889159923694e-05, 'epoch': 0.04}
  4%|▎         | 16/436 [02:25<54:29,  7.79s/it]  4%|▍         | 17/436 [02:33<54:30,  7.81s/it]                                                {'loss': 1.457, 'learning_rate': 1.9997506155872246e-05, 'epoch': 0.04}
  4%|▍         | 17/436 [02:33<54:30,  7.81s/it]  4%|▍         | 18/436 [02:40<54:07,  7.77s/it]                                                {'loss': 1.2319, 'learning_rate': 1.9995566642658208e-05, 'epoch': 0.04}
  4%|▍         | 18/436 [02:40<54:07,  7.77s/it]  4%|▍         | 19/436 [02:48<54:22,  7.82s/it]                                                {'loss': 1.1487, 'learning_rate': 1.999307316708421e-05, 'epoch': 0.04}
  4%|▍         | 19/436 [02:48<54:22,  7.82s/it]  5%|▍         | 20/436 [02:56<54:01,  7.79s/it]                                                {'loss': 1.1213, 'learning_rate': 1.9990025867340683e-05, 'epoch': 0.05}
  5%|▍         | 20/436 [02:56<54:01,  7.79s/it]  5%|▍         | 21/436 [03:03<53:20,  7.71s/it]                                                {'loss': 1.2703, 'learning_rate': 1.998642491231143e-05, 'epoch': 0.05}
  5%|▍         | 21/436 [03:03<53:20,  7.71s/it]  5%|▌         | 22/436 [03:11<52:39,  7.63s/it]                                                {'loss': 1.0752, 'learning_rate': 1.9982270501564286e-05, 'epoch': 0.05}
  5%|▌         | 22/436 [03:11<52:39,  7.63s/it]  5%|▌         | 23/436 [03:19<53:04,  7.71s/it]                                                {'loss': 0.9883, 'learning_rate': 1.997756286534004e-05, 'epoch': 0.05}
  5%|▌         | 23/436 [03:19<53:04,  7.71s/it]  6%|▌         | 24/436 [03:27<53:05,  7.73s/it]                                                {'loss': 0.9465, 'learning_rate': 1.9972302264539686e-05, 'epoch': 0.05}
  6%|▌         | 24/436 [03:27<53:05,  7.73s/it]  6%|▌         | 25/436 [03:34<52:05,  7.60s/it]                                                {'loss': 1.1917, 'learning_rate': 1.996648899070996e-05, 'epoch': 0.06}
  6%|▌         | 25/436 [03:34<52:05,  7.60s/it]  6%|▌         | 26/436 [03:41<51:44,  7.57s/it]                                                {'loss': 1.001, 'learning_rate': 1.9960123366027187e-05, 'epoch': 0.06}
  6%|▌         | 26/436 [03:41<51:44,  7.57s/it]  6%|▌         | 27/436 [03:50<53:06,  7.79s/it]                                                {'loss': 1.1099, 'learning_rate': 1.995320574327941e-05, 'epoch': 0.06}
  6%|▌         | 27/436 [03:50<53:06,  7.79s/it]  6%|▋         | 28/436 [03:57<52:40,  7.75s/it]                                                {'loss': 1.3491, 'learning_rate': 1.9945736505846866e-05, 'epoch': 0.06}
  6%|▋         | 28/436 [03:57<52:40,  7.75s/it]  7%|▋         | 29/436 [04:05<52:15,  7.70s/it]                                                {'loss': 1.3672, 'learning_rate': 1.9937716067680712e-05, 'epoch': 0.07}
  7%|▋         | 29/436 [04:05<52:15,  7.70s/it]  7%|▋         | 30/436 [04:12<51:48,  7.66s/it]                                                {'loss': 1.1367, 'learning_rate': 1.9929144873280092e-05, 'epoch': 0.07}
  7%|▋         | 30/436 [04:12<51:48,  7.66s/it]  7%|▋         | 31/436 [04:20<51:49,  7.68s/it]                                                {'loss': 1.1753, 'learning_rate': 1.992002339766751e-05, 'epoch': 0.07}
  7%|▋         | 31/436 [04:20<51:49,  7.68s/it]  7%|▋         | 32/436 [04:28<51:33,  7.66s/it]                                                {'loss': 0.8489, 'learning_rate': 1.99103521463625e-05, 'epoch': 0.07}
  7%|▋         | 32/436 [04:28<51:33,  7.66s/it]  8%|▊         | 33/436 [04:35<51:08,  7.61s/it]                                                {'loss': 1.1023, 'learning_rate': 1.9900131655353597e-05, 'epoch': 0.08}
  8%|▊         | 33/436 [04:35<51:08,  7.61s/it]  8%|▊         | 34/436 [04:43<50:42,  7.57s/it]                                                {'loss': 1.0503, 'learning_rate': 1.9889362491068658e-05, 'epoch': 0.08}
  8%|▊         | 34/436 [04:43<50:42,  7.57s/it]  8%|▊         | 35/436 [04:50<50:29,  7.55s/it]                                                {'loss': 1.0518, 'learning_rate': 1.9878045250343445e-05, 'epoch': 0.08}
  8%|▊         | 35/436 [04:50<50:29,  7.55s/it]  8%|▊         | 36/436 [04:58<50:09,  7.52s/it]                                                {'loss': 0.9487, 'learning_rate': 1.986618056038856e-05, 'epoch': 0.08}
  8%|▊         | 36/436 [04:58<50:09,  7.52s/it]  8%|▊         | 37/436 [05:05<49:50,  7.49s/it]                                                {'loss': 1.293, 'learning_rate': 1.9853769078754685e-05, 'epoch': 0.08}
  8%|▊         | 37/436 [05:05<49:50,  7.49s/it]  9%|▊         | 38/436 [05:12<49:21,  7.44s/it]                                                {'loss': 0.8121, 'learning_rate': 1.9840811493296134e-05, 'epoch': 0.09}
  9%|▊         | 38/436 [05:12<49:21,  7.44s/it]  9%|▉         | 39/436 [05:20<49:28,  7.48s/it]                                                {'loss': 1.0132, 'learning_rate': 1.982730852213274e-05, 'epoch': 0.09}
  9%|▉         | 39/436 [05:20<49:28,  7.48s/it]  9%|▉         | 40/436 [05:28<49:29,  7.50s/it]                                                {'loss': 0.967, 'learning_rate': 1.9813260913610048e-05, 'epoch': 0.09}
  9%|▉         | 40/436 [05:28<49:29,  7.50s/it]  9%|▉         | 41/436 [05:35<50:01,  7.60s/it]                                                {'loss': 0.8479, 'learning_rate': 1.9798669446257844e-05, 'epoch': 0.09}
  9%|▉         | 41/436 [05:35<50:01,  7.60s/it] 10%|▉         | 42/436 [05:43<49:27,  7.53s/it]                                                {'loss': 0.8326, 'learning_rate': 1.9783534928747006e-05, 'epoch': 0.1}
 10%|▉         | 42/436 [05:43<49:27,  7.53s/it] 10%|▉         | 43/436 [05:50<49:16,  7.52s/it]                                                {'loss': 0.9424, 'learning_rate': 1.9767858199844697e-05, 'epoch': 0.1}
 10%|▉         | 43/436 [05:50<49:16,  7.52s/it] 10%|█         | 44/436 [05:58<49:17,  7.54s/it]                                                {'loss': 1.1475, 'learning_rate': 1.9751640128367872e-05, 'epoch': 0.1}
 10%|█         | 44/436 [05:58<49:17,  7.54s/it] 10%|█         | 45/436 [06:05<49:00,  7.52s/it]                                                {'loss': 1.1477, 'learning_rate': 1.973488161313512e-05, 'epoch': 0.1}
 10%|█         | 45/436 [06:05<49:00,  7.52s/it] 11%|█         | 46/436 [06:13<48:34,  7.47s/it]                                                {'loss': 0.8099, 'learning_rate': 1.9717583582916862e-05, 'epoch': 0.11}
 11%|█         | 46/436 [06:13<48:34,  7.47s/it] 11%|█         | 47/436 [06:20<48:57,  7.55s/it]                                                {'loss': 1.1523, 'learning_rate': 1.969974699638388e-05, 'epoch': 0.11}
 11%|█         | 47/436 [06:20<48:57,  7.55s/it] 11%|█         | 48/436 [06:28<48:52,  7.56s/it]                                                {'loss': 1.116, 'learning_rate': 1.968137284205417e-05, 'epoch': 0.11}
 11%|█         | 48/436 [06:28<48:52,  7.56s/it] 11%|█         | 49/436 [06:36<48:45,  7.56s/it]                                                {'loss': 1.0186, 'learning_rate': 1.966246213823818e-05, 'epoch': 0.11}
 11%|█         | 49/436 [06:36<48:45,  7.56s/it] 11%|█▏        | 50/436 [06:43<48:56,  7.61s/it]                                                {'loss': 1.0273, 'learning_rate': 1.9643015932982355e-05, 'epoch': 0.11}
 11%|█▏        | 50/436 [06:43<48:56,  7.61s/it] 12%|█▏        | 51/436 [06:51<49:45,  7.75s/it]                                                {'loss': 1.0903, 'learning_rate': 1.9623035304011062e-05, 'epoch': 0.12}
 12%|█▏        | 51/436 [06:51<49:45,  7.75s/it] 12%|█▏        | 52/436 [06:59<48:35,  7.59s/it]                                                {'loss': 1.0854, 'learning_rate': 1.960252135866687e-05, 'epoch': 0.12}
 12%|█▏        | 52/436 [06:59<48:35,  7.59s/it] 12%|█▏        | 53/436 [07:06<48:04,  7.53s/it]                                                {'loss': 0.9155, 'learning_rate': 1.9581475233849165e-05, 'epoch': 0.12}
 12%|█▏        | 53/436 [07:06<48:04,  7.53s/it] 12%|█▏        | 54/436 [07:14<48:03,  7.55s/it]                                                {'loss': 1.1985, 'learning_rate': 1.9559898095951137e-05, 'epoch': 0.12}
 12%|█▏        | 54/436 [07:14<48:03,  7.55s/it] 13%|█▎        | 55/436 [07:21<48:17,  7.61s/it]                                                {'loss': 0.9463, 'learning_rate': 1.953779114079517e-05, 'epoch': 0.13}
 13%|█▎        | 55/436 [07:21<48:17,  7.61s/it] 13%|█▎        | 56/436 [07:29<48:06,  7.60s/it]                                                {'loss': 1.1787, 'learning_rate': 1.9515155593566536e-05, 'epoch': 0.13}
 13%|█▎        | 56/436 [07:29<48:06,  7.60s/it] 13%|█▎        | 57/436 [07:36<47:39,  7.54s/it]                                                {'loss': 0.9602, 'learning_rate': 1.9491992708745502e-05, 'epoch': 0.13}
 13%|█▎        | 57/436 [07:36<47:39,  7.54s/it] 13%|█▎        | 58/436 [07:44<47:51,  7.60s/it]                                                {'loss': 1.188, 'learning_rate': 1.946830377003782e-05, 'epoch': 0.13}
 13%|█▎        | 58/436 [07:44<47:51,  7.60s/it] 14%|█▎        | 59/436 [07:51<47:23,  7.54s/it]                                                {'loss': 0.7788, 'learning_rate': 1.9444090090303567e-05, 'epoch': 0.14}
 14%|█▎        | 59/436 [07:51<47:23,  7.54s/it] 14%|█▍        | 60/436 [07:59<46:58,  7.50s/it]                                                {'loss': 0.8018, 'learning_rate': 1.941935301148439e-05, 'epoch': 0.14}
 14%|█▍        | 60/436 [07:59<46:58,  7.50s/it] 14%|█▍        | 61/436 [08:06<47:05,  7.54s/it]                                                {'loss': 0.8735, 'learning_rate': 1.939409390452913e-05, 'epoch': 0.14}
 14%|█▍        | 61/436 [08:06<47:05,  7.54s/it] 14%|█▍        | 62/436 [08:14<47:21,  7.60s/it]                                                {'loss': 0.7827, 'learning_rate': 1.9368314169317858e-05, 'epoch': 0.14}
 14%|█▍        | 62/436 [08:14<47:21,  7.60s/it] 14%|█▍        | 63/436 [08:22<46:57,  7.55s/it]                                                {'loss': 1.0608, 'learning_rate': 1.9342015234584277e-05, 'epoch': 0.14}
 14%|█▍        | 63/436 [08:22<46:57,  7.55s/it] 15%|█▍        | 64/436 [08:29<46:44,  7.54s/it]                                                {'loss': 0.8638, 'learning_rate': 1.9315198557836555e-05, 'epoch': 0.15}
 15%|█▍        | 64/436 [08:29<46:44,  7.54s/it] 15%|█▍        | 65/436 [08:37<47:02,  7.61s/it]                                                {'loss': 0.9856, 'learning_rate': 1.928786562527652e-05, 'epoch': 0.15}
 15%|█▍        | 65/436 [08:37<47:02,  7.61s/it] 15%|█▌        | 66/436 [08:44<46:31,  7.55s/it]                                                {'loss': 0.7747, 'learning_rate': 1.9260017951717334e-05, 'epoch': 0.15}
 15%|█▌        | 66/436 [08:44<46:31,  7.55s/it] 15%|█▌        | 67/436 [08:52<46:06,  7.50s/it]                                                {'loss': 0.8301, 'learning_rate': 1.9231657080499507e-05, 'epoch': 0.15}
 15%|█▌        | 67/436 [08:52<46:06,  7.50s/it] 16%|█▌        | 68/436 [08:59<46:09,  7.52s/it]                                                {'loss': 1.0081, 'learning_rate': 1.9202784583405386e-05, 'epoch': 0.16}
 16%|█▌        | 68/436 [08:59<46:09,  7.52s/it] 16%|█▌        | 69/436 [09:07<45:47,  7.49s/it]                                                {'loss': 0.9338, 'learning_rate': 1.9173402060572028e-05, 'epoch': 0.16}
 16%|█▌        | 69/436 [09:07<45:47,  7.49s/it] 16%|█▌        | 70/436 [09:14<45:40,  7.49s/it]                                                {'loss': 0.7737, 'learning_rate': 1.9143511140402532e-05, 'epoch': 0.16}
 16%|█▌        | 70/436 [09:14<45:40,  7.49s/it] 16%|█▋        | 71/436 [09:22<45:35,  7.49s/it]                                                {'loss': 0.6974, 'learning_rate': 1.9113113479475784e-05, 'epoch': 0.16}
 16%|█▋        | 71/436 [09:22<45:35,  7.49s/it] 17%|█▋        | 72/436 [09:29<45:33,  7.51s/it]                                                {'loss': 1.2664, 'learning_rate': 1.908221076245466e-05, 'epoch': 0.16}
 17%|█▋        | 72/436 [09:29<45:33,  7.51s/it] 17%|█▋        | 73/436 [09:37<45:18,  7.49s/it]                                                {'loss': 1.0083, 'learning_rate': 1.905080470199264e-05, 'epoch': 0.17}
 17%|█▋        | 73/436 [09:37<45:18,  7.49s/it] 17%|█▋        | 74/436 [09:44<45:14,  7.50s/it]                                                {'loss': 0.7527, 'learning_rate': 1.901889703863891e-05, 'epoch': 0.17}
 17%|█▋        | 74/436 [09:44<45:14,  7.50s/it] 17%|█▋        | 75/436 [09:52<45:08,  7.50s/it]                                                {'loss': 0.7458, 'learning_rate': 1.8986489540741895e-05, 'epoch': 0.17}
 17%|█▋        | 75/436 [09:52<45:08,  7.50s/it] 17%|█▋        | 76/436 [09:59<45:25,  7.57s/it]                                                {'loss': 0.7393, 'learning_rate': 1.8953584004351243e-05, 'epoch': 0.17}
 17%|█▋        | 76/436 [09:59<45:25,  7.57s/it] 18%|█▊        | 77/436 [10:07<45:14,  7.56s/it]                                                {'loss': 0.9065, 'learning_rate': 1.892018225311831e-05, 'epoch': 0.18}
 18%|█▊        | 77/436 [10:07<45:14,  7.56s/it] 18%|█▊        | 78/436 [10:15<45:01,  7.55s/it]                                                {'loss': 0.6255, 'learning_rate': 1.8886286138195063e-05, 'epoch': 0.18}
 18%|█▊        | 78/436 [10:15<45:01,  7.55s/it] 18%|█▊        | 79/436 [10:22<45:02,  7.57s/it]                                                {'loss': 0.7756, 'learning_rate': 1.885189753813152e-05, 'epoch': 0.18}
 18%|█▊        | 79/436 [10:22<45:02,  7.57s/it] 18%|█▊        | 80/436 [10:30<44:50,  7.56s/it]                                                {'loss': 0.7556, 'learning_rate': 1.8817018358771612e-05, 'epoch': 0.18}
 18%|█▊        | 80/436 [10:30<44:50,  7.56s/it] 19%|█▊        | 81/436 [10:37<44:33,  7.53s/it]                                                {'loss': 0.865, 'learning_rate': 1.8781650533147572e-05, 'epoch': 0.19}
 19%|█▊        | 81/436 [10:37<44:33,  7.53s/it] 19%|█▉        | 82/436 [10:45<44:21,  7.52s/it]                                                {'loss': 0.8623, 'learning_rate': 1.87457960213728e-05, 'epoch': 0.19}
 19%|█▉        | 82/436 [10:45<44:21,  7.52s/it] 19%|█▉        | 83/436 [10:52<44:04,  7.49s/it]                                                {'loss': 0.4965, 'learning_rate': 1.8709456810533248e-05, 'epoch': 0.19}
 19%|█▉        | 83/436 [10:52<44:04,  7.49s/it] 19%|█▉        | 84/436 [11:00<44:00,  7.50s/it]                                                {'loss': 0.7986, 'learning_rate': 1.867263491457726e-05, 'epoch': 0.19}
 19%|█▉        | 84/436 [11:00<44:00,  7.50s/it] 19%|█▉        | 85/436 [11:07<43:40,  7.47s/it]                                                {'loss': 1.0515, 'learning_rate': 1.8635332374203993e-05, 'epoch': 0.19}
 19%|█▉        | 85/436 [11:07<43:40,  7.47s/it] 20%|█▉        | 86/436 [11:15<43:46,  7.50s/it]                                                {'loss': 0.7832, 'learning_rate': 1.85975512567503e-05, 'epoch': 0.2}
 20%|█▉        | 86/436 [11:15<43:46,  7.50s/it] 20%|█▉        | 87/436 [11:22<43:19,  7.45s/it]                                                {'loss': 1.0588, 'learning_rate': 1.8559293656076167e-05, 'epoch': 0.2}
 20%|█▉        | 87/436 [11:22<43:19,  7.45s/it] 20%|██        | 88/436 [11:29<43:02,  7.42s/it]                                                {'loss': 0.9885, 'learning_rate': 1.8520561692448655e-05, 'epoch': 0.2}
 20%|██        | 88/436 [11:29<43:02,  7.42s/it] 20%|██        | 89/436 [11:37<43:12,  7.47s/it]                                                {'loss': 1.0117, 'learning_rate': 1.848135751242441e-05, 'epoch': 0.2}
 20%|██        | 89/436 [11:37<43:12,  7.47s/it] 21%|██        | 90/436 [11:45<43:29,  7.54s/it]                                                {'loss': 0.6505, 'learning_rate': 1.8441683288730686e-05, 'epoch': 0.21}
 21%|██        | 90/436 [11:45<43:29,  7.54s/it] 21%|██        | 91/436 [11:52<43:30,  7.57s/it]                                                {'loss': 0.6404, 'learning_rate': 1.840154122014494e-05, 'epoch': 0.21}
 21%|██        | 91/436 [11:52<43:30,  7.57s/it] 21%|██        | 92/436 [12:00<43:11,  7.53s/it]                                                {'loss': 0.8291, 'learning_rate': 1.836093353137297e-05, 'epoch': 0.21}
 21%|██        | 92/436 [12:00<43:11,  7.53s/it] 21%|██▏       | 93/436 [12:07<43:14,  7.56s/it]                                                {'loss': 0.8208, 'learning_rate': 1.831986247292561e-05, 'epoch': 0.21}
 21%|██▏       | 93/436 [12:07<43:14,  7.56s/it] 22%|██▏       | 94/436 [12:15<43:03,  7.55s/it]                                                {'loss': 0.8215, 'learning_rate': 1.8278330320994035e-05, 'epoch': 0.22}
 22%|██▏       | 94/436 [12:15<43:03,  7.55s/it] 22%|██▏       | 95/436 [12:23<43:36,  7.67s/it]                                                {'loss': 0.9348, 'learning_rate': 1.823633937732357e-05, 'epoch': 0.22}
 22%|██▏       | 95/436 [12:23<43:36,  7.67s/it] 22%|██▏       | 96/436 [12:30<43:36,  7.70s/it]                                                {'loss': 0.8794, 'learning_rate': 1.8193891969086164e-05, 'epoch': 0.22}
 22%|██▏       | 96/436 [12:30<43:36,  7.70s/it] 22%|██▏       | 97/436 [12:38<43:19,  7.67s/it]                                                {'loss': 0.6892, 'learning_rate': 1.8150990448751393e-05, 'epoch': 0.22}
 22%|██▏       | 97/436 [12:38<43:19,  7.67s/it] 22%|██▏       | 98/436 [12:46<43:07,  7.66s/it]                                                {'loss': 1.0535, 'learning_rate': 1.8107637193956102e-05, 'epoch': 0.22}
 22%|██▏       | 98/436 [12:46<43:07,  7.66s/it] 23%|██▎       | 99/436 [12:54<43:16,  7.70s/it]                                                {'loss': 0.8267, 'learning_rate': 1.8063834607372603e-05, 'epoch': 0.23}
 23%|██▎       | 99/436 [12:54<43:16,  7.70s/it] 23%|██▎       | 100/436 [13:01<42:52,  7.66s/it]                                                 {'loss': 0.7881, 'learning_rate': 1.8019585116575554e-05, 'epoch': 0.23}
 23%|██▎       | 100/436 [13:01<42:52,  7.66s/it] 23%|██▎       | 101/436 [13:09<42:28,  7.61s/it]                                                 {'loss': 0.7476, 'learning_rate': 1.7974891173907406e-05, 'epoch': 0.23}
 23%|██▎       | 101/436 [13:09<42:28,  7.61s/it] 23%|██▎       | 102/436 [13:16<42:26,  7.63s/it]                                                 {'loss': 1.1741, 'learning_rate': 1.792975525634248e-05, 'epoch': 0.23}
 23%|██▎       | 102/436 [13:16<42:26,  7.63s/it] 24%|██▎       | 103/436 [13:24<43:13,  7.79s/it]                                                 {'loss': 0.7, 'learning_rate': 1.7884179865349713e-05, 'epoch': 0.24}
 24%|██▎       | 103/436 [13:24<43:13,  7.79s/it] 24%|██▍       | 104/436 [13:32<42:37,  7.70s/it]                                                 {'loss': 0.8325, 'learning_rate': 1.7838167526754002e-05, 'epoch': 0.24}
 24%|██▍       | 104/436 [13:32<42:37,  7.70s/it] 24%|██▍       | 105/436 [13:39<42:02,  7.62s/it]                                                 {'loss': 0.7202, 'learning_rate': 1.7791720790596242e-05, 'epoch': 0.24}
 24%|██▍       | 105/436 [13:39<42:02,  7.62s/it] 24%|██▍       | 106/436 [13:47<42:22,  7.70s/it]                                                 {'loss': 0.8684, 'learning_rate': 1.774484223099199e-05, 'epoch': 0.24}
 24%|██▍       | 106/436 [13:47<42:22,  7.70s/it] 25%|██▍       | 107/436 [13:55<42:52,  7.82s/it]                                                 {'loss': 1.0298, 'learning_rate': 1.7697534445988804e-05, 'epoch': 0.25}
 25%|██▍       | 107/436 [13:55<42:52,  7.82s/it] 25%|██▍       | 108/436 [14:03<42:17,  7.73s/it]                                                 {'loss': 0.8999, 'learning_rate': 1.7649800057422256e-05, 'epoch': 0.25}
 25%|██▍       | 108/436 [14:03<42:17,  7.73s/it] 25%|██▌       | 109/436 [14:10<41:49,  7.67s/it]                                                 {'loss': 1.0657, 'learning_rate': 1.760164171077064e-05, 'epoch': 0.25}
 25%|██▌       | 109/436 [14:10<41:49,  7.67s/it] 25%|██▌       | 110/436 [14:18<41:46,  7.69s/it]                                                 {'loss': 0.7119, 'learning_rate': 1.755306207500834e-05, 'epoch': 0.25}
 25%|██▌       | 110/436 [14:18<41:46,  7.69s/it] 25%|██▌       | 111/436 [14:26<41:16,  7.62s/it]                                                 {'loss': 0.7102, 'learning_rate': 1.750406384245793e-05, 'epoch': 0.25}
 25%|██▌       | 111/436 [14:26<41:16,  7.62s/it] 26%|██▌       | 112/436 [14:33<41:14,  7.64s/it]                                                 {'loss': 0.5547, 'learning_rate': 1.7454649728640944e-05, 'epoch': 0.26}
 26%|██▌       | 112/436 [14:33<41:14,  7.64s/it] 26%|██▌       | 113/436 [14:41<40:56,  7.61s/it]                                                 {'loss': 0.853, 'learning_rate': 1.7404822472127406e-05, 'epoch': 0.26}
 26%|██▌       | 113/436 [14:41<40:56,  7.61s/it] 26%|██▌       | 114/436 [14:49<41:08,  7.67s/it]                                                 {'loss': 0.9673, 'learning_rate': 1.7354584834384036e-05, 'epoch': 0.26}
 26%|██▌       | 114/436 [14:49<41:08,  7.67s/it] 26%|██▋       | 115/436 [14:56<41:06,  7.68s/it]                                                 {'loss': 0.6797, 'learning_rate': 1.73039395996212e-05, 'epoch': 0.26}
 26%|██▋       | 115/436 [14:56<41:06,  7.68s/it] 27%|██▋       | 116/436 [15:04<40:42,  7.63s/it]                                                 {'loss': 0.5876, 'learning_rate': 1.725288957463864e-05, 'epoch': 0.27}
 27%|██▋       | 116/436 [15:04<40:42,  7.63s/it] 27%|██▋       | 117/436 [15:11<40:05,  7.54s/it]                                                 {'loss': 1.0251, 'learning_rate': 1.720143758866988e-05, 'epoch': 0.27}
 27%|██▋       | 117/436 [15:11<40:05,  7.54s/it] 27%|██▋       | 118/436 [15:19<41:06,  7.76s/it]                                                 {'loss': 0.8198, 'learning_rate': 1.7149586493225453e-05, 'epoch': 0.27}
 27%|██▋       | 118/436 [15:19<41:06,  7.76s/it] 27%|██▋       | 119/436 [15:27<40:39,  7.70s/it]                                                 {'loss': 0.5586, 'learning_rate': 1.709733916193487e-05, 'epoch': 0.27}
 27%|██▋       | 119/436 [15:27<40:39,  7.70s/it] 28%|██▊       | 120/436 [15:34<39:56,  7.58s/it]                                                 {'loss': 0.8457, 'learning_rate': 1.704469849038734e-05, 'epoch': 0.27}
 28%|██▊       | 120/436 [15:34<39:56,  7.58s/it] 28%|██▊       | 121/436 [15:42<39:22,  7.50s/it]                                                 {'loss': 0.6941, 'learning_rate': 1.6991667395971306e-05, 'epoch': 0.28}
 28%|██▊       | 121/436 [15:42<39:22,  7.50s/it] 28%|██▊       | 122/436 [15:49<39:43,  7.59s/it]                                                 {'loss': 0.7258, 'learning_rate': 1.6938248817712767e-05, 'epoch': 0.28}
 28%|██▊       | 122/436 [15:49<39:43,  7.59s/it] 28%|██▊       | 123/436 [15:57<39:24,  7.55s/it]                                                 {'loss': 0.5628, 'learning_rate': 1.6884445716112388e-05, 'epoch': 0.28}
 28%|██▊       | 123/436 [15:57<39:24,  7.55s/it] 28%|██▊       | 124/436 [16:04<39:16,  7.55s/it]                                                 {'loss': 0.832, 'learning_rate': 1.6830261072981423e-05, 'epoch': 0.28}
 28%|██▊       | 124/436 [16:04<39:16,  7.55s/it] 29%|██▊       | 125/436 [16:12<38:51,  7.50s/it]                                                 {'loss': 0.7583, 'learning_rate': 1.677569789127647e-05, 'epoch': 0.29}
 29%|██▊       | 125/436 [16:12<38:51,  7.50s/it] 29%|██▉       | 126/436 [16:20<39:12,  7.59s/it]                                                 {'loss': 0.901, 'learning_rate': 1.6720759194933037e-05, 'epoch': 0.29}
 29%|██▉       | 126/436 [16:20<39:12,  7.59s/it] 29%|██▉       | 127/436 [16:27<39:02,  7.58s/it]                                                 {'loss': 0.6807, 'learning_rate': 1.666544802869796e-05, 'epoch': 0.29}
 29%|██▉       | 127/436 [16:27<39:02,  7.58s/it] 29%|██▉       | 128/436 [16:35<38:44,  7.55s/it]                                                 {'loss': 0.6826, 'learning_rate': 1.660976745796065e-05, 'epoch': 0.29}
 29%|██▉       | 128/436 [16:35<38:44,  7.55s/it] 30%|██▉       | 129/436 [16:42<38:22,  7.50s/it]                                                 {'loss': 0.6305, 'learning_rate': 1.655372056858322e-05, 'epoch': 0.3}
 30%|██▉       | 129/436 [16:42<38:22,  7.50s/it] 30%|██▉       | 130/436 [16:50<38:21,  7.52s/it]                                                 {'loss': 0.9718, 'learning_rate': 1.6497310466729448e-05, 'epoch': 0.3}
 30%|██▉       | 130/436 [16:50<38:21,  7.52s/it] 30%|███       | 131/436 [16:57<38:06,  7.50s/it]                                                 {'loss': 1.0242, 'learning_rate': 1.6440540278692656e-05, 'epoch': 0.3}
 30%|███       | 131/436 [16:57<38:06,  7.50s/it] 30%|███       | 132/436 [17:04<37:59,  7.50s/it]                                                 {'loss': 0.8595, 'learning_rate': 1.6383413150722417e-05, 'epoch': 0.3}
 30%|███       | 132/436 [17:05<37:59,  7.50s/it] 31%|███       | 133/436 [17:12<37:36,  7.45s/it]                                                 {'loss': 0.7932, 'learning_rate': 1.6325932248850206e-05, 'epoch': 0.3}
 31%|███       | 133/436 [17:12<37:36,  7.45s/it] 31%|███       | 134/436 [17:20<37:57,  7.54s/it]                                                 {'loss': 0.9692, 'learning_rate': 1.626810075871394e-05, 'epoch': 0.31}
 31%|███       | 134/436 [17:20<37:57,  7.54s/it] 31%|███       | 135/436 [17:27<37:52,  7.55s/it]                                                 {'loss': 0.9299, 'learning_rate': 1.6209921885381418e-05, 'epoch': 0.31}
 31%|███       | 135/436 [17:27<37:52,  7.55s/it] 31%|███       | 136/436 [17:35<37:49,  7.56s/it]                                                 {'loss': 0.6698, 'learning_rate': 1.615139885317269e-05, 'epoch': 0.31}
 31%|███       | 136/436 [17:35<37:49,  7.56s/it] 31%|███▏      | 137/436 [17:42<37:29,  7.52s/it]                                                 {'loss': 0.8413, 'learning_rate': 1.6092534905481367e-05, 'epoch': 0.31}
 31%|███▏      | 137/436 [17:42<37:29,  7.52s/it] 32%|███▏      | 138/436 [17:50<37:48,  7.61s/it]                                                 {'loss': 0.6333, 'learning_rate': 1.6033333304594886e-05, 'epoch': 0.32}
 32%|███▏      | 138/436 [17:50<37:48,  7.61s/it] 32%|███▏      | 139/436 [17:57<37:19,  7.54s/it]                                                 {'loss': 0.6545, 'learning_rate': 1.5973797331513674e-05, 'epoch': 0.32}
 32%|███▏      | 139/436 [17:57<37:19,  7.54s/it] 32%|███▏      | 140/436 [18:05<37:15,  7.55s/it]                                                 {'loss': 0.6787, 'learning_rate': 1.5913930285769356e-05, 'epoch': 0.32}
 32%|███▏      | 140/436 [18:05<37:15,  7.55s/it] 32%|███▏      | 141/436 [18:13<37:12,  7.57s/it]                                                 {'loss': 0.7021, 'learning_rate': 1.5853735485241858e-05, 'epoch': 0.32}
 32%|███▏      | 141/436 [18:13<37:12,  7.57s/it] 33%|███▎      | 142/436 [18:20<37:03,  7.56s/it]                                                 {'loss': 0.9548, 'learning_rate': 1.579321626597554e-05, 'epoch': 0.33}
 33%|███▎      | 142/436 [18:20<37:03,  7.56s/it] 33%|███▎      | 143/436 [18:27<36:38,  7.50s/it]                                                 {'loss': 0.6514, 'learning_rate': 1.573237598199432e-05, 'epoch': 0.33}
 33%|███▎      | 143/436 [18:27<36:38,  7.50s/it] 33%|███▎      | 144/436 [18:35<36:35,  7.52s/it]                                                 {'loss': 0.6715, 'learning_rate': 1.5671218005115767e-05, 'epoch': 0.33}
 33%|███▎      | 144/436 [18:35<36:35,  7.52s/it] 33%|███▎      | 145/436 [18:42<36:23,  7.50s/it]                                                 {'loss': 0.6233, 'learning_rate': 1.5609745724764264e-05, 'epoch': 0.33}
 33%|███▎      | 145/436 [18:43<36:23,  7.50s/it] 33%|███▎      | 146/436 [18:50<36:35,  7.57s/it]                                                 {'loss': 0.8032, 'learning_rate': 1.5547962547783126e-05, 'epoch': 0.33}
 33%|███▎      | 146/436 [18:50<36:35,  7.57s/it] 34%|███▎      | 147/436 [18:58<36:22,  7.55s/it]                                                 {'loss': 0.6243, 'learning_rate': 1.5485871898245824e-05, 'epoch': 0.34}
 34%|███▎      | 147/436 [18:58<36:22,  7.55s/it] 34%|███▍      | 148/436 [19:05<36:14,  7.55s/it]                                                 {'loss': 0.6399, 'learning_rate': 1.54234772172662e-05, 'epoch': 0.34}
 34%|███▍      | 148/436 [19:05<36:14,  7.55s/it] 34%|███▍      | 149/436 [19:13<36:08,  7.56s/it]                                                 {'loss': 0.5438, 'learning_rate': 1.536078196280777e-05, 'epoch': 0.34}
 34%|███▍      | 149/436 [19:13<36:08,  7.56s/it] 34%|███▍      | 150/436 [19:21<36:10,  7.59s/it]                                                 {'loss': 0.7527, 'learning_rate': 1.5297789609492062e-05, 'epoch': 0.34}
 34%|███▍      | 150/436 [19:21<36:10,  7.59s/it] 35%|███▍      | 151/436 [19:28<35:59,  7.58s/it]                                                 {'loss': 0.8479, 'learning_rate': 1.5234503648406075e-05, 'epoch': 0.35}
 35%|███▍      | 151/436 [19:28<35:59,  7.58s/it] 35%|███▍      | 152/436 [19:35<35:38,  7.53s/it]                                                 {'loss': 0.5671, 'learning_rate': 1.5170927586908787e-05, 'epoch': 0.35}
 35%|███▍      | 152/436 [19:35<35:38,  7.53s/it] 35%|███▌      | 153/436 [19:43<35:34,  7.54s/it]                                                 {'loss': 0.7964, 'learning_rate': 1.5107064948436758e-05, 'epoch': 0.35}
 35%|███▌      | 153/436 [19:43<35:34,  7.54s/it] 35%|███▌      | 154/436 [19:50<35:07,  7.47s/it]                                                 {'loss': 0.5425, 'learning_rate': 1.5042919272308895e-05, 'epoch': 0.35}
 35%|███▌      | 154/436 [19:50<35:07,  7.47s/it] 36%|███▌      | 155/436 [19:58<34:50,  7.44s/it]                                                 {'loss': 0.6021, 'learning_rate': 1.4978494113530268e-05, 'epoch': 0.36}
 36%|███▌      | 155/436 [19:58<34:50,  7.44s/it] 36%|███▌      | 156/436 [20:05<34:41,  7.43s/it]                                                 {'loss': 0.6747, 'learning_rate': 1.4913793042595109e-05, 'epoch': 0.36}
 36%|███▌      | 156/436 [20:05<34:41,  7.43s/it] 36%|███▌      | 157/436 [20:13<34:41,  7.46s/it]                                                 {'loss': 0.7395, 'learning_rate': 1.4848819645288915e-05, 'epoch': 0.36}
 36%|███▌      | 157/436 [20:13<34:41,  7.46s/it] 36%|███▌      | 158/436 [20:20<34:52,  7.53s/it]                                                 {'loss': 0.5949, 'learning_rate': 1.4783577522489733e-05, 'epoch': 0.36}
 36%|███▌      | 158/436 [20:20<34:52,  7.53s/it] 36%|███▋      | 159/436 [20:28<34:38,  7.50s/it]                                                 {'loss': 0.6787, 'learning_rate': 1.4718070289968602e-05, 'epoch': 0.36}
 36%|███▋      | 159/436 [20:28<34:38,  7.50s/it] 37%|███▋      | 160/436 [20:35<34:28,  7.49s/it]                                                 {'loss': 0.6633, 'learning_rate': 1.4652301578189141e-05, 'epoch': 0.37}
 37%|███▋      | 160/436 [20:35<34:28,  7.49s/it] 37%|███▋      | 161/436 [20:43<34:08,  7.45s/it]                                                 {'loss': 0.4469, 'learning_rate': 1.4586275032106373e-05, 'epoch': 0.37}
 37%|███▋      | 161/436 [20:43<34:08,  7.45s/it] 37%|███▋      | 162/436 [20:50<34:35,  7.57s/it]                                                 {'loss': 0.679, 'learning_rate': 1.4519994310964697e-05, 'epoch': 0.37}
 37%|███▋      | 162/436 [20:50<34:35,  7.57s/it] 37%|███▋      | 163/436 [20:58<34:10,  7.51s/it]                                                 {'loss': 0.7251, 'learning_rate': 1.4453463088095108e-05, 'epoch': 0.37}
 37%|███▋      | 163/436 [20:58<34:10,  7.51s/it] 38%|███▊      | 164/436 [21:06<34:14,  7.55s/it]                                                 {'loss': 0.8458, 'learning_rate': 1.4386685050711593e-05, 'epoch': 0.38}
 38%|███▊      | 164/436 [21:06<34:14,  7.55s/it] 38%|███▊      | 165/436 [21:13<34:15,  7.59s/it]                                                 {'loss': 0.6752, 'learning_rate': 1.4319663899706818e-05, 'epoch': 0.38}
 38%|███▊      | 165/436 [21:13<34:15,  7.59s/it] 38%|███▊      | 166/436 [21:21<34:11,  7.60s/it]                                                 {'loss': 0.7871, 'learning_rate': 1.4252403349446986e-05, 'epoch': 0.38}
 38%|███▊      | 166/436 [21:21<34:11,  7.60s/it] 38%|███▊      | 167/436 [21:28<33:58,  7.58s/it]                                                 {'loss': 0.6345, 'learning_rate': 1.4184907127566006e-05, 'epoch': 0.38}
 38%|███▊      | 167/436 [21:28<33:58,  7.58s/it] 39%|███▊      | 168/436 [21:36<33:45,  7.56s/it]                                                 {'loss': 0.4452, 'learning_rate': 1.4117178974758903e-05, 'epoch': 0.38}
 39%|███▊      | 168/436 [21:36<33:45,  7.56s/it] 39%|███▉      | 169/436 [21:43<33:21,  7.50s/it]                                                 {'loss': 0.3571, 'learning_rate': 1.404922264457449e-05, 'epoch': 0.39}
 39%|███▉      | 169/436 [21:43<33:21,  7.50s/it] 39%|███▉      | 170/436 [21:51<33:01,  7.45s/it]                                                 {'loss': 0.7065, 'learning_rate': 1.3981041903207364e-05, 'epoch': 0.39}
 39%|███▉      | 170/436 [21:51<33:01,  7.45s/it] 39%|███▉      | 171/436 [21:58<33:03,  7.49s/it]                                                 {'loss': 0.9795, 'learning_rate': 1.3912640529289163e-05, 'epoch': 0.39}
 39%|███▉      | 171/436 [21:58<33:03,  7.49s/it] 39%|███▉      | 172/436 [22:06<33:10,  7.54s/it]                                                 {'loss': 0.6356, 'learning_rate': 1.3844022313679167e-05, 'epoch': 0.39}
 39%|███▉      | 172/436 [22:06<33:10,  7.54s/it] 40%|███▉      | 173/436 [22:13<32:57,  7.52s/it]                                                 {'loss': 0.8809, 'learning_rate': 1.3775191059254185e-05, 'epoch': 0.4}
 40%|███▉      | 173/436 [22:13<32:57,  7.52s/it] 40%|███▉      | 174/436 [22:21<32:43,  7.49s/it]                                                 {'loss': 0.8611, 'learning_rate': 1.3706150580697826e-05, 'epoch': 0.4}
 40%|███▉      | 174/436 [22:21<32:43,  7.49s/it] 40%|████      | 175/436 [22:28<32:22,  7.44s/it]                                                 {'loss': 0.6978, 'learning_rate': 1.3636904704289053e-05, 'epoch': 0.4}
 40%|████      | 175/436 [22:28<32:22,  7.44s/it] 40%|████      | 176/436 [22:35<32:15,  7.45s/it]                                                 {'loss': 0.6129, 'learning_rate': 1.3567457267690152e-05, 'epoch': 0.4}
 40%|████      | 176/436 [22:35<32:15,  7.45s/it] 41%|████      | 177/436 [22:43<32:15,  7.47s/it]                                                 {'loss': 0.6714, 'learning_rate': 1.3497812119734037e-05, 'epoch': 0.41}
 41%|████      | 177/436 [22:43<32:15,  7.47s/it] 41%|████      | 178/436 [22:51<32:24,  7.54s/it]                                                 {'loss': 0.8354, 'learning_rate': 1.342797312021094e-05, 'epoch': 0.41}
 41%|████      | 178/436 [22:51<32:24,  7.54s/it] 41%|████      | 179/436 [22:58<32:38,  7.62s/it]                                                 {'loss': 0.6356, 'learning_rate': 1.3357944139654508e-05, 'epoch': 0.41}
 41%|████      | 179/436 [22:58<32:38,  7.62s/it] 41%|████▏     | 180/436 [23:06<32:11,  7.54s/it]                                                 {'loss': 0.6685, 'learning_rate': 1.3287729059127288e-05, 'epoch': 0.41}
 41%|████▏     | 180/436 [23:06<32:11,  7.54s/it] 42%|████▏     | 181/436 [23:13<31:58,  7.52s/it]                                                 {'loss': 0.5725, 'learning_rate': 1.3217331770005639e-05, 'epoch': 0.41}
 42%|████▏     | 181/436 [23:13<31:58,  7.52s/it] 42%|████▏     | 182/436 [23:21<31:36,  7.47s/it]                                                 {'loss': 0.4279, 'learning_rate': 1.3146756173764061e-05, 'epoch': 0.42}
 42%|████▏     | 182/436 [23:21<31:36,  7.47s/it] 42%|████▏     | 183/436 [23:28<31:21,  7.44s/it]                                                 {'loss': 0.5599, 'learning_rate': 1.3076006181758989e-05, 'epoch': 0.42}
 42%|████▏     | 183/436 [23:28<31:21,  7.44s/it] 42%|████▏     | 184/436 [23:35<31:12,  7.43s/it]                                                 {'loss': 0.8242, 'learning_rate': 1.3005085715012003e-05, 'epoch': 0.42}
 42%|████▏     | 184/436 [23:35<31:12,  7.43s/it] 42%|████▏     | 185/436 [23:43<31:06,  7.44s/it]                                                 {'loss': 0.6375, 'learning_rate': 1.2933998703992531e-05, 'epoch': 0.42}
 42%|████▏     | 185/436 [23:43<31:06,  7.44s/it] 43%|████▎     | 186/436 [23:50<30:59,  7.44s/it]                                                 {'loss': 0.7363, 'learning_rate': 1.2862749088400026e-05, 'epoch': 0.43}
 43%|████▎     | 186/436 [23:50<30:59,  7.44s/it] 43%|████▎     | 187/436 [23:58<31:06,  7.50s/it]                                                 {'loss': 0.9556, 'learning_rate': 1.279134081694561e-05, 'epoch': 0.43}
 43%|████▎     | 187/436 [23:58<31:06,  7.50s/it] 43%|████▎     | 188/436 [24:06<31:04,  7.52s/it]                                                 {'loss': 0.989, 'learning_rate': 1.2719777847133241e-05, 'epoch': 0.43}
 43%|████▎     | 188/436 [24:06<31:04,  7.52s/it] 43%|████▎     | 189/436 [24:13<31:04,  7.55s/it]                                                 {'loss': 0.7603, 'learning_rate': 1.2648064145040392e-05, 'epoch': 0.43}
 43%|████▎     | 189/436 [24:13<31:04,  7.55s/it] 44%|████▎     | 190/436 [24:21<30:50,  7.52s/it]                                                 {'loss': 0.8042, 'learning_rate': 1.2576203685098233e-05, 'epoch': 0.44}
 44%|████▎     | 190/436 [24:21<30:50,  7.52s/it] 44%|████▍     | 191/436 [24:28<30:44,  7.53s/it]                                                 {'loss': 0.4238, 'learning_rate': 1.2504200449871378e-05, 'epoch': 0.44}
 44%|████▍     | 191/436 [24:28<30:44,  7.53s/it] 44%|████▍     | 192/436 [24:35<30:15,  7.44s/it]                                                 {'loss': 0.7227, 'learning_rate': 1.2432058429837153e-05, 'epoch': 0.44}
 44%|████▍     | 192/436 [24:35<30:15,  7.44s/it] 44%|████▍     | 193/436 [24:43<30:13,  7.46s/it]                                                 {'loss': 0.6311, 'learning_rate': 1.2359781623164465e-05, 'epoch': 0.44}
 44%|████▍     | 193/436 [24:43<30:13,  7.46s/it] 44%|████▍     | 194/436 [24:50<29:59,  7.44s/it]                                                 {'loss': 0.7931, 'learning_rate': 1.2287374035492184e-05, 'epoch': 0.44}
 44%|████▍     | 194/436 [24:50<29:59,  7.44s/it] 45%|████▍     | 195/436 [24:58<29:56,  7.45s/it]                                                 {'loss': 0.4191, 'learning_rate': 1.2214839679707193e-05, 'epoch': 0.45}
 45%|████▍     | 195/436 [24:58<29:56,  7.45s/it] 45%|████▍     | 196/436 [25:05<29:57,  7.49s/it]                                                 {'loss': 0.9021, 'learning_rate': 1.2142182575721946e-05, 'epoch': 0.45}
 45%|████▍     | 196/436 [25:05<29:57,  7.49s/it] 45%|████▌     | 197/436 [25:13<29:47,  7.48s/it]                                                 {'loss': 0.6677, 'learning_rate': 1.2069406750251713e-05, 'epoch': 0.45}
 45%|████▌     | 197/436 [25:13<29:47,  7.48s/it] 45%|████▌     | 198/436 [25:20<29:36,  7.46s/it]                                                 {'loss': 0.5701, 'learning_rate': 1.1996516236591398e-05, 'epoch': 0.45}
 45%|████▌     | 198/436 [25:20<29:36,  7.46s/it] 46%|████▌     | 199/436 [25:28<29:22,  7.43s/it]                                                 {'loss': 0.7913, 'learning_rate': 1.1923515074392022e-05, 'epoch': 0.46}
 46%|████▌     | 199/436 [25:28<29:22,  7.43s/it] 46%|████▌     | 200/436 [25:35<29:10,  7.42s/it]                                                 {'loss': 0.7451, 'learning_rate': 1.1850407309436831e-05, 'epoch': 0.46}
 46%|████▌     | 200/436 [25:35<29:10,  7.42s/it]/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 46%|████▌     | 201/436 [27:29<2:34:15, 39.39s/it]                                                   {'loss': 0.5999, 'learning_rate': 1.1777196993417087e-05, 'epoch': 0.46}
 46%|████▌     | 201/436 [27:29<2:34:15, 39.39s/it] 46%|████▋     | 202/436 [27:36<1:56:15, 29.81s/it]                                                   {'loss': 0.5446, 'learning_rate': 1.1703888183707513e-05, 'epoch': 0.46}
 46%|████▋     | 202/436 [27:36<1:56:15, 29.81s/it] 47%|████▋     | 203/436 [27:44<1:29:47, 23.12s/it]                                                   {'loss': 0.6151, 'learning_rate': 1.1630484943141428e-05, 'epoch': 0.47}
 47%|████▋     | 203/436 [27:44<1:29:47, 23.12s/it] 47%|████▋     | 204/436 [27:51<1:11:11, 18.41s/it]                                                   {'loss': 0.5845, 'learning_rate': 1.1556991339785595e-05, 'epoch': 0.47}
 47%|████▋     | 204/436 [27:51<1:11:11, 18.41s/it] 47%|████▋     | 205/436 [27:59<58:22, 15.16s/it]                                                   {'loss': 0.7021, 'learning_rate': 1.1483411446714744e-05, 'epoch': 0.47}
 47%|████▋     | 205/436 [27:59<58:22, 15.16s/it] 47%|████▋     | 206/436 [28:06<49:13, 12.84s/it]                                                 {'loss': 0.7694, 'learning_rate': 1.1409749341785859e-05, 'epoch': 0.47}
 47%|████▋     | 206/436 [28:06<49:13, 12.84s/it] 47%|████▋     | 207/436 [28:14<42:52, 11.23s/it]                                                 {'loss': 0.6196, 'learning_rate': 1.1336009107412162e-05, 'epoch': 0.47}
 47%|████▋     | 207/436 [28:14<42:52, 11.23s/it] 48%|████▊     | 208/436 [28:21<38:14, 10.07s/it]                                                 {'loss': 0.5836, 'learning_rate': 1.1262194830336888e-05, 'epoch': 0.48}
 48%|████▊     | 208/436 [28:21<38:14, 10.07s/it] 48%|████▊     | 209/436 [28:29<35:05,  9.28s/it]                                                 {'loss': 0.8499, 'learning_rate': 1.118831060140676e-05, 'epoch': 0.48}
 48%|████▊     | 209/436 [28:29<35:05,  9.28s/it] 48%|████▊     | 210/436 [28:36<32:47,  8.70s/it]                                                 {'loss': 0.5127, 'learning_rate': 1.1114360515345301e-05, 'epoch': 0.48}
 48%|████▊     | 210/436 [28:36<32:47,  8.70s/it] 48%|████▊     | 211/436 [28:44<31:31,  8.40s/it]                                                 {'loss': 0.8391, 'learning_rate': 1.1040348670525889e-05, 'epoch': 0.48}
 48%|████▊     | 211/436 [28:44<31:31,  8.40s/it] 49%|████▊     | 212/436 [28:51<30:22,  8.14s/it]                                                 {'loss': 0.6178, 'learning_rate': 1.096627916874461e-05, 'epoch': 0.49}
 49%|████▊     | 212/436 [28:51<30:22,  8.14s/it] 49%|████▉     | 213/436 [28:59<29:25,  7.92s/it]                                                 {'loss': 0.5559, 'learning_rate': 1.0892156114992963e-05, 'epoch': 0.49}
 49%|████▉     | 213/436 [28:59<29:25,  7.92s/it] 49%|████▉     | 214/436 [29:06<28:34,  7.72s/it]                                                 {'loss': 0.6377, 'learning_rate': 1.0817983617230326e-05, 'epoch': 0.49}
 49%|████▉     | 214/436 [29:06<28:34,  7.72s/it] 49%|████▉     | 215/436 [29:13<28:10,  7.65s/it]                                                 {'loss': 0.7075, 'learning_rate': 1.0743765786156313e-05, 'epoch': 0.49}
 49%|████▉     | 215/436 [29:13<28:10,  7.65s/it] 50%|████▉     | 216/436 [29:21<28:18,  7.72s/it]                                                 {'loss': 0.676, 'learning_rate': 1.066950673498294e-05, 'epoch': 0.49}
 50%|████▉     | 216/436 [29:21<28:18,  7.72s/it] 50%|████▉     | 217/436 [29:29<27:50,  7.63s/it]                                                 {'loss': 0.7004, 'learning_rate': 1.0595210579206676e-05, 'epoch': 0.5}
 50%|████▉     | 217/436 [29:29<27:50,  7.63s/it] 50%|█████     | 218/436 [29:36<27:24,  7.54s/it]                                                 {'loss': 0.6389, 'learning_rate': 1.0520881436380366e-05, 'epoch': 0.5}
 50%|█████     | 218/436 [29:36<27:24,  7.54s/it] 50%|█████     | 219/436 [29:43<27:09,  7.51s/it]                                                 {'loss': 0.604, 'learning_rate': 1.0446523425885008e-05, 'epoch': 0.5}
 50%|█████     | 219/436 [29:43<27:09,  7.51s/it] 50%|█████     | 220/436 [29:51<27:23,  7.61s/it]                                                 {'loss': 0.5955, 'learning_rate': 1.0372140668701483e-05, 'epoch': 0.5}
 50%|█████     | 220/436 [29:51<27:23,  7.61s/it] 51%|█████     | 221/436 [29:59<27:08,  7.57s/it]                                                 {'loss': 0.6672, 'learning_rate': 1.0297737287182144e-05, 'epoch': 0.51}
 51%|█████     | 221/436 [29:59<27:08,  7.57s/it] 51%|█████     | 222/436 [30:06<26:43,  7.49s/it]                                                 {'loss': 0.5007, 'learning_rate': 1.022331740482237e-05, 'epoch': 0.51}
 51%|█████     | 222/436 [30:06<26:43,  7.49s/it] 51%|█████     | 223/436 [30:14<26:36,  7.50s/it]                                                 {'loss': 0.6519, 'learning_rate': 1.014888514603202e-05, 'epoch': 0.51}
 51%|█████     | 223/436 [30:14<26:36,  7.50s/it] 51%|█████▏    | 224/436 [30:21<26:35,  7.52s/it]                                                 {'loss': 0.6145, 'learning_rate': 1.0074444635906875e-05, 'epoch': 0.51}
 51%|█████▏    | 224/436 [30:21<26:35,  7.52s/it] 52%|█████▏    | 225/436 [30:28<26:14,  7.46s/it]                                                 {'loss': 0.7764, 'learning_rate': 1e-05, 'epoch': 0.52}
 52%|█████▏    | 225/436 [30:28<26:14,  7.46s/it] 52%|█████▏    | 226/436 [30:36<26:04,  7.45s/it]                                                 {'loss': 0.4453, 'learning_rate': 9.92555536409313e-06, 'epoch': 0.52}
 52%|█████▏    | 226/436 [30:36<26:04,  7.45s/it] 52%|█████▏    | 227/436 [30:43<26:00,  7.47s/it]                                                 {'loss': 0.6776, 'learning_rate': 9.85111485396798e-06, 'epoch': 0.52}
 52%|█████▏    | 227/436 [30:43<26:00,  7.47s/it] 52%|█████▏    | 228/436 [30:51<26:17,  7.58s/it]                                                 {'loss': 0.5759, 'learning_rate': 9.776682595177633e-06, 'epoch': 0.52}
 52%|█████▏    | 228/436 [30:51<26:17,  7.58s/it] 53%|█████▎    | 229/436 [30:59<25:58,  7.53s/it]                                                 {'loss': 0.5349, 'learning_rate': 9.702262712817857e-06, 'epoch': 0.52}
 53%|█████▎    | 229/436 [30:59<25:58,  7.53s/it] 53%|█████▎    | 230/436 [31:06<25:43,  7.49s/it]                                                 {'loss': 0.6167, 'learning_rate': 9.627859331298522e-06, 'epoch': 0.53}
 53%|█████▎    | 230/436 [31:06<25:43,  7.49s/it] 53%|█████▎    | 231/436 [31:13<25:26,  7.44s/it]                                                 {'loss': 0.6086, 'learning_rate': 9.553476574114993e-06, 'epoch': 0.53}
 53%|█████▎    | 231/436 [31:13<25:26,  7.44s/it] 53%|█████▎    | 232/436 [31:21<25:08,  7.39s/it]                                                 {'loss': 0.634, 'learning_rate': 9.479118563619638e-06, 'epoch': 0.53}
 53%|█████▎    | 232/436 [31:21<25:08,  7.39s/it] 53%|█████▎    | 233/436 [31:28<25:02,  7.40s/it]                                                 {'loss': 0.761, 'learning_rate': 9.404789420793327e-06, 'epoch': 0.53}
 53%|█████▎    | 233/436 [31:28<25:02,  7.40s/it] 54%|█████▎    | 234/436 [31:35<24:48,  7.37s/it]                                                 {'loss': 0.5349, 'learning_rate': 9.330493265017062e-06, 'epoch': 0.54}
 54%|█████▎    | 234/436 [31:35<24:48,  7.37s/it] 54%|█████▍    | 235/436 [31:43<24:41,  7.37s/it]                                                 {'loss': 0.424, 'learning_rate': 9.25623421384369e-06, 'epoch': 0.54}
 54%|█████▍    | 235/436 [31:43<24:41,  7.37s/it] 54%|█████▍    | 236/436 [31:50<24:39,  7.40s/it]                                                 {'loss': 0.7129, 'learning_rate': 9.182016382769678e-06, 'epoch': 0.54}
 54%|█████▍    | 236/436 [31:50<24:39,  7.40s/it] 54%|█████▍    | 237/436 [31:58<24:41,  7.45s/it]                                                 {'loss': 0.5874, 'learning_rate': 9.107843885007042e-06, 'epoch': 0.54}
 54%|█████▍    | 237/436 [31:58<24:41,  7.45s/it] 55%|█████▍    | 238/436 [32:05<24:38,  7.47s/it]                                                 {'loss': 0.7333, 'learning_rate': 9.033720831255391e-06, 'epoch': 0.55}
 55%|█████▍    | 238/436 [32:05<24:38,  7.47s/it] 55%|█████▍    | 239/436 [32:13<24:29,  7.46s/it]                                                 {'loss': 0.5208, 'learning_rate': 8.959651329474115e-06, 'epoch': 0.55}
 55%|█████▍    | 239/436 [32:13<24:29,  7.46s/it] 55%|█████▌    | 240/436 [32:20<24:25,  7.48s/it]                                                 {'loss': 0.6609, 'learning_rate': 8.8856394846547e-06, 'epoch': 0.55}
 55%|█████▌    | 240/436 [32:20<24:25,  7.48s/it] 55%|█████▌    | 241/436 [32:28<24:11,  7.44s/it]                                                 {'loss': 0.4851, 'learning_rate': 8.811689398593245e-06, 'epoch': 0.55}
 55%|█████▌    | 241/436 [32:28<24:11,  7.44s/it] 56%|█████▌    | 242/436 [32:35<24:11,  7.48s/it]                                                 {'loss': 0.5359, 'learning_rate': 8.737805169663113e-06, 'epoch': 0.55}
 56%|█████▌    | 242/436 [32:35<24:11,  7.48s/it] 56%|█████▌    | 243/436 [32:43<24:07,  7.50s/it]                                                 {'loss': 0.6995, 'learning_rate': 8.663990892587839e-06, 'epoch': 0.56}
 56%|█████▌    | 243/436 [32:43<24:07,  7.50s/it] 56%|█████▌    | 244/436 [32:50<24:07,  7.54s/it]                                                 {'loss': 0.6194, 'learning_rate': 8.590250658214148e-06, 'epoch': 0.56}
 56%|█████▌    | 244/436 [32:50<24:07,  7.54s/it] 56%|█████▌    | 245/436 [32:58<23:58,  7.53s/it]                                                 {'loss': 0.7441, 'learning_rate': 8.516588553285258e-06, 'epoch': 0.56}
 56%|█████▌    | 245/436 [32:58<23:58,  7.53s/it] 56%|█████▋    | 246/436 [33:05<23:45,  7.50s/it]                                                 {'loss': 0.5906, 'learning_rate': 8.443008660214409e-06, 'epoch': 0.56}
 56%|█████▋    | 246/436 [33:05<23:45,  7.50s/it] 57%|█████▋    | 247/436 [33:13<23:36,  7.49s/it]                                                 {'loss': 0.728, 'learning_rate': 8.369515056858575e-06, 'epoch': 0.57}
 57%|█████▋    | 247/436 [33:13<23:36,  7.49s/it] 57%|█████▋    | 248/436 [33:21<23:40,  7.56s/it]                                                 {'loss': 0.4349, 'learning_rate': 8.296111816292494e-06, 'epoch': 0.57}
 57%|█████▋    | 248/436 [33:21<23:40,  7.56s/it] 57%|█████▋    | 249/436 [33:28<23:31,  7.55s/it]                                                 {'loss': 0.5422, 'learning_rate': 8.222803006582915e-06, 'epoch': 0.57}
 57%|█████▋    | 249/436 [33:28<23:31,  7.55s/it] 57%|█████▋    | 250/436 [33:36<23:29,  7.58s/it]                                                 {'loss': 0.4999, 'learning_rate': 8.149592690563172e-06, 'epoch': 0.57}
 57%|█████▋    | 250/436 [33:36<23:29,  7.58s/it] 58%|█████▊    | 251/436 [33:43<23:14,  7.54s/it]                                                 {'loss': 0.4434, 'learning_rate': 8.076484925607983e-06, 'epoch': 0.58}
 58%|█████▊    | 251/436 [33:43<23:14,  7.54s/it] 58%|█████▊    | 252/436 [33:51<23:49,  7.77s/it]                                                 {'loss': 0.7041, 'learning_rate': 8.003483763408604e-06, 'epoch': 0.58}
 58%|█████▊    | 252/436 [33:51<23:49,  7.77s/it] 58%|█████▊    | 253/436 [33:59<23:24,  7.68s/it]                                                 {'loss': 0.4622, 'learning_rate': 7.930593249748289e-06, 'epoch': 0.58}
 58%|█████▊    | 253/436 [33:59<23:24,  7.68s/it] 58%|█████▊    | 254/436 [34:06<23:03,  7.60s/it]                                                 {'loss': 0.5662, 'learning_rate': 7.857817424278056e-06, 'epoch': 0.58}
 58%|█████▊    | 254/436 [34:06<23:03,  7.60s/it] 58%|█████▊    | 255/436 [34:14<22:59,  7.62s/it]                                                 {'loss': 0.5228, 'learning_rate': 7.785160320292812e-06, 'epoch': 0.58}
 58%|█████▊    | 255/436 [34:14<22:59,  7.62s/it] 59%|█████▊    | 256/436 [34:21<22:37,  7.54s/it]                                                 {'loss': 0.6094, 'learning_rate': 7.712625964507818e-06, 'epoch': 0.59}
 59%|█████▊    | 256/436 [34:21<22:37,  7.54s/it] 59%|█████▉    | 257/436 [34:29<22:24,  7.51s/it]                                                 {'loss': 0.6242, 'learning_rate': 7.64021837683554e-06, 'epoch': 0.59}
 59%|█████▉    | 257/436 [34:29<22:24,  7.51s/it] 59%|█████▉    | 258/436 [34:36<22:13,  7.49s/it]                                                 {'loss': 0.7871, 'learning_rate': 7.567941570162849e-06, 'epoch': 0.59}
 59%|█████▉    | 258/436 [34:36<22:13,  7.49s/it] 59%|█████▉    | 259/436 [34:44<22:01,  7.47s/it]                                                 {'loss': 0.7463, 'learning_rate': 7.495799550128625e-06, 'epoch': 0.59}
 59%|█████▉    | 259/436 [34:44<22:01,  7.47s/it] 60%|█████▉    | 260/436 [34:51<21:58,  7.49s/it]                                                 {'loss': 0.4971, 'learning_rate': 7.423796314901769e-06, 'epoch': 0.6}
 60%|█████▉    | 260/436 [34:51<21:58,  7.49s/it] 60%|█████▉    | 261/436 [34:59<21:46,  7.47s/it]                                                 {'loss': 0.6927, 'learning_rate': 7.351935854959608e-06, 'epoch': 0.6}
 60%|█████▉    | 261/436 [34:59<21:46,  7.47s/it] 60%|██████    | 262/436 [35:06<21:44,  7.50s/it]                                                 {'loss': 0.7417, 'learning_rate': 7.2802221528667604e-06, 'epoch': 0.6}
 60%|██████    | 262/436 [35:06<21:44,  7.50s/it] 60%|██████    | 263/436 [35:14<21:39,  7.51s/it]                                                 {'loss': 0.8115, 'learning_rate': 7.208659183054393e-06, 'epoch': 0.6}
 60%|██████    | 263/436 [35:14<21:39,  7.51s/it] 61%|██████    | 264/436 [35:21<21:32,  7.51s/it]                                                 {'loss': 0.5156, 'learning_rate': 7.137250911599978e-06, 'epoch': 0.6}
 61%|██████    | 264/436 [35:21<21:32,  7.51s/it] 61%|██████    | 265/436 [35:29<21:26,  7.52s/it]                                                 {'loss': 0.7749, 'learning_rate': 7.066001296007469e-06, 'epoch': 0.61}
 61%|██████    | 265/436 [35:29<21:26,  7.52s/it] 61%|██████    | 266/436 [35:36<21:18,  7.52s/it]                                                 {'loss': 0.6931, 'learning_rate': 6.9949142849880015e-06, 'epoch': 0.61}
 61%|██████    | 266/436 [35:36<21:18,  7.52s/it] 61%|██████    | 267/436 [35:44<21:04,  7.48s/it]                                                 {'loss': 0.7043, 'learning_rate': 6.9239938182410126e-06, 'epoch': 0.61}
 61%|██████    | 267/436 [35:44<21:04,  7.48s/it] 61%|██████▏   | 268/436 [35:51<21:00,  7.50s/it]                                                 {'loss': 0.5092, 'learning_rate': 6.8532438262359404e-06, 'epoch': 0.61}
 61%|██████▏   | 268/436 [35:51<21:00,  7.50s/it] 62%|██████▏   | 269/436 [35:59<21:00,  7.55s/it]                                                 {'loss': 0.6543, 'learning_rate': 6.7826682299943635e-06, 'epoch': 0.62}
 62%|██████▏   | 269/436 [35:59<21:00,  7.55s/it] 62%|██████▏   | 270/436 [36:06<20:51,  7.54s/it]                                                 {'loss': 0.6163, 'learning_rate': 6.712270940872713e-06, 'epoch': 0.62}
 62%|██████▏   | 270/436 [36:06<20:51,  7.54s/it] 62%|██████▏   | 271/436 [36:14<20:30,  7.46s/it]                                                 {'loss': 0.8318, 'learning_rate': 6.642055860345494e-06, 'epoch': 0.62}
 62%|██████▏   | 271/436 [36:14<20:30,  7.46s/it] 62%|██████▏   | 272/436 [36:21<20:31,  7.51s/it]                                                 {'loss': 0.7289, 'learning_rate': 6.572026879789064e-06, 'epoch': 0.62}
 62%|██████▏   | 272/436 [36:21<20:31,  7.51s/it] 63%|██████▎   | 273/436 [36:29<20:23,  7.51s/it]                                                 {'loss': 0.2738, 'learning_rate': 6.502187880265969e-06, 'epoch': 0.63}
 63%|██████▎   | 273/436 [36:29<20:23,  7.51s/it] 63%|██████▎   | 274/436 [36:36<20:15,  7.50s/it]                                                 {'loss': 0.811, 'learning_rate': 6.43254273230985e-06, 'epoch': 0.63}
 63%|██████▎   | 274/436 [36:36<20:15,  7.50s/it] 63%|██████▎   | 275/436 [36:44<20:10,  7.52s/it]                                                 {'loss': 0.573, 'learning_rate': 6.36309529571095e-06, 'epoch': 0.63}
 63%|██████▎   | 275/436 [36:44<20:10,  7.52s/it] 63%|██████▎   | 276/436 [36:51<19:56,  7.48s/it]                                                 {'loss': 0.52, 'learning_rate': 6.293849419302179e-06, 'epoch': 0.63}
 63%|██████▎   | 276/436 [36:51<19:56,  7.48s/it] 64%|██████▎   | 277/436 [36:59<19:41,  7.43s/it]                                                 {'loss': 0.5396, 'learning_rate': 6.224808940745814e-06, 'epoch': 0.63}
 64%|██████▎   | 277/436 [36:59<19:41,  7.43s/it] 64%|██████▍   | 278/436 [37:06<19:34,  7.44s/it]                                                 {'loss': 0.3723, 'learning_rate': 6.155977686320837e-06, 'epoch': 0.64}
 64%|██████▍   | 278/436 [37:06<19:34,  7.44s/it] 64%|██████▍   | 279/436 [37:13<19:21,  7.40s/it]                                                 {'loss': 0.5699, 'learning_rate': 6.087359470710841e-06, 'epoch': 0.64}
 64%|██████▍   | 279/436 [37:13<19:21,  7.40s/it] 64%|██████▍   | 280/436 [37:21<19:12,  7.39s/it]                                                 {'loss': 0.5038, 'learning_rate': 6.018958096792642e-06, 'epoch': 0.64}
 64%|██████▍   | 280/436 [37:21<19:12,  7.39s/it] 64%|██████▍   | 281/436 [37:28<19:07,  7.40s/it]                                                 {'loss': 0.554, 'learning_rate': 5.950777355425511e-06, 'epoch': 0.64}
 64%|██████▍   | 281/436 [37:28<19:07,  7.40s/it] 65%|██████▍   | 282/436 [37:36<19:07,  7.45s/it]                                                 {'loss': 0.4413, 'learning_rate': 5.8828210252411e-06, 'epoch': 0.65}
 65%|██████▍   | 282/436 [37:36<19:07,  7.45s/it] 65%|██████▍   | 283/436 [37:43<19:00,  7.45s/it]                                                 {'loss': 0.4963, 'learning_rate': 5.815092872433994e-06, 'epoch': 0.65}
 65%|██████▍   | 283/436 [37:43<19:00,  7.45s/it] 65%|██████▌   | 284/436 [37:51<19:02,  7.51s/it]                                                 {'loss': 0.6821, 'learning_rate': 5.74759665055302e-06, 'epoch': 0.65}
 65%|██████▌   | 284/436 [37:51<19:02,  7.51s/it] 65%|██████▌   | 285/436 [37:58<18:54,  7.51s/it]                                                 {'loss': 0.6424, 'learning_rate': 5.680336100293182e-06, 'epoch': 0.65}
 65%|██████▌   | 285/436 [37:58<18:54,  7.51s/it] 66%|██████▌   | 286/436 [38:06<18:44,  7.49s/it]                                                 {'loss': 0.6983, 'learning_rate': 5.613314949288409e-06, 'epoch': 0.66}
 66%|██████▌   | 286/436 [38:06<18:44,  7.49s/it] 66%|██████▌   | 287/436 [38:13<18:28,  7.44s/it]                                                 {'loss': 0.4429, 'learning_rate': 5.546536911904896e-06, 'epoch': 0.66}
 66%|██████▌   | 287/436 [38:13<18:28,  7.44s/it] 66%|██████▌   | 288/436 [38:21<18:20,  7.44s/it]                                                 {'loss': 0.4509, 'learning_rate': 5.4800056890353025e-06, 'epoch': 0.66}
 66%|██████▌   | 288/436 [38:21<18:20,  7.44s/it] 66%|██████▋   | 289/436 [38:28<18:22,  7.50s/it]                                                 {'loss': 0.4266, 'learning_rate': 5.4137249678936265e-06, 'epoch': 0.66}
 66%|██████▋   | 289/436 [38:28<18:22,  7.50s/it] 67%|██████▋   | 290/436 [38:36<18:10,  7.47s/it]                                                 {'loss': 0.8083, 'learning_rate': 5.347698421810861e-06, 'epoch': 0.66}
 67%|██████▋   | 290/436 [38:36<18:10,  7.47s/it] 67%|██████▋   | 291/436 [38:43<18:06,  7.50s/it]                                                 {'loss': 0.5848, 'learning_rate': 5.2819297100314e-06, 'epoch': 0.67}
 67%|██████▋   | 291/436 [38:43<18:06,  7.50s/it] 67%|██████▋   | 292/436 [38:51<18:02,  7.52s/it]                                                 {'loss': 0.5574, 'learning_rate': 5.216422477510267e-06, 'epoch': 0.67}
 67%|██████▋   | 292/436 [38:51<18:02,  7.52s/it] 67%|██████▋   | 293/436 [38:58<18:02,  7.57s/it]                                                 {'loss': 0.6326, 'learning_rate': 5.151180354711087e-06, 'epoch': 0.67}
 67%|██████▋   | 293/436 [38:58<18:02,  7.57s/it] 67%|██████▋   | 294/436 [39:06<17:49,  7.53s/it]                                                 {'loss': 0.647, 'learning_rate': 5.0862069574048956e-06, 'epoch': 0.67}
 67%|██████▋   | 294/436 [39:06<17:49,  7.53s/it] 68%|██████▊   | 295/436 [39:13<17:41,  7.53s/it]                                                 {'loss': 0.4856, 'learning_rate': 5.021505886469733e-06, 'epoch': 0.68}
 68%|██████▊   | 295/436 [39:13<17:41,  7.53s/it] 68%|██████▊   | 296/436 [39:21<17:57,  7.70s/it]                                                 {'loss': 0.4735, 'learning_rate': 4.957080727691107e-06, 'epoch': 0.68}
 68%|██████▊   | 296/436 [39:21<17:57,  7.70s/it] 68%|██████▊   | 297/436 [39:29<17:40,  7.63s/it]                                                 {'loss': 0.7794, 'learning_rate': 4.892935051563243e-06, 'epoch': 0.68}
 68%|██████▊   | 297/436 [39:29<17:40,  7.63s/it] 68%|██████▊   | 298/436 [39:37<17:33,  7.63s/it]                                                 {'loss': 0.3684, 'learning_rate': 4.829072413091219e-06, 'epoch': 0.68}
 68%|██████▊   | 298/436 [39:37<17:33,  7.63s/it] 69%|██████▊   | 299/436 [39:44<17:16,  7.56s/it]                                                 {'loss': 0.7976, 'learning_rate': 4.765496351593927e-06, 'epoch': 0.68}
 69%|██████▊   | 299/436 [39:44<17:16,  7.56s/it] 69%|██████▉   | 300/436 [39:51<17:07,  7.56s/it]                                                 {'loss': 0.876, 'learning_rate': 4.7022103905079405e-06, 'epoch': 0.69}
 69%|██████▉   | 300/436 [39:51<17:07,  7.56s/it] 69%|██████▉   | 301/436 [39:59<16:51,  7.49s/it]                                                 {'loss': 0.551, 'learning_rate': 4.639218037192235e-06, 'epoch': 0.69}
 69%|██████▉   | 301/436 [39:59<16:51,  7.49s/it] 69%|██████▉   | 302/436 [40:06<16:44,  7.50s/it]                                                 {'loss': 0.5468, 'learning_rate': 4.576522782733802e-06, 'epoch': 0.69}
 69%|██████▉   | 302/436 [40:06<16:44,  7.50s/it] 69%|██████▉   | 303/436 [40:14<16:35,  7.49s/it]                                                 {'loss': 0.5168, 'learning_rate': 4.514128101754183e-06, 'epoch': 0.69}
 69%|██████▉   | 303/436 [40:14<16:35,  7.49s/it] 70%|██████▉   | 304/436 [40:21<16:30,  7.50s/it]                                                 {'loss': 0.5196, 'learning_rate': 4.45203745221688e-06, 'epoch': 0.7}
 70%|██████▉   | 304/436 [40:21<16:30,  7.50s/it] 70%|██████▉   | 305/436 [40:29<16:15,  7.44s/it]                                                 {'loss': 0.437, 'learning_rate': 4.3902542752357415e-06, 'epoch': 0.7}
 70%|██████▉   | 305/436 [40:29<16:15,  7.44s/it] 70%|███████   | 306/436 [40:36<16:15,  7.51s/it]                                                 {'loss': 0.6351, 'learning_rate': 4.3287819948842334e-06, 'epoch': 0.7}
 70%|███████   | 306/436 [40:36<16:15,  7.51s/it] 70%|███████   | 307/436 [40:44<16:07,  7.50s/it]                                                 {'loss': 0.7734, 'learning_rate': 4.267624018005686e-06, 'epoch': 0.7}
 70%|███████   | 307/436 [40:44<16:07,  7.50s/it] 71%|███████   | 308/436 [40:51<16:01,  7.51s/it]                                                 {'loss': 0.5908, 'learning_rate': 4.206783734024463e-06, 'epoch': 0.71}
 71%|███████   | 308/436 [40:51<16:01,  7.51s/it] 71%|███████   | 309/436 [40:59<15:49,  7.48s/it]                                                 {'loss': 0.6466, 'learning_rate': 4.1462645147581456e-06, 'epoch': 0.71}
 71%|███████   | 309/436 [40:59<15:49,  7.48s/it] 71%|███████   | 310/436 [41:06<15:41,  7.47s/it]                                                 {'loss': 0.8459, 'learning_rate': 4.086069714230646e-06, 'epoch': 0.71}
 71%|███████   | 310/436 [41:06<15:41,  7.47s/it] 71%|███████▏  | 311/436 [41:14<15:29,  7.44s/it]                                                 {'loss': 0.6669, 'learning_rate': 4.0262026684863295e-06, 'epoch': 0.71}
 71%|███████▏  | 311/436 [41:14<15:29,  7.44s/it] 72%|███████▏  | 312/436 [41:21<15:33,  7.53s/it]                                                 {'loss': 0.6323, 'learning_rate': 3.96666669540512e-06, 'epoch': 0.71}
 72%|███████▏  | 312/436 [41:21<15:33,  7.53s/it] 72%|███████▏  | 313/436 [41:29<15:27,  7.54s/it]                                                 {'loss': 0.6594, 'learning_rate': 3.907465094518636e-06, 'epoch': 0.72}
 72%|███████▏  | 313/436 [41:29<15:27,  7.54s/it] 72%|███████▏  | 314/436 [41:36<15:20,  7.55s/it]                                                 {'loss': 0.7512, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72}
 72%|███████▏  | 314/436 [41:36<15:20,  7.55s/it] 72%|███████▏  | 315/436 [41:44<15:08,  7.51s/it]                                                 {'loss': 0.4191, 'learning_rate': 3.790078114618586e-06, 'epoch': 0.72}
 72%|███████▏  | 315/436 [41:44<15:08,  7.51s/it] 72%|███████▏  | 316/436 [41:51<15:02,  7.52s/it]                                                 {'loss': 0.4961, 'learning_rate': 3.731899241286061e-06, 'epoch': 0.72}
 72%|███████▏  | 316/436 [41:51<15:02,  7.52s/it] 73%|███████▎  | 317/436 [41:59<14:54,  7.52s/it]                                                 {'loss': 0.7909, 'learning_rate': 3.6740677511497958e-06, 'epoch': 0.73}
 73%|███████▎  | 317/436 [41:59<14:54,  7.52s/it] 73%|███████▎  | 318/436 [42:06<14:42,  7.48s/it]                                                 {'loss': 0.3878, 'learning_rate': 3.616586849277587e-06, 'epoch': 0.73}
 73%|███████▎  | 318/436 [42:06<14:42,  7.48s/it] 73%|███████▎  | 319/436 [42:14<14:28,  7.42s/it]                                                 {'loss': 0.39, 'learning_rate': 3.559459721307349e-06, 'epoch': 0.73}
 73%|███████▎  | 319/436 [42:14<14:28,  7.42s/it] 73%|███████▎  | 320/436 [42:21<14:31,  7.51s/it]                                                 {'loss': 0.5116, 'learning_rate': 3.5026895332705504e-06, 'epoch': 0.73}
 73%|███████▎  | 320/436 [42:21<14:31,  7.51s/it] 74%|███████▎  | 321/436 [42:29<14:16,  7.45s/it]                                                 {'loss': 0.3965, 'learning_rate': 3.4462794314167846e-06, 'epoch': 0.74}
 74%|███████▎  | 321/436 [42:29<14:16,  7.45s/it] 74%|███████▍  | 322/436 [42:36<14:08,  7.44s/it]                                                 {'loss': 0.5961, 'learning_rate': 3.390232542039352e-06, 'epoch': 0.74}
 74%|███████▍  | 322/436 [42:36<14:08,  7.44s/it] 74%|███████▍  | 323/436 [42:44<14:04,  7.47s/it]                                                 {'loss': 0.5082, 'learning_rate': 3.3345519713020445e-06, 'epoch': 0.74}
 74%|███████▍  | 323/436 [42:44<14:04,  7.47s/it] 74%|███████▍  | 324/436 [42:51<14:04,  7.54s/it]                                                 {'loss': 0.5686, 'learning_rate': 3.2792408050669634e-06, 'epoch': 0.74}
 74%|███████▍  | 324/436 [42:51<14:04,  7.54s/it] 75%|███████▍  | 325/436 [42:59<13:51,  7.49s/it]                                                 {'loss': 0.6186, 'learning_rate': 3.2243021087235336e-06, 'epoch': 0.74}
 75%|███████▍  | 325/436 [42:59<13:51,  7.49s/it] 75%|███████▍  | 326/436 [43:06<13:40,  7.46s/it]                                                 {'loss': 0.5042, 'learning_rate': 3.16973892701858e-06, 'epoch': 0.75}
 75%|███████▍  | 326/436 [43:06<13:40,  7.46s/it] 75%|███████▌  | 327/436 [43:14<13:38,  7.51s/it]                                                 {'loss': 0.7086, 'learning_rate': 3.115554283887614e-06, 'epoch': 0.75}
 75%|███████▌  | 327/436 [43:14<13:38,  7.51s/it] 75%|███████▌  | 328/436 [43:21<13:38,  7.58s/it]                                                 {'loss': 0.5444, 'learning_rate': 3.0617511822872337e-06, 'epoch': 0.75}
 75%|███████▌  | 328/436 [43:21<13:38,  7.58s/it] 75%|███████▌  | 329/436 [43:29<13:33,  7.60s/it]                                                 {'loss': 0.5412, 'learning_rate': 3.0083326040286977e-06, 'epoch': 0.75}
 75%|███████▌  | 329/436 [43:29<13:33,  7.60s/it] 76%|███████▌  | 330/436 [43:37<13:27,  7.61s/it]                                                 {'loss': 0.5551, 'learning_rate': 2.9553015096126638e-06, 'epoch': 0.76}
 76%|███████▌  | 330/436 [43:37<13:27,  7.61s/it] 76%|███████▌  | 331/436 [43:44<13:06,  7.49s/it]                                                 {'loss': 0.4752, 'learning_rate': 2.902660838065131e-06, 'epoch': 0.76}
 76%|███████▌  | 331/436 [43:44<13:06,  7.49s/it] 76%|███████▌  | 332/436 [43:51<12:58,  7.49s/it]                                                 {'loss': 0.5959, 'learning_rate': 2.8504135067745463e-06, 'epoch': 0.76}
 76%|███████▌  | 332/436 [43:51<12:58,  7.49s/it] 76%|███████▋  | 333/436 [43:59<12:48,  7.46s/it]                                                 {'loss': 0.4756, 'learning_rate': 2.798562411330126e-06, 'epoch': 0.76}
 76%|███████▋  | 333/436 [43:59<12:48,  7.46s/it] 77%|███████▋  | 334/436 [44:06<12:45,  7.50s/it]                                                 {'loss': 0.6897, 'learning_rate': 2.7471104253613645e-06, 'epoch': 0.77}
 77%|███████▋  | 334/436 [44:06<12:45,  7.50s/it] 77%|███████▋  | 335/436 [44:14<12:40,  7.53s/it]                                                 {'loss': 0.3967, 'learning_rate': 2.6960604003788014e-06, 'epoch': 0.77}
 77%|███████▋  | 335/436 [44:14<12:40,  7.53s/it] 77%|███████▋  | 336/436 [44:21<12:28,  7.48s/it]                                                 {'loss': 0.8259, 'learning_rate': 2.6454151656159666e-06, 'epoch': 0.77}
 77%|███████▋  | 336/436 [44:21<12:28,  7.48s/it] 77%|███████▋  | 337/436 [44:29<12:21,  7.49s/it]                                                 {'loss': 0.3944, 'learning_rate': 2.5951775278725956e-06, 'epoch': 0.77}
 77%|███████▋  | 337/436 [44:29<12:21,  7.49s/it] 78%|███████▊  | 338/436 [44:36<12:15,  7.50s/it]                                                 {'loss': 0.5112, 'learning_rate': 2.545350271359055e-06, 'epoch': 0.77}
 78%|███████▊  | 338/436 [44:36<12:15,  7.50s/it] 78%|███████▊  | 339/436 [44:44<12:12,  7.55s/it]                                                 {'loss': 0.7023, 'learning_rate': 2.495936157542074e-06, 'epoch': 0.78}
 78%|███████▊  | 339/436 [44:44<12:12,  7.55s/it] 78%|███████▊  | 340/436 [44:52<12:06,  7.57s/it]                                                 {'loss': 0.8929, 'learning_rate': 2.4469379249916614e-06, 'epoch': 0.78}
 78%|███████▊  | 340/436 [44:52<12:06,  7.57s/it] 78%|███████▊  | 341/436 [44:59<12:03,  7.61s/it]                                                 {'loss': 0.6088, 'learning_rate': 2.3983582892293642e-06, 'epoch': 0.78}
 78%|███████▊  | 341/436 [44:59<12:03,  7.61s/it] 78%|███████▊  | 342/436 [45:07<11:58,  7.64s/it]                                                 {'loss': 0.5265, 'learning_rate': 2.3501999425777433e-06, 'epoch': 0.78}
 78%|███████▊  | 342/436 [45:07<11:58,  7.64s/it] 79%|███████▊  | 343/436 [45:15<11:49,  7.63s/it]                                                 {'loss': 0.4877, 'learning_rate': 2.3024655540111984e-06, 'epoch': 0.79}
 79%|███████▊  | 343/436 [45:15<11:49,  7.63s/it] 79%|███████▉  | 344/436 [45:23<11:51,  7.73s/it]                                                 {'loss': 0.5325, 'learning_rate': 2.255157769008011e-06, 'epoch': 0.79}
 79%|███████▉  | 344/436 [45:23<11:51,  7.73s/it] 79%|███████▉  | 345/436 [45:30<11:40,  7.70s/it]                                                 {'loss': 0.6746, 'learning_rate': 2.2082792094037585e-06, 'epoch': 0.79}
 79%|███████▉  | 345/436 [45:30<11:40,  7.70s/it] 79%|███████▉  | 346/436 [45:38<11:32,  7.70s/it]                                                 {'loss': 0.5414, 'learning_rate': 2.1618324732459993e-06, 'epoch': 0.79}
 79%|███████▉  | 346/436 [45:38<11:32,  7.70s/it] 80%|███████▉  | 347/436 [45:45<11:22,  7.66s/it]                                                 {'loss': 0.4742, 'learning_rate': 2.1158201346502927e-06, 'epoch': 0.79}
 80%|███████▉  | 347/436 [45:46<11:22,  7.66s/it] 80%|███████▉  | 348/436 [45:53<11:17,  7.70s/it]                                                 {'loss': 0.5796, 'learning_rate': 2.0702447436575223e-06, 'epoch': 0.8}
 80%|███████▉  | 348/436 [45:53<11:17,  7.70s/it] 80%|████████  | 349/436 [46:01<11:07,  7.67s/it]                                                 {'loss': 0.5803, 'learning_rate': 2.0251088260925967e-06, 'epoch': 0.8}
 80%|████████  | 349/436 [46:01<11:07,  7.67s/it] 80%|████████  | 350/436 [46:08<10:55,  7.62s/it]                                                 {'loss': 0.5204, 'learning_rate': 1.9804148834244465e-06, 'epoch': 0.8}
 80%|████████  | 350/436 [46:08<10:55,  7.62s/it] 81%|████████  | 351/436 [46:16<10:51,  7.66s/it]                                                 {'loss': 0.7367, 'learning_rate': 1.9361653926274016e-06, 'epoch': 0.8}
 81%|████████  | 351/436 [46:16<10:51,  7.66s/it] 81%|████████  | 352/436 [46:24<10:46,  7.70s/it]                                                 {'loss': 0.5138, 'learning_rate': 1.8923628060439037e-06, 'epoch': 0.81}
 81%|████████  | 352/436 [46:24<10:46,  7.70s/it] 81%|████████  | 353/436 [46:31<10:34,  7.64s/it]                                                 {'loss': 0.6733, 'learning_rate': 1.8490095512486072e-06, 'epoch': 0.81}
 81%|████████  | 353/436 [46:31<10:34,  7.64s/it] 81%|████████  | 354/436 [46:39<10:23,  7.61s/it]                                                 {'loss': 0.347, 'learning_rate': 1.8061080309138379e-06, 'epoch': 0.81}
 81%|████████  | 354/436 [46:39<10:23,  7.61s/it] 81%|████████▏ | 355/436 [46:47<10:16,  7.61s/it]                                                 {'loss': 0.6769, 'learning_rate': 1.7636606226764353e-06, 'epoch': 0.81}
 81%|████████▏ | 355/436 [46:47<10:16,  7.61s/it] 82%|████████▏ | 356/436 [46:55<10:21,  7.77s/it]                                                 {'loss': 0.7832, 'learning_rate': 1.7216696790059718e-06, 'epoch': 0.82}
 82%|████████▏ | 356/436 [46:55<10:21,  7.77s/it] 82%|████████▏ | 357/436 [47:02<10:07,  7.68s/it]                                                 {'loss': 0.7246, 'learning_rate': 1.6801375270743925e-06, 'epoch': 0.82}
 82%|████████▏ | 357/436 [47:02<10:07,  7.68s/it] 82%|████████▏ | 358/436 [47:10<09:53,  7.61s/it]                                                 {'loss': 0.4237, 'learning_rate': 1.6390664686270342e-06, 'epoch': 0.82}
 82%|████████▏ | 358/436 [47:10<09:53,  7.61s/it] 82%|████████▏ | 359/436 [47:17<09:39,  7.53s/it]                                                 {'loss': 0.5646, 'learning_rate': 1.5984587798550633e-06, 'epoch': 0.82}
 82%|████████▏ | 359/436 [47:17<09:39,  7.53s/it] 83%|████████▎ | 360/436 [47:25<09:37,  7.60s/it]                                                 {'loss': 0.772, 'learning_rate': 1.5583167112693153e-06, 'epoch': 0.82}
 83%|████████▎ | 360/436 [47:25<09:37,  7.60s/it] 83%|████████▎ | 361/436 [47:32<09:30,  7.61s/it]                                                 {'loss': 0.4191, 'learning_rate': 1.518642487575591e-06, 'epoch': 0.83}
 83%|████████▎ | 361/436 [47:32<09:30,  7.61s/it] 83%|████████▎ | 362/436 [47:40<09:19,  7.57s/it]                                                 {'loss': 0.5896, 'learning_rate': 1.4794383075513453e-06, 'epoch': 0.83}
 83%|████████▎ | 362/436 [47:40<09:19,  7.57s/it] 83%|████████▎ | 363/436 [47:48<09:16,  7.62s/it]                                                 {'loss': 0.5535, 'learning_rate': 1.4407063439238333e-06, 'epoch': 0.83}
 83%|████████▎ | 363/436 [47:48<09:16,  7.62s/it] 83%|████████▎ | 364/436 [47:55<09:12,  7.68s/it]                                                 {'loss': 0.9304, 'learning_rate': 1.4024487432497013e-06, 'epoch': 0.83}
 83%|████████▎ | 364/436 [47:55<09:12,  7.68s/it] 84%|████████▎ | 365/436 [48:03<09:05,  7.68s/it]                                                 {'loss': 0.5839, 'learning_rate': 1.36466762579601e-06, 'epoch': 0.84}
 84%|████████▎ | 365/436 [48:03<09:05,  7.68s/it] 84%|████████▍ | 366/436 [48:11<08:53,  7.62s/it]                                                 {'loss': 0.3207, 'learning_rate': 1.3273650854227438e-06, 'epoch': 0.84}
 84%|████████▍ | 366/436 [48:11<08:53,  7.62s/it] 84%|████████▍ | 367/436 [48:19<08:53,  7.72s/it]                                                 {'loss': 0.4851, 'learning_rate': 1.2905431894667552e-06, 'epoch': 0.84}
 84%|████████▍ | 367/436 [48:19<08:53,  7.72s/it] 84%|████████▍ | 368/436 [48:26<08:42,  7.68s/it]                                                 {'loss': 0.5811, 'learning_rate': 1.2542039786272008e-06, 'epoch': 0.84}
 84%|████████▍ | 368/436 [48:26<08:42,  7.68s/it] 85%|████████▍ | 369/436 [48:34<08:34,  7.68s/it]                                                 {'loss': 0.6663, 'learning_rate': 1.218349466852432e-06, 'epoch': 0.85}
 85%|████████▍ | 369/436 [48:34<08:34,  7.68s/it] 85%|████████▍ | 370/436 [48:41<08:21,  7.60s/it]                                                 {'loss': 0.6346, 'learning_rate': 1.1829816412283912e-06, 'epoch': 0.85}
 85%|████████▍ | 370/436 [48:41<08:21,  7.60s/it] 85%|████████▌ | 371/436 [48:50<08:30,  7.85s/it]                                                 {'loss': 0.4006, 'learning_rate': 1.1481024618684821e-06, 'epoch': 0.85}
 85%|████████▌ | 371/436 [48:50<08:30,  7.85s/it] 85%|████████▌ | 372/436 [48:57<08:17,  7.78s/it]                                                 {'loss': 0.4635, 'learning_rate': 1.1137138618049403e-06, 'epoch': 0.85}
 85%|████████▌ | 372/436 [48:57<08:17,  7.78s/it] 86%|████████▌ | 373/436 [49:05<08:03,  7.68s/it]                                                 {'loss': 0.4905, 'learning_rate': 1.079817746881696e-06, 'epoch': 0.85}
 86%|████████▌ | 373/436 [49:05<08:03,  7.68s/it] 86%|████████▌ | 374/436 [49:12<07:50,  7.60s/it]                                                 {'loss': 0.5276, 'learning_rate': 1.0464159956487596e-06, 'epoch': 0.86}
 86%|████████▌ | 374/436 [49:12<07:50,  7.60s/it] 86%|████████▌ | 375/436 [49:20<07:50,  7.71s/it]                                                 {'loss': 0.569, 'learning_rate': 1.013510459258108e-06, 'epoch': 0.86}
 86%|████████▌ | 375/436 [49:20<07:50,  7.71s/it] 86%|████████▌ | 376/436 [49:27<07:37,  7.63s/it]                                                 {'loss': 0.5472, 'learning_rate': 9.811029613610913e-07, 'epoch': 0.86}
 86%|████████▌ | 376/436 [49:27<07:37,  7.63s/it] 86%|████████▋ | 377/436 [49:35<07:28,  7.61s/it]                                                 {'loss': 0.4937, 'learning_rate': 9.491952980073604e-07, 'epoch': 0.86}
 86%|████████▋ | 377/436 [49:35<07:28,  7.61s/it] 87%|████████▋ | 378/436 [49:43<07:19,  7.59s/it]                                                 {'loss': 0.4991, 'learning_rate': 9.177892375453413e-07, 'epoch': 0.87}
 87%|████████▋ | 378/436 [49:43<07:19,  7.59s/it] 87%|████████▋ | 379/436 [49:50<07:09,  7.54s/it]                                                 {'loss': 0.4734, 'learning_rate': 8.86886520524216e-07, 'epoch': 0.87}
 87%|████████▋ | 379/436 [49:50<07:09,  7.54s/it] 87%|████████▋ | 380/436 [49:58<07:03,  7.57s/it]                                                 {'loss': 0.3878, 'learning_rate': 8.564888595974718e-07, 'epoch': 0.87}
 87%|████████▋ | 380/436 [49:58<07:03,  7.57s/it] 87%|████████▋ | 381/436 [50:05<06:54,  7.54s/it]                                                 {'loss': 0.5736, 'learning_rate': 8.265979394279732e-07, 'epoch': 0.87}
 87%|████████▋ | 381/436 [50:05<06:54,  7.54s/it] 88%|████████▊ | 382/436 [50:13<06:47,  7.55s/it]                                                 {'loss': 0.4604, 'learning_rate': 7.972154165946155e-07, 'epoch': 0.88}
 88%|████████▊ | 382/436 [50:13<06:47,  7.55s/it] 88%|████████▊ | 383/436 [50:21<06:44,  7.63s/it]                                                 {'loss': 0.4562, 'learning_rate': 7.683429195004932e-07, 'epoch': 0.88}
 88%|████████▊ | 383/436 [50:21<06:44,  7.63s/it] 88%|████████▊ | 384/436 [50:28<06:32,  7.56s/it]                                                 {'loss': 0.5972, 'learning_rate': 7.399820482826692e-07, 'epoch': 0.88}
 88%|████████▊ | 384/436 [50:28<06:32,  7.56s/it] 88%|████████▊ | 385/436 [50:35<06:24,  7.54s/it]                                                 {'loss': 0.5481, 'learning_rate': 7.12134374723481e-07, 'epoch': 0.88}
 88%|████████▊ | 385/436 [50:35<06:24,  7.54s/it] 89%|████████▊ | 386/436 [50:43<06:17,  7.55s/it]                                                 {'loss': 0.6831, 'learning_rate': 6.848014421634497e-07, 'epoch': 0.88}
 89%|████████▊ | 386/436 [50:43<06:17,  7.55s/it] 89%|████████▉ | 387/436 [50:51<06:19,  7.74s/it]                                                 {'loss': 0.8794, 'learning_rate': 6.579847654157234e-07, 'epoch': 0.89}
 89%|████████▉ | 387/436 [50:51<06:19,  7.74s/it] 89%|████████▉ | 388/436 [50:59<06:05,  7.62s/it]                                                 {'loss': 0.3928, 'learning_rate': 6.316858306821449e-07, 'epoch': 0.89}
 89%|████████▉ | 388/436 [50:59<06:05,  7.62s/it] 89%|████████▉ | 389/436 [51:06<05:55,  7.57s/it]                                                 {'loss': 0.5099, 'learning_rate': 6.05906095470874e-07, 'epoch': 0.89}
 89%|████████▉ | 389/436 [51:06<05:55,  7.57s/it] 89%|████████▉ | 390/436 [51:13<05:46,  7.53s/it]                                                 {'loss': 0.4309, 'learning_rate': 5.806469885156163e-07, 'epoch': 0.89}
 89%|████████▉ | 390/436 [51:13<05:46,  7.53s/it] 90%|████████▉ | 391/436 [51:21<05:43,  7.64s/it]                                                 {'loss': 0.6533, 'learning_rate': 5.55909909696436e-07, 'epoch': 0.9}
 90%|████████▉ | 391/436 [51:21<05:43,  7.64s/it] 90%|████████▉ | 392/436 [51:29<05:32,  7.55s/it]                                                 {'loss': 0.4708, 'learning_rate': 5.316962299621808e-07, 'epoch': 0.9}
 90%|████████▉ | 392/436 [51:29<05:32,  7.55s/it] 90%|█████████ | 393/436 [51:36<05:23,  7.52s/it]                                                 {'loss': 0.5125, 'learning_rate': 5.080072912544987e-07, 'epoch': 0.9}
 90%|█████████ | 393/436 [51:36<05:23,  7.52s/it] 90%|█████████ | 394/436 [51:44<05:16,  7.54s/it]                                                 {'loss': 0.426, 'learning_rate': 4.848444064334679e-07, 'epoch': 0.9}
 90%|█████████ | 394/436 [51:44<05:16,  7.54s/it] 91%|█████████ | 395/436 [51:51<05:09,  7.55s/it]                                                 {'loss': 0.3732, 'learning_rate': 4.6220885920483014e-07, 'epoch': 0.9}
 91%|█████████ | 395/436 [51:51<05:09,  7.55s/it] 91%|█████████ | 396/436 [51:59<05:01,  7.53s/it]                                                 {'loss': 0.4341, 'learning_rate': 4.401019040488652e-07, 'epoch': 0.91}
 91%|█████████ | 396/436 [51:59<05:01,  7.53s/it] 91%|█████████ | 397/436 [52:06<04:54,  7.55s/it]                                                 {'loss': 0.3868, 'learning_rate': 4.1852476615083957e-07, 'epoch': 0.91}
 91%|█████████ | 397/436 [52:06<04:54,  7.55s/it] 91%|█████████▏| 398/436 [52:14<04:44,  7.50s/it]                                                 {'loss': 0.5311, 'learning_rate': 3.974786413331311e-07, 'epoch': 0.91}
 91%|█████████▏| 398/436 [52:14<04:44,  7.50s/it] 92%|█████████▏| 399/436 [52:21<04:38,  7.53s/it]                                                 {'loss': 0.576, 'learning_rate': 3.7696469598893727e-07, 'epoch': 0.91}
 92%|█████████▏| 399/436 [52:21<04:38,  7.53s/it] 92%|█████████▏| 400/436 [52:29<04:29,  7.50s/it]                                                 {'loss': 0.5735, 'learning_rate': 3.569840670176483e-07, 'epoch': 0.92}
 92%|█████████▏| 400/436 [52:29<04:29,  7.50s/it] 92%|█████████▏| 401/436 [54:25<23:22, 40.07s/it]                                                 {'loss': 0.517, 'learning_rate': 3.3753786176182303e-07, 'epoch': 0.92}
 92%|█████████▏| 401/436 [54:25<23:22, 40.07s/it] 92%|█████████▏| 402/436 [54:32<17:09, 30.26s/it]                                                 {'loss': 0.5079, 'learning_rate': 3.186271579458333e-07, 'epoch': 0.92}
 92%|█████████▏| 402/436 [54:32<17:09, 30.26s/it] 92%|█████████▏| 403/436 [54:40<12:52, 23.40s/it]                                                 {'loss': 0.5994, 'learning_rate': 3.002530036161222e-07, 'epoch': 0.92}
 92%|█████████▏| 403/436 [54:40<12:52, 23.40s/it] 93%|█████████▎| 404/436 [54:47<10:00, 18.76s/it]                                                 {'loss': 0.5739, 'learning_rate': 2.824164170831389e-07, 'epoch': 0.93}
 93%|█████████▎| 404/436 [54:48<10:00, 18.76s/it] 93%|█████████▎| 405/436 [54:55<07:58, 15.44s/it]                                                 {'loss': 0.4801, 'learning_rate': 2.651183868648821e-07, 'epoch': 0.93}
 93%|█████████▎| 405/436 [54:55<07:58, 15.44s/it] 93%|█████████▎| 406/436 [55:03<06:31, 13.03s/it]                                                 {'loss': 0.5529, 'learning_rate': 2.483598716321289e-07, 'epoch': 0.93}
 93%|█████████▎| 406/436 [55:03<06:31, 13.03s/it] 93%|█████████▎| 407/436 [55:10<05:29, 11.35s/it]                                                 {'loss': 0.5696, 'learning_rate': 2.321418001553022e-07, 'epoch': 0.93}
 93%|█████████▎| 407/436 [55:10<05:29, 11.35s/it] 94%|█████████▎| 408/436 [55:17<04:44, 10.17s/it]                                                 {'loss': 0.4669, 'learning_rate': 2.1646507125299587e-07, 'epoch': 0.93}
 94%|█████████▎| 408/436 [55:17<04:44, 10.17s/it] 94%|█████████▍| 409/436 [55:25<04:13,  9.39s/it]                                                 {'loss': 0.7527, 'learning_rate': 2.013305537421606e-07, 'epoch': 0.94}
 94%|█████████▍| 409/436 [55:25<04:13,  9.39s/it] 94%|█████████▍| 410/436 [55:32<03:48,  8.80s/it]                                                 {'loss': 0.6439, 'learning_rate': 1.867390863899543e-07, 'epoch': 0.94}
 94%|█████████▍| 410/436 [55:32<03:48,  8.80s/it] 94%|█████████▍| 411/436 [55:40<03:30,  8.43s/it]                                                 {'loss': 0.5583, 'learning_rate': 1.726914778672606e-07, 'epoch': 0.94}
 94%|█████████▍| 411/436 [55:40<03:30,  8.43s/it] 94%|█████████▍| 412/436 [55:47<03:15,  8.13s/it]                                                 {'loss': 0.325, 'learning_rate': 1.5918850670386677e-07, 'epoch': 0.94}
 94%|█████████▍| 412/436 [55:47<03:15,  8.13s/it] 95%|█████████▍| 413/436 [55:56<03:07,  8.16s/it]                                                 {'loss': 0.4253, 'learning_rate': 1.4623092124531613e-07, 'epoch': 0.95}
 95%|█████████▍| 413/436 [55:56<03:07,  8.16s/it] 95%|█████████▍| 414/436 [56:03<02:54,  7.95s/it]                                                 {'loss': 0.78, 'learning_rate': 1.3381943961144118e-07, 'epoch': 0.95}
 95%|█████████▍| 414/436 [56:03<02:54,  7.95s/it] 95%|█████████▌| 415/436 [56:10<02:42,  7.75s/it]                                                 {'loss': 0.6425, 'learning_rate': 1.2195474965655652e-07, 'epoch': 0.95}
 95%|█████████▌| 415/436 [56:10<02:42,  7.75s/it] 95%|█████████▌| 416/436 [56:18<02:34,  7.73s/it]                                                 {'loss': 0.4739, 'learning_rate': 1.1063750893134273e-07, 'epoch': 0.95}
 95%|█████████▌| 416/436 [56:18<02:34,  7.73s/it] 96%|█████████▌| 417/436 [56:26<02:25,  7.67s/it]                                                 {'loss': 0.5717, 'learning_rate': 9.986834464640328e-08, 'epoch': 0.96}
 96%|█████████▌| 417/436 [56:26<02:25,  7.67s/it] 96%|█████████▌| 418/436 [56:33<02:16,  7.59s/it]                                                 {'loss': 0.6938, 'learning_rate': 8.964785363750228e-08, 'epoch': 0.96}
 96%|█████████▌| 418/436 [56:33<02:16,  7.59s/it] 96%|█████████▌| 419/436 [56:40<02:08,  7.54s/it]                                                 {'loss': 0.5375, 'learning_rate': 7.997660233249105e-08, 'epoch': 0.96}
 96%|█████████▌| 419/436 [56:40<02:08,  7.54s/it] 96%|█████████▋| 420/436 [56:48<02:01,  7.60s/it]                                                 {'loss': 0.5421, 'learning_rate': 7.08551267199098e-08, 'epoch': 0.96}
 96%|█████████▋| 420/436 [56:48<02:01,  7.60s/it] 97%|█████████▋| 421/436 [56:56<01:54,  7.61s/it]                                                 {'loss': 0.5741, 'learning_rate': 6.22839323192892e-08, 'epoch': 0.96}
 97%|█████████▋| 421/436 [56:56<01:54,  7.61s/it] 97%|█████████▋| 422/436 [57:03<01:46,  7.57s/it]                                                 {'loss': 0.4238, 'learning_rate': 5.426349415313503e-08, 'epoch': 0.97}
 97%|█████████▋| 422/436 [57:03<01:46,  7.57s/it] 97%|█████████▋| 423/436 [57:10<01:36,  7.46s/it]                                                 {'loss': 0.2944, 'learning_rate': 4.679425672059035e-08, 'epoch': 0.97}
 97%|█████████▋| 423/436 [57:10<01:36,  7.46s/it] 97%|█████████▋| 424/436 [57:18<01:30,  7.56s/it]                                                 {'loss': 0.7367, 'learning_rate': 3.987663397281627e-08, 'epoch': 0.97}
 97%|█████████▋| 424/436 [57:18<01:30,  7.56s/it] 97%|█████████▋| 425/436 [57:26<01:22,  7.51s/it]                                                 {'loss': 0.4121, 'learning_rate': 3.3511009290042585e-08, 'epoch': 0.97}
 97%|█████████▋| 425/436 [57:26<01:22,  7.51s/it] 98%|█████████▊| 426/436 [57:33<01:14,  7.44s/it]                                                 {'loss': 0.5428, 'learning_rate': 2.7697735460316954e-08, 'epoch': 0.98}
 98%|█████████▊| 426/436 [57:33<01:14,  7.44s/it] 98%|█████████▊| 427/436 [57:40<01:06,  7.44s/it]                                                 {'loss': 0.4089, 'learning_rate': 2.2437134659962777e-08, 'epoch': 0.98}
 98%|█████████▊| 427/436 [57:40<01:06,  7.44s/it] 98%|█████████▊| 428/436 [57:48<01:00,  7.54s/it]                                                 {'loss': 0.4003, 'learning_rate': 1.7729498435716808e-08, 'epoch': 0.98}
 98%|█████████▊| 428/436 [57:48<01:00,  7.54s/it] 98%|█████████▊| 429/436 [57:56<00:52,  7.52s/it]                                                 {'loss': 0.5042, 'learning_rate': 1.3575087688570965e-08, 'epoch': 0.98}
 98%|█████████▊| 429/436 [57:56<00:52,  7.52s/it] 99%|█████████▊| 430/436 [58:03<00:44,  7.48s/it]                                                 {'loss': 0.583, 'learning_rate': 9.974132659319457e-09, 'epoch': 0.99}
 99%|█████████▊| 430/436 [58:03<00:44,  7.48s/it] 99%|█████████▉| 431/436 [58:10<00:37,  7.47s/it]                                                 {'loss': 0.4794, 'learning_rate': 6.926832915791215e-09, 'epoch': 0.99}
 99%|█████████▉| 431/436 [58:11<00:37,  7.47s/it] 99%|█████████▉| 432/436 [58:18<00:30,  7.53s/it]                                                 {'loss': 0.4084, 'learning_rate': 4.433357341795397e-09, 'epoch': 0.99}
 99%|█████████▉| 432/436 [58:18<00:30,  7.53s/it] 99%|█████████▉| 433/436 [58:26<00:22,  7.50s/it]                                                 {'loss': 0.4895, 'learning_rate': 2.4938441277566615e-09, 'epoch': 0.99}
 99%|█████████▉| 433/436 [58:26<00:22,  7.50s/it]100%|█████████▉| 434/436 [58:33<00:14,  7.44s/it]                                                 {'loss': 0.4196, 'learning_rate': 1.1084007630612903e-09, 'epoch': 0.99}
100%|█████████▉| 434/436 [58:33<00:14,  7.44s/it]100%|█████████▉| 435/436 [58:40<00:07,  7.48s/it]                                                 {'loss': 0.6196, 'learning_rate': 2.7710403009750807e-10, 'epoch': 1.0}
100%|█████████▉| 435/436 [58:40<00:07,  7.48s/it]100%|██████████| 436/436 [58:48<00:00,  7.55s/it]                                                 {'loss': 0.3456, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 436/436 [58:48<00:00,  7.55s/it]                                                 {'train_runtime': 3553.3377, 'train_samples_per_second': 0.982, 'train_steps_per_second': 0.123, 'train_loss': 0.748067750843293, 'epoch': 1.0}
100%|██████████| 436/436 [58:49<00:00,  7.55s/it]100%|██████████| 436/436 [58:49<00:00,  8.09s/it]
[2024-05-23 05:25:49,211] [INFO] [launch.py:347:main] Process 491921 exits successfully.
[2024-05-23 05:25:49,212] [INFO] [launch.py:347:main] Process 491920 exits successfully.
[2024-05-23 05:25:49,213] [INFO] [launch.py:347:main] Process 491919 exits successfully.
wandb: - 0.011 MB of 0.011 MB uploadedwandb: \ 0.011 MB of 0.030 MB uploadedwandb: | 0.057 MB of 0.057 MB uploadedwandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▃▇██████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                     train/loss █▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▁▂▁▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 436
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3456
wandb:               train/total_flos 1.902551951946547e+16
wandb:               train/train_loss 0.74807
wandb:            train/train_runtime 3553.3377
wandb: train/train_samples_per_second 0.982
wandb:   train/train_steps_per_second 0.123
wandb: 
wandb: 🚀 View run hearty-thunder-26 at: https://wandb.ai/tonytragoudaras/huggingface/runs/5s5xihry
wandb: ⭐️ View project at: https://wandb.ai/tonytragoudaras/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240523_042607-5s5xihry/logs
[2024-05-23 05:26:12,237] [INFO] [launch.py:347:main] Process 491918 exits successfully.

JOB STATISTICS
==============
Job ID: 6344688
Cluster: snellius
User/Group: scur0405/scur0405
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 72
CPU Utilized: 15:47:21
CPU Efficiency: 20.04% of 3-06:48:00 core-walltime
Job Wall-clock time: 01:05:40
Memory Utilized: 238.68 GB
Memory Efficiency: 49.73% of 480.00 GB
