============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[2024-05-22 11:18:37,461] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 11:18:37,461] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 11:18:37,461] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 11:18:37,461] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.96s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.02s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.05s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.06s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_val_projector.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vlm_att_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Traceback (most recent call last):
Traceback (most recent call last):
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 189, in <module>
Traceback (most recent call last):
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 189, in <module>
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 189, in <module>
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
  0%|          | 0/64 [00:00<?, ?it/s]    run_inference(args)
    run_inference(args)
    run_inference(args)
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 84, in run_inference
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 84, in run_inference
  File "/gpfs/home4/scur0405/LLaMA-VID/llamavid/eval/model_ucf_crime_qa.py", line 84, in run_inference
    os.makedirs(args.output_dir)
    os.makedirs(args.output_dir)
    os.makedirs(args.output_dir)
  File "/home/scur0405/.conda/envs/llamavid/lib/python3.10/os.py", line 225, in makedirs
  File "/home/scur0405/.conda/envs/llamavid/lib/python3.10/os.py", line 225, in makedirs
  File "/home/scur0405/.conda/envs/llamavid/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: './work_dirs/zs_eval_ucf_crime/llama-vid/llama-vid-7b-full-224-video-fps-1'
    mkdir(name, mode)
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: './work_dirs/zs_eval_ucf_crime/llama-vid/llama-vid-7b-full-224-video-fps-1'
FileExistsError: [Errno 17] File exists: './work_dirs/zs_eval_ucf_crime/llama-vid/llama-vid-7b-full-224-video-fps-1'
  2%|▏         | 1/64 [00:08<09:18,  8.86s/it]  3%|▎         | 2/64 [00:09<04:12,  4.07s/it]  5%|▍         | 3/64 [00:11<02:58,  2.92s/it]  6%|▋         | 4/64 [00:17<04:19,  4.33s/it]  8%|▊         | 5/64 [00:19<03:15,  3.32s/it]  9%|▉         | 6/64 [00:21<02:45,  2.85s/it] 11%|█         | 7/64 [00:23<02:26,  2.58s/it] 12%|█▎        | 8/64 [00:23<01:52,  2.01s/it] 14%|█▍        | 9/64 [00:24<01:27,  1.58s/it] 16%|█▌        | 10/64 [00:26<01:34,  1.75s/it] 17%|█▋        | 11/64 [00:27<01:13,  1.40s/it] 19%|█▉        | 12/64 [00:28<01:11,  1.37s/it] 20%|██        | 13/64 [00:30<01:22,  1.62s/it] 22%|██▏       | 14/64 [00:36<02:27,  2.94s/it] 23%|██▎       | 15/64 [00:39<02:28,  3.02s/it] 25%|██▌       | 16/64 [00:42<02:12,  2.75s/it] 27%|██▋       | 17/64 [00:43<01:43,  2.20s/it] 28%|██▊       | 18/64 [00:49<02:36,  3.40s/it] 30%|██▉       | 19/64 [01:04<05:13,  6.97s/it] 31%|███▏      | 20/64 [01:26<08:19, 11.34s/it] 33%|███▎      | 21/64 [01:34<07:27, 10.40s/it] 34%|███▍      | 22/64 [01:36<05:29,  7.85s/it] 36%|███▌      | 23/64 [01:44<05:28,  8.00s/it] 38%|███▊      | 24/64 [01:46<04:09,  6.24s/it] 39%|███▉      | 25/64 [01:48<03:13,  4.97s/it] 41%|████      | 26/64 [01:51<02:45,  4.35s/it] 42%|████▏     | 27/64 [01:53<02:17,  3.72s/it] 44%|████▍     | 28/64 [01:59<02:38,  4.40s/it] 45%|████▌     | 29/64 [02:00<01:56,  3.33s/it] 47%|████▋     | 30/64 [02:02<01:35,  2.80s/it] 48%|████▊     | 31/64 [02:11<02:38,  4.81s/it] 50%|█████     | 32/64 [02:12<01:57,  3.66s/it] 52%|█████▏    | 33/64 [02:15<01:49,  3.54s/it] 53%|█████▎    | 34/64 [02:21<02:00,  4.03s/it] 55%|█████▍    | 35/64 [02:22<01:30,  3.13s/it] 56%|█████▋    | 36/64 [02:25<01:30,  3.22s/it] 58%|█████▊    | 37/64 [02:33<02:08,  4.74s/it] 59%|█████▉    | 38/64 [02:37<01:53,  4.37s/it] 61%|██████    | 39/64 [02:38<01:25,  3.43s/it] 62%|██████▎   | 40/64 [02:39<01:07,  2.79s/it] 64%|██████▍   | 41/64 [02:40<00:50,  2.19s/it] 66%|██████▌   | 42/64 [02:41<00:41,  1.90s/it] 67%|██████▋   | 43/64 [02:42<00:34,  1.65s/it] 69%|██████▉   | 44/64 [02:48<00:58,  2.93s/it] 70%|███████   | 45/64 [02:57<01:27,  4.61s/it] 72%|███████▏  | 46/64 [02:58<01:01,  3.43s/it] 73%|███████▎  | 47/64 [02:59<00:50,  2.97s/it] 75%|███████▌  | 48/64 [03:01<00:41,  2.60s/it] 77%|███████▋  | 49/64 [03:04<00:41,  2.75s/it] 78%|███████▊  | 50/64 [03:11<00:55,  3.94s/it] 80%|███████▉  | 51/64 [03:15<00:49,  3.80s/it] 81%|████████▏ | 52/64 [03:17<00:41,  3.50s/it] 83%|████████▎ | 53/64 [03:20<00:35,  3.22s/it] 84%|████████▍ | 54/64 [03:28<00:47,  4.75s/it] 86%|████████▌ | 55/64 [03:30<00:35,  3.93s/it] 88%|████████▊ | 56/64 [03:32<00:27,  3.43s/it] 89%|████████▉ | 57/64 [04:20<01:56, 16.59s/it] 91%|█████████ | 58/64 [04:20<01:10, 11.82s/it] 92%|█████████▏| 59/64 [04:35<01:03, 12.78s/it] 94%|█████████▍| 60/64 [04:39<00:39,  9.90s/it] 95%|█████████▌| 61/64 [04:40<00:22,  7.37s/it] 97%|█████████▋| 62/64 [04:42<00:11,  5.85s/it] 98%|█████████▊| 63/64 [04:59<00:08,  8.93s/it]100%|██████████| 64/64 [05:03<00:00,  7.67s/it]100%|██████████| 64/64 [05:03<00:00,  4.75s/it]
Current Video Frames: 5327 and FPS: 30 ---- Explosion036_x264.mp4
Output video frames: 178, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 155 and FPS: 30 ---- RoadAccidents021_x264.mp4
Output video frames: 6, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1150 and FPS: 30 ---- Normal_Videos_937_x264.mp4
Output video frames: 39, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 5408 and FPS: 30 ---- Normal_Videos_210_x264.mp4
Output video frames: 181, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1076 and FPS: 30 ---- Normal_Videos_024_x264.mp4
Output video frames: 36, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1499 and FPS: 30 ---- Normal_Videos_014_x264.mp4
Output video frames: 50, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1455 and FPS: 30 ---- Normal_Videos_900_x264.mp4
Output video frames: 49, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 405 and FPS: 30 ---- Normal_Videos_251_x264.mp4
Output video frames: 14, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 305 and FPS: 30 ---- Normal_Videos_745_x264.mp4
Output video frames: 11, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1799 and FPS: 30 ---- Normal_Videos_873_x264.mp4
Output video frames: 60, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 314 and FPS: 30 ---- Normal_Videos_889_x264.mp4
Output video frames: 11, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 776 and FPS: 30 ---- Explosion027_x264.mp4
Output video frames: 26, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 1817 and FPS: 30 ---- Normal_Videos_217_x264.mp4
Output video frames: 61, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 5322 and FPS: 30 ---- Normal_Videos_453_x264.mp4
Output video frames: 178, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 2565 and FPS: 30 ---- Normal_Videos_875_x264.mp4
Output video frames: 86, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1799 and FPS: 30 ---- Shooting018_x264.mp4
Output video frames: 60, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 574 and FPS: 30 ---- Normal_Videos_888_x264.mp4
Output video frames: 20, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1795 and FPS: 30 ---- Arson016_x264.mp4
Output video frames: 60, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a bus where a fire starts in the middle of the bus and spreads quickly, causing the bus to fill with smoke. The passengers try to escape the bus, but some are trapped inside. The firefighters arrive and put out the fire, but the bus is destroyed.
------------------------------------------ final output: The video shows a bus where a fire starts in the middle of the bus and spreads quickly, causing the bus to fill with smoke. The passengers try to escape the bus, but some are trapped inside. The firefighters arrive and put out the fire, but the bus is destroyed.
Current Video Frames: 13459 and FPS: 30 ---- Normal_Videos_634_x264.mp4
Output video frames: 449, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 15835 and FPS: 30 ---- Arrest039_x264.mp4
Output video frames: 528, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a man walking around a hallway and then sitting down. He then gets up and walks around again. Based on this information, the activity in the video can be classified as Normal Video.
------------------------------------------ final output: The video shows a man walking around a hallway and then sitting down. He then gets up and walks around again. Based on this information, the activity in the video can be classified as Normal Video.
Current Video Frames: 7646 and FPS: 30 ---- Explosion043_x264.mp4
Output video frames: 255, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1625 and FPS: 30 ---- Shooting008_x264.mp4
Output video frames: 55, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 2571 and FPS: 30 ---- Normal_Videos_203_x264.mp4
Output video frames: 86, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a car parked in a parking lot with people walking around and a man getting out of the car. There is no evidence of any activity that could be classified as abuse, arrest, arson, assault, burglary, explosion, fighting, robbery, road accident, shooting, or normal video. Therefore, the category for this video is "Normal Video."
------------------------------------------ final output: The video shows a car parked in a parking lot with people walking around and a man getting out of the car. There is no evidence of any activity that could be classified as abuse, arrest, arson, assault, burglary, explosion, fighting, robbery, road accident, shooting, or normal video. Therefore, the category for this video is "Normal Video."
Current Video Frames: 1571 and FPS: 30 ---- Explosion011_x264.mp4
Output video frames: 53, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1495 and FPS: 30 ---- RoadAccidents124_x264.mp4
Output video frames: 50, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 2843 and FPS: 30 ---- Normal_Videos_019_x264.mp4
Output video frames: 95, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1862 and FPS: 30 ---- RoadAccidents132_x264.mp4
Output video frames: 63, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 4013 and FPS: 30 ---- Explosion002_x264.mp4
Output video frames: 134, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a car accident where a car exploded and caught fire. Therefore, the activity in the video is Arson.
------------------------------------------ final output: The video shows a car accident where a car exploded and caught fire. Therefore, the activity in the video is Arson.
Current Video Frames: 448 and FPS: 30 ---- Normal_Videos_831_x264.mp4
Output video frames: 15, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1068 and FPS: 30 ---- Normal_Videos_067_x264.mp4
Output video frames: 36, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 8642 and FPS: 30 ---- Arrest030_x264.mp4
Output video frames: 289, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 624 and FPS: 30 ---- Normal_Videos_867_x264.mp4
Output video frames: 21, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 2756 and FPS: 30 ---- Shooting019_x264.mp4
Output video frames: 92, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 4226 and FPS: 30 ---- Normal_Videos_439_x264.mp4
Output video frames: 141, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 627 and FPS: 30 ---- Normal_Videos_100_x264.mp4
Output video frames: 21, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 2498 and FPS: 30 ---- Explosion010_x264.mp4
Output video frames: 84, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a forest fire burning in the woods.
------------------------------------------ final output: The video shows a forest fire burning in the woods.
Current Video Frames: 7170 and FPS: 30 ---- Normal_Videos_944_x264.mp4
Output video frames: 239, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 2744 and FPS: 30 ---- Normal_Videos_801_x264.mp4
Output video frames: 92, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 790 and FPS: 30 ---- Normal_Videos_903_x264.mp4
Output video frames: 27, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1125 and FPS: 30 ---- Burglary018_x264.mp4
Output video frames: 38, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 389 and FPS: 30 ---- RoadAccidents004_x264.mp4
Output video frames: 13, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 908 and FPS: 30 ---- Normal_Videos_904_x264.mp4
Output video frames: 31, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 716 and FPS: 30 ---- RoadAccidents022_x264.mp4
Output video frames: 24, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 4993 and FPS: 30 ---- Normal_Videos_246_x264.mp4
Output video frames: 167, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 7726 and FPS: 30 ---- Normal_Videos_925_x264.mp4
Output video frames: 258, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 351 and FPS: 30 ---- Normal_Videos_876_x264.mp4
Output video frames: 12, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1430 and FPS: 30 ---- Shooting007_x264.mp4
Output video frames: 48, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1245 and FPS: 30 ---- Normal_Videos_915_x264.mp4
Output video frames: 42, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 2575 and FPS: 30 ---- Normal_Videos_894_x264.mp4
Output video frames: 86, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 5543 and FPS: 30 ---- Normal_Videos_782_x264.mp4
Output video frames: 185, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1713 and FPS: 30 ---- Shooting015_x264.mp4
Output video frames: 58, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 2374 and FPS: 30 ---- Arrest001_x264.mp4
Output video frames: 80, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 2229 and FPS: 30 ---- Normal_Videos_597_x264.mp4
Output video frames: 75, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 3277 and FPS: 30 ---- Normal_Videos_603_x264.mp4
Output video frames: 110, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a gas station with cars and people, and a man is seen pumping gas into his car. There is no evidence of any abuse, arrest, arson, assault, burglary, explosion, fighting, robbery, road accident, shooting, or normal video in the video.
------------------------------------------ final output: The video shows a gas station with cars and people, and a man is seen pumping gas into his car. There is no evidence of any abuse, arrest, arson, assault, burglary, explosion, fighting, robbery, road accident, shooting, or normal video in the video.
Current Video Frames: 1384 and FPS: 30 ---- Normal_Videos_902_x264.mp4
Output video frames: 47, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1765 and FPS: 30 ---- Normal_Videos_934_x264.mp4
Output video frames: 59, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 36017 and FPS: 30 ---- Normal_Videos_940_x264.mp4
Output video frames: 1201, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a man walking around a store and looking at different items. He then proceeds to walk around the store again and looks at more items.
------------------------------------------ final output: The video shows a man walking around a store and looking at different items. He then proceeds to walk around the store again and looks at more items.
Current Video Frames: 209 and FPS: 30 ---- Normal_Videos_345_x264.mp4
Output video frames: 7, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 14853 and FPS: 30 ---- Burglary079_x264.mp4
Output video frames: 496, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Abuse
------------------------------------------ final output: Abuse
Current Video Frames: 2580 and FPS: 30 ---- RoadAccidents127_x264.mp4
Output video frames: 86, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Road Accident
------------------------------------------ final output: Road Accident
Current Video Frames: 1143 and FPS: 30 ---- Normal_Videos_879_x264.mp4
Output video frames: 39, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 1800 and FPS: 30 ---- Normal_Videos_891_x264.mp4
Output video frames: 60, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video
Current Video Frames: 12923 and FPS: 30 ---- Burglary076_x264.mp4
Output video frames: 431, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: The video shows a man walking up to a house and looking inside. He then walks around the yard and enters the house. Based on this information, the activity in the video can be classified as Normal Video.
------------------------------------------ final output: The video shows a man walking up to a house and looking inside. He then walks around the yard and enters the house. Based on this information, the activity in the video can be classified as Normal Video.
Current Video Frames: 4094 and FPS: 30 ---- Normal_Videos_182_x264.mp4
Output video frames: 137, fps: 1
------------------------------------------ question: You will perform anomaly detection on a surveillance camera video. Classify the activity in the video by using ONLY ONE of the mentioned categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Robbery, Road Accident, Shooting, or Normal Video.
------------------------------------------ initial output: Normal Video
------------------------------------------ final output: Normal Video

JOB STATISTICS
==============
Job ID: 6326396
Cluster: snellius
User/Group: scur0405/scur0405
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 72
CPU Utilized: 00:23:09
CPU Efficiency: 4.28% of 09:01:12 core-walltime
Job Wall-clock time: 00:07:31
Memory Utilized: 32.20 GB
Memory Efficiency: 6.71% of 480.00 GB
