============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[2024-05-22 09:52:39,600] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 09:52:45,323] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-05-22 09:52:45,324] [INFO] [runner.py:555:main] cmd = /home/scur0405/.conda/envs/llamavid/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llamavid/train/train_mem.py --deepspeed ./scripts/zero2_offload.json --model_name_or_path ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 --version imgsp_v1 --data_path ./data/LLaMA-VID-Finetune/animal-grounding/train_grounding_animals.json --image_folder ./data/LLaMA-VID-Finetune --video_folder ./data/LLaMA-VID-Finetune --vision_tower ./model_zoo/LAVIS/eva_vit_g.pth --image_processor ./llamavid/processor/clip-patch14-224 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length False --video_fps 1 --video_token 2 --bert_type qformer_pretrain_freeze_all --num_query 32 --compress_type mean --bf16 True --output_dir ./work_dirs/llama-vid-7b-full-224-video-fps-1-finetuning-grounding-animals-scratch --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 200 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 65536 --gradient_checkpointing True --dataloader_num_workers 1 --lazy_preprocess True --report_to wandb
[2024-05-22 09:52:48,722] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 09:52:53,543] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-22 09:52:53,543] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-22 09:52:53,543] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-22 09:52:53,544] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-22 09:52:53,544] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-22 09:53:00,995] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 09:53:01,212] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 09:53:01,236] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-22 09:53:01,267] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
[2024-05-22 09:53:03,453] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-22 09:53:03,454] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-22 09:53:03,602] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-22 09:53:03,603] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-22 09:53:03,659] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-22 09:53:03,659] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-22 09:53:03,728] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-22 09:53:03,728] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-22 09:53:03,728] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.54s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_query', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_val_projector.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_ln.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.42s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_ln.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_key_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.52s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_key_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.05s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_ln.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.bias']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Loading pretrained weights...
Loading pretrained weights...
Loading pretrained weights...
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Formatting inputs...Skip in lazy mode
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.340935230255127 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.3678967952728271 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.410508155822754 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.4038903713226318 seconds
Rank: 0 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 3 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 2 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 1 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
wandb: Currently logged in as: antonios-tragoudaras (tonytragoudaras). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /gpfs/home4/scur0405/LLaMA-VID/wandb/run-20240522_095851-5jr0mvpm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-silence-20
wandb: ⭐️ View project at https://wandb.ai/tonytragoudaras/huggingface
wandb: 🚀 View run at https://wandb.ai/tonytragoudaras/huggingface/runs/5jr0mvpm
  0%|          | 0/436 [00:00<?, ?it/s]  0%|          | 1/436 [00:28<3:27:57, 28.68s/it]                                                 {'loss': 3.9453, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.0}
  0%|          | 1/436 [00:28<3:27:57, 28.68s/it]  0%|          | 2/436 [00:36<1:57:48, 16.29s/it]                                                 {'loss': 3.4883, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.0}
  0%|          | 2/436 [00:36<1:57:48, 16.29s/it]  1%|          | 3/436 [00:43<1:28:31, 12.27s/it]                                                 {'loss': 3.4512, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.01}
  1%|          | 3/436 [00:43<1:28:31, 12.27s/it]  1%|          | 4/436 [00:51<1:15:15, 10.45s/it]                                                 {'loss': 3.1465, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.01}
  1%|          | 4/436 [00:51<1:15:15, 10.45s/it]  1%|          | 5/436 [00:58<1:07:06,  9.34s/it]                                                 {'loss': 3.2891, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.01}
  1%|          | 5/436 [00:58<1:07:06,  9.34s/it]  1%|▏         | 6/436 [01:06<1:02:44,  8.76s/it]                                                 {'loss': 2.7705, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.01}
  1%|▏         | 6/436 [01:06<1:02:44,  8.76s/it]  2%|▏         | 7/436 [01:14<59:52,  8.37s/it]                                                 {'loss': 2.4043, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|▏         | 7/436 [01:14<59:52,  8.37s/it]  2%|▏         | 8/436 [01:21<58:07,  8.15s/it]                                               {'loss': 1.8887, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.02}
  2%|▏         | 8/436 [01:21<58:07,  8.15s/it]  2%|▏         | 9/436 [01:29<57:01,  8.01s/it]                                               {'loss': 1.6738, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.02}
  2%|▏         | 9/436 [01:29<57:01,  8.01s/it]  2%|▏         | 10/436 [01:37<57:03,  8.04s/it]                                                {'loss': 1.7861, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.02}
  2%|▏         | 10/436 [01:37<57:03,  8.04s/it]  3%|▎         | 11/436 [01:44<55:08,  7.79s/it]                                                {'loss': 1.6655, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.03}
  3%|▎         | 11/436 [01:44<55:08,  7.79s/it]  3%|▎         | 12/436 [01:52<54:37,  7.73s/it]                                                {'loss': 1.3423, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.03}
  3%|▎         | 12/436 [01:52<54:37,  7.73s/it]  3%|▎         | 13/436 [01:59<53:48,  7.63s/it]                                                {'loss': 1.3174, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.03}
  3%|▎         | 13/436 [01:59<53:48,  7.63s/it]  3%|▎         | 14/436 [02:07<54:38,  7.77s/it]                                                {'loss': 1.0835, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|▎         | 14/436 [02:07<54:38,  7.77s/it]  3%|▎         | 15/436 [02:15<53:29,  7.62s/it]                                                {'loss': 1.373, 'learning_rate': 1.9999722895969904e-05, 'epoch': 0.03}
  3%|▎         | 15/436 [02:15<53:29,  7.62s/it]  4%|▎         | 16/436 [02:22<53:17,  7.61s/it]                                                {'loss': 1.1611, 'learning_rate': 1.999889159923694e-05, 'epoch': 0.04}
  4%|▎         | 16/436 [02:22<53:17,  7.61s/it]  4%|▍         | 17/436 [02:30<53:14,  7.62s/it]                                                {'loss': 1.4644, 'learning_rate': 1.9997506155872246e-05, 'epoch': 0.04}
  4%|▍         | 17/436 [02:30<53:14,  7.62s/it]  4%|▍         | 18/436 [02:38<53:37,  7.70s/it]                                                {'loss': 1.1948, 'learning_rate': 1.9995566642658208e-05, 'epoch': 0.04}
  4%|▍         | 18/436 [02:38<53:37,  7.70s/it]  4%|▍         | 19/436 [02:45<53:11,  7.65s/it]                                                {'loss': 1.146, 'learning_rate': 1.999307316708421e-05, 'epoch': 0.04}
  4%|▍         | 19/436 [02:45<53:11,  7.65s/it]  5%|▍         | 20/436 [02:53<52:45,  7.61s/it]                                                {'loss': 1.1257, 'learning_rate': 1.9990025867340683e-05, 'epoch': 0.05}
  5%|▍         | 20/436 [02:53<52:45,  7.61s/it]  5%|▍         | 21/436 [03:00<52:20,  7.57s/it]                                                {'loss': 1.2739, 'learning_rate': 1.998642491231143e-05, 'epoch': 0.05}
  5%|▍         | 21/436 [03:00<52:20,  7.57s/it]  5%|▌         | 22/436 [03:08<52:59,  7.68s/it]                                                {'loss': 1.0728, 'learning_rate': 1.9982270501564286e-05, 'epoch': 0.05}
  5%|▌         | 22/436 [03:08<52:59,  7.68s/it]  5%|▌         | 23/436 [03:16<52:23,  7.61s/it]                                                {'loss': 0.9849, 'learning_rate': 1.997756286534004e-05, 'epoch': 0.05}
  5%|▌         | 23/436 [03:16<52:23,  7.61s/it]  6%|▌         | 24/436 [03:23<51:57,  7.57s/it]                                                {'loss': 0.9333, 'learning_rate': 1.9972302264539686e-05, 'epoch': 0.05}
  6%|▌         | 24/436 [03:23<51:57,  7.57s/it]  6%|▌         | 25/436 [03:31<51:36,  7.53s/it]                                                {'loss': 1.1948, 'learning_rate': 1.996648899070996e-05, 'epoch': 0.06}
  6%|▌         | 25/436 [03:31<51:36,  7.53s/it]  6%|▌         | 26/436 [03:39<52:40,  7.71s/it]                                                {'loss': 1.0034, 'learning_rate': 1.9960123366027187e-05, 'epoch': 0.06}
  6%|▌         | 26/436 [03:39<52:40,  7.71s/it]  6%|▌         | 27/436 [03:46<52:09,  7.65s/it]                                                {'loss': 1.1162, 'learning_rate': 1.995320574327941e-05, 'epoch': 0.06}
  6%|▌         | 27/436 [03:46<52:09,  7.65s/it]  6%|▋         | 28/436 [03:54<51:57,  7.64s/it]                                                {'loss': 1.3301, 'learning_rate': 1.9945736505846866e-05, 'epoch': 0.06}
  6%|▋         | 28/436 [03:54<51:57,  7.64s/it]  7%|▋         | 29/436 [04:02<52:32,  7.75s/it]                                                {'loss': 1.3694, 'learning_rate': 1.9937716067680712e-05, 'epoch': 0.07}
  7%|▋         | 29/436 [04:02<52:32,  7.75s/it]  7%|▋         | 30/436 [04:10<52:26,  7.75s/it]                                                {'loss': 1.1328, 'learning_rate': 1.9929144873280092e-05, 'epoch': 0.07}
  7%|▋         | 30/436 [04:10<52:26,  7.75s/it]  7%|▋         | 31/436 [04:17<51:44,  7.67s/it]                                                {'loss': 1.1724, 'learning_rate': 1.992002339766751e-05, 'epoch': 0.07}
  7%|▋         | 31/436 [04:17<51:44,  7.67s/it]  7%|▋         | 32/436 [04:25<51:26,  7.64s/it]                                                {'loss': 0.8506, 'learning_rate': 1.99103521463625e-05, 'epoch': 0.07}
  7%|▋         | 32/436 [04:25<51:26,  7.64s/it]  8%|▊         | 33/436 [04:32<51:41,  7.70s/it]                                                {'loss': 1.094, 'learning_rate': 1.9900131655353597e-05, 'epoch': 0.08}
  8%|▊         | 33/436 [04:32<51:41,  7.70s/it]  8%|▊         | 34/436 [04:41<52:35,  7.85s/it]                                                {'loss': 1.0449, 'learning_rate': 1.9889362491068658e-05, 'epoch': 0.08}
  8%|▊         | 34/436 [04:41<52:35,  7.85s/it]  8%|▊         | 35/436 [04:48<51:52,  7.76s/it]                                                {'loss': 1.0217, 'learning_rate': 1.9878045250343445e-05, 'epoch': 0.08}
  8%|▊         | 35/436 [04:48<51:52,  7.76s/it]  8%|▊         | 36/436 [04:56<51:13,  7.68s/it]                                                {'loss': 0.9534, 'learning_rate': 1.986618056038856e-05, 'epoch': 0.08}
  8%|▊         | 36/436 [04:56<51:13,  7.68s/it]  8%|▊         | 37/436 [05:04<51:27,  7.74s/it]                                                {'loss': 1.2939, 'learning_rate': 1.9853769078754685e-05, 'epoch': 0.08}
  8%|▊         | 37/436 [05:04<51:27,  7.74s/it]  9%|▊         | 38/436 [05:11<50:30,  7.61s/it]                                                {'loss': 0.8066, 'learning_rate': 1.9840811493296134e-05, 'epoch': 0.09}
  9%|▊         | 38/436 [05:11<50:30,  7.61s/it]  9%|▉         | 39/436 [05:18<49:44,  7.52s/it]                                                {'loss': 1.0083, 'learning_rate': 1.982730852213274e-05, 'epoch': 0.09}
  9%|▉         | 39/436 [05:18<49:44,  7.52s/it]  9%|▉         | 40/436 [05:26<49:43,  7.53s/it]                                                {'loss': 0.9675, 'learning_rate': 1.9813260913610048e-05, 'epoch': 0.09}
  9%|▉         | 40/436 [05:26<49:43,  7.53s/it]  9%|▉         | 41/436 [05:34<50:30,  7.67s/it]                                                {'loss': 0.8613, 'learning_rate': 1.9798669446257844e-05, 'epoch': 0.09}
  9%|▉         | 41/436 [05:34<50:30,  7.67s/it] 10%|▉         | 42/436 [05:41<50:11,  7.64s/it]                                                {'loss': 0.8315, 'learning_rate': 1.9783534928747006e-05, 'epoch': 0.1}
 10%|▉         | 42/436 [05:41<50:11,  7.64s/it] 10%|▉         | 43/436 [05:49<49:44,  7.59s/it]                                                {'loss': 0.9348, 'learning_rate': 1.9767858199844697e-05, 'epoch': 0.1}
 10%|▉         | 43/436 [05:49<49:44,  7.59s/it] 10%|█         | 44/436 [05:56<49:27,  7.57s/it]                                                {'loss': 1.1514, 'learning_rate': 1.9751640128367872e-05, 'epoch': 0.1}
 10%|█         | 44/436 [05:56<49:27,  7.57s/it] 10%|█         | 45/436 [06:04<50:16,  7.71s/it]                                                {'loss': 1.1472, 'learning_rate': 1.973488161313512e-05, 'epoch': 0.1}
 10%|█         | 45/436 [06:04<50:16,  7.71s/it] 11%|█         | 46/436 [06:12<49:49,  7.67s/it]                                                {'loss': 0.8055, 'learning_rate': 1.9717583582916862e-05, 'epoch': 0.11}
 11%|█         | 46/436 [06:12<49:49,  7.67s/it] 11%|█         | 47/436 [06:19<49:30,  7.64s/it]                                                {'loss': 1.1599, 'learning_rate': 1.969974699638388e-05, 'epoch': 0.11}
 11%|█         | 47/436 [06:19<49:30,  7.64s/it] 11%|█         | 48/436 [06:27<49:21,  7.63s/it]                                                {'loss': 1.134, 'learning_rate': 1.968137284205417e-05, 'epoch': 0.11}
 11%|█         | 48/436 [06:27<49:21,  7.63s/it] 11%|█         | 49/436 [06:35<49:25,  7.66s/it]                                                {'loss': 1.0134, 'learning_rate': 1.966246213823818e-05, 'epoch': 0.11}
 11%|█         | 49/436 [06:35<49:25,  7.66s/it] 11%|█▏        | 50/436 [06:43<49:28,  7.69s/it]                                                {'loss': 1.0232, 'learning_rate': 1.9643015932982355e-05, 'epoch': 0.11}
 11%|█▏        | 50/436 [06:43<49:28,  7.69s/it] 12%|█▏        | 51/436 [06:50<49:08,  7.66s/it]                                                {'loss': 1.0894, 'learning_rate': 1.9623035304011062e-05, 'epoch': 0.12}
 12%|█▏        | 51/436 [06:50<49:08,  7.66s/it] 12%|█▏        | 52/436 [06:57<48:19,  7.55s/it]                                                {'loss': 1.0913, 'learning_rate': 1.960252135866687e-05, 'epoch': 0.12}
 12%|█▏        | 52/436 [06:57<48:19,  7.55s/it] 12%|█▏        | 53/436 [07:05<47:47,  7.49s/it]                                                {'loss': 0.9016, 'learning_rate': 1.9581475233849165e-05, 'epoch': 0.12}
 12%|█▏        | 53/436 [07:05<47:47,  7.49s/it] 12%|█▏        | 54/436 [07:12<47:58,  7.53s/it]                                                {'loss': 1.2351, 'learning_rate': 1.9559898095951137e-05, 'epoch': 0.12}
 12%|█▏        | 54/436 [07:12<47:58,  7.53s/it] 13%|█▎        | 55/436 [07:20<47:52,  7.54s/it]                                                {'loss': 0.9624, 'learning_rate': 1.953779114079517e-05, 'epoch': 0.13}
 13%|█▎        | 55/436 [07:20<47:52,  7.54s/it] 13%|█▎        | 56/436 [07:28<47:41,  7.53s/it]                                                {'loss': 1.1746, 'learning_rate': 1.9515155593566536e-05, 'epoch': 0.13}
 13%|█▎        | 56/436 [07:28<47:41,  7.53s/it] 13%|█▎        | 57/436 [07:35<47:28,  7.52s/it]                                                {'loss': 0.9865, 'learning_rate': 1.9491992708745502e-05, 'epoch': 0.13}
 13%|█▎        | 57/436 [07:35<47:28,  7.52s/it] 13%|█▎        | 58/436 [07:43<47:29,  7.54s/it]                                                {'loss': 1.1963, 'learning_rate': 1.946830377003782e-05, 'epoch': 0.13}
 13%|█▎        | 58/436 [07:43<47:29,  7.54s/it] 14%|█▎        | 59/436 [07:50<47:19,  7.53s/it]                                                {'loss': 0.769, 'learning_rate': 1.9444090090303567e-05, 'epoch': 0.14}
 14%|█▎        | 59/436 [07:50<47:19,  7.53s/it] 14%|█▍        | 60/436 [07:58<47:04,  7.51s/it]                                                {'loss': 0.793, 'learning_rate': 1.941935301148439e-05, 'epoch': 0.14}
 14%|█▍        | 60/436 [07:58<47:04,  7.51s/it] 14%|█▍        | 61/436 [08:05<47:40,  7.63s/it]                                                {'loss': 0.8623, 'learning_rate': 1.939409390452913e-05, 'epoch': 0.14}
 14%|█▍        | 61/436 [08:05<47:40,  7.63s/it] 14%|█▍        | 62/436 [08:13<47:45,  7.66s/it]                                                {'loss': 0.7922, 'learning_rate': 1.9368314169317858e-05, 'epoch': 0.14}
 14%|█▍        | 62/436 [08:13<47:45,  7.66s/it] 14%|█▍        | 63/436 [08:21<47:18,  7.61s/it]                                                {'loss': 1.0605, 'learning_rate': 1.9342015234584277e-05, 'epoch': 0.14}
 14%|█▍        | 63/436 [08:21<47:18,  7.61s/it] 15%|█▍        | 64/436 [08:28<47:10,  7.61s/it]                                                {'loss': 0.8564, 'learning_rate': 1.9315198557836555e-05, 'epoch': 0.15}
 15%|█▍        | 64/436 [08:28<47:10,  7.61s/it] 15%|█▍        | 65/436 [08:36<47:22,  7.66s/it]                                                {'loss': 0.991, 'learning_rate': 1.928786562527652e-05, 'epoch': 0.15}
 15%|█▍        | 65/436 [08:36<47:22,  7.66s/it] 15%|█▌        | 66/436 [08:44<47:06,  7.64s/it]                                                {'loss': 0.76, 'learning_rate': 1.9260017951717334e-05, 'epoch': 0.15}
 15%|█▌        | 66/436 [08:44<47:06,  7.64s/it] 15%|█▌        | 67/436 [08:51<46:35,  7.58s/it]                                                {'loss': 0.8391, 'learning_rate': 1.9231657080499507e-05, 'epoch': 0.15}
 15%|█▌        | 67/436 [08:51<46:35,  7.58s/it] 16%|█▌        | 68/436 [08:59<46:35,  7.60s/it]                                                {'loss': 1.0137, 'learning_rate': 1.9202784583405386e-05, 'epoch': 0.16}
 16%|█▌        | 68/436 [08:59<46:35,  7.60s/it] 16%|█▌        | 69/436 [09:06<46:14,  7.56s/it]                                                {'loss': 0.9338, 'learning_rate': 1.9173402060572028e-05, 'epoch': 0.16}
 16%|█▌        | 69/436 [09:06<46:14,  7.56s/it] 16%|█▌        | 70/436 [09:14<46:30,  7.62s/it]                                                {'loss': 0.7654, 'learning_rate': 1.9143511140402532e-05, 'epoch': 0.16}
 16%|█▌        | 70/436 [09:14<46:30,  7.62s/it] 16%|█▋        | 71/436 [09:22<46:20,  7.62s/it]                                                {'loss': 0.6964, 'learning_rate': 1.9113113479475784e-05, 'epoch': 0.16}
 16%|█▋        | 71/436 [09:22<46:20,  7.62s/it] 17%|█▋        | 72/436 [09:29<46:04,  7.59s/it]                                                {'loss': 1.2539, 'learning_rate': 1.908221076245466e-05, 'epoch': 0.16}
 17%|█▋        | 72/436 [09:29<46:04,  7.59s/it] 17%|█▋        | 73/436 [09:37<46:51,  7.75s/it]                                                {'loss': 1.0078, 'learning_rate': 1.905080470199264e-05, 'epoch': 0.17}
 17%|█▋        | 73/436 [09:37<46:51,  7.75s/it] 17%|█▋        | 74/436 [09:45<46:38,  7.73s/it]                                                {'loss': 0.7699, 'learning_rate': 1.901889703863891e-05, 'epoch': 0.17}
 17%|█▋        | 74/436 [09:45<46:38,  7.73s/it] 17%|█▋        | 75/436 [09:53<46:24,  7.71s/it]                                                {'loss': 0.748, 'learning_rate': 1.8986489540741895e-05, 'epoch': 0.17}
 17%|█▋        | 75/436 [09:53<46:24,  7.71s/it] 17%|█▋        | 76/436 [10:00<46:30,  7.75s/it]                                                {'loss': 0.7371, 'learning_rate': 1.8953584004351243e-05, 'epoch': 0.17}
 17%|█▋        | 76/436 [10:00<46:30,  7.75s/it] 18%|█▊        | 77/436 [10:09<47:33,  7.95s/it]                                                {'loss': 0.9055, 'learning_rate': 1.892018225311831e-05, 'epoch': 0.18}
 18%|█▊        | 77/436 [10:09<47:33,  7.95s/it] 18%|█▊        | 78/436 [10:17<47:07,  7.90s/it]                                                {'loss': 0.6365, 'learning_rate': 1.8886286138195063e-05, 'epoch': 0.18}
 18%|█▊        | 78/436 [10:17<47:07,  7.90s/it] 18%|█▊        | 79/436 [10:24<46:20,  7.79s/it]                                                {'loss': 0.781, 'learning_rate': 1.885189753813152e-05, 'epoch': 0.18}
 18%|█▊        | 79/436 [10:24<46:20,  7.79s/it] 18%|█▊        | 80/436 [10:32<46:15,  7.80s/it]                                                {'loss': 0.7671, 'learning_rate': 1.8817018358771612e-05, 'epoch': 0.18}
 18%|█▊        | 80/436 [10:32<46:15,  7.80s/it] 19%|█▊        | 81/436 [10:40<46:11,  7.81s/it]                                                {'loss': 0.8755, 'learning_rate': 1.8781650533147572e-05, 'epoch': 0.19}
 19%|█▊        | 81/436 [10:40<46:11,  7.81s/it] 19%|█▉        | 82/436 [10:47<45:31,  7.72s/it]                                                {'loss': 0.8691, 'learning_rate': 1.87457960213728e-05, 'epoch': 0.19}
 19%|█▉        | 82/436 [10:47<45:31,  7.72s/it] 19%|█▉        | 83/436 [10:55<45:20,  7.71s/it]                                                {'loss': 0.5057, 'learning_rate': 1.8709456810533248e-05, 'epoch': 0.19}
 19%|█▉        | 83/436 [10:55<45:20,  7.71s/it] 19%|█▉        | 84/436 [11:03<45:07,  7.69s/it]                                                {'loss': 0.8052, 'learning_rate': 1.867263491457726e-05, 'epoch': 0.19}
 19%|█▉        | 84/436 [11:03<45:07,  7.69s/it] 19%|█▉        | 85/436 [11:11<45:22,  7.76s/it]                                                {'loss': 1.0432, 'learning_rate': 1.8635332374203993e-05, 'epoch': 0.19}
 19%|█▉        | 85/436 [11:11<45:22,  7.76s/it] 20%|█▉        | 86/436 [11:18<44:58,  7.71s/it]                                                {'loss': 0.7874, 'learning_rate': 1.85975512567503e-05, 'epoch': 0.2}
 20%|█▉        | 86/436 [11:18<44:58,  7.71s/it] 20%|█▉        | 87/436 [11:26<44:32,  7.66s/it]                                                {'loss': 1.0542, 'learning_rate': 1.8559293656076167e-05, 'epoch': 0.2}
 20%|█▉        | 87/436 [11:26<44:32,  7.66s/it] 20%|██        | 88/436 [11:34<44:49,  7.73s/it]                                                {'loss': 0.9827, 'learning_rate': 1.8520561692448655e-05, 'epoch': 0.2}
 20%|██        | 88/436 [11:34<44:49,  7.73s/it] 20%|██        | 89/436 [11:41<44:31,  7.70s/it]                                                {'loss': 1.0166, 'learning_rate': 1.848135751242441e-05, 'epoch': 0.2}
 20%|██        | 89/436 [11:41<44:31,  7.70s/it] 21%|██        | 90/436 [11:49<44:13,  7.67s/it]                                                {'loss': 0.6431, 'learning_rate': 1.8441683288730686e-05, 'epoch': 0.21}
 21%|██        | 90/436 [11:49<44:13,  7.67s/it] 21%|██        | 91/436 [11:56<44:00,  7.65s/it]                                                {'loss': 0.6409, 'learning_rate': 1.840154122014494e-05, 'epoch': 0.21}
 21%|██        | 91/436 [11:56<44:00,  7.65s/it] 21%|██        | 92/436 [12:04<44:06,  7.69s/it]                                                {'loss': 0.8475, 'learning_rate': 1.836093353137297e-05, 'epoch': 0.21}
 21%|██        | 92/436 [12:04<44:06,  7.69s/it] 21%|██▏       | 93/436 [12:12<43:36,  7.63s/it]                                                {'loss': 0.8052, 'learning_rate': 1.831986247292561e-05, 'epoch': 0.21}
 21%|██▏       | 93/436 [12:12<43:36,  7.63s/it] 22%|██▏       | 94/436 [12:19<43:08,  7.57s/it]                                                {'loss': 0.8181, 'learning_rate': 1.8278330320994035e-05, 'epoch': 0.22}
 22%|██▏       | 94/436 [12:19<43:08,  7.57s/it] 22%|██▏       | 95/436 [12:27<42:46,  7.53s/it]                                                {'loss': 0.9331, 'learning_rate': 1.823633937732357e-05, 'epoch': 0.22}
 22%|██▏       | 95/436 [12:27<42:46,  7.53s/it] 22%|██▏       | 96/436 [12:34<42:54,  7.57s/it]                                                {'loss': 0.8641, 'learning_rate': 1.8193891969086164e-05, 'epoch': 0.22}
 22%|██▏       | 96/436 [12:34<42:54,  7.57s/it] 22%|██▏       | 97/436 [12:42<42:58,  7.61s/it]                                                {'loss': 0.689, 'learning_rate': 1.8150990448751393e-05, 'epoch': 0.22}
 22%|██▏       | 97/436 [12:42<42:58,  7.61s/it] 22%|██▏       | 98/436 [12:49<42:31,  7.55s/it]                                                {'loss': 1.0068, 'learning_rate': 1.8107637193956102e-05, 'epoch': 0.22}
 22%|██▏       | 98/436 [12:49<42:31,  7.55s/it] 23%|██▎       | 99/436 [12:57<42:01,  7.48s/it]                                                {'loss': 0.8196, 'learning_rate': 1.8063834607372603e-05, 'epoch': 0.23}
 23%|██▎       | 99/436 [12:57<42:01,  7.48s/it] 23%|██▎       | 100/436 [13:05<42:38,  7.62s/it]                                                 {'loss': 0.7937, 'learning_rate': 1.8019585116575554e-05, 'epoch': 0.23}
 23%|██▎       | 100/436 [13:05<42:38,  7.62s/it] 23%|██▎       | 101/436 [13:12<42:19,  7.58s/it]                                                 {'loss': 0.7661, 'learning_rate': 1.7974891173907406e-05, 'epoch': 0.23}
 23%|██▎       | 101/436 [13:12<42:19,  7.58s/it] 23%|██▎       | 102/436 [13:20<42:07,  7.57s/it]                                                 {'loss': 1.144, 'learning_rate': 1.792975525634248e-05, 'epoch': 0.23}
 23%|██▎       | 102/436 [13:20<42:07,  7.57s/it] 24%|██▎       | 103/436 [13:27<42:04,  7.58s/it]                                                 {'loss': 0.7246, 'learning_rate': 1.7884179865349713e-05, 'epoch': 0.24}
 24%|██▎       | 103/436 [13:27<42:04,  7.58s/it] 24%|██▍       | 104/436 [13:35<41:41,  7.54s/it]                                                 {'loss': 0.8345, 'learning_rate': 1.7838167526754002e-05, 'epoch': 0.24}
 24%|██▍       | 104/436 [13:35<41:41,  7.54s/it] 24%|██▍       | 105/436 [13:42<41:33,  7.53s/it]                                                 {'loss': 0.7119, 'learning_rate': 1.7791720790596242e-05, 'epoch': 0.24}
 24%|██▍       | 105/436 [13:42<41:33,  7.53s/it] 24%|██▍       | 106/436 [13:50<41:25,  7.53s/it]                                                 {'loss': 0.875, 'learning_rate': 1.774484223099199e-05, 'epoch': 0.24}
 24%|██▍       | 106/436 [13:50<41:25,  7.53s/it] 25%|██▍       | 107/436 [13:57<41:26,  7.56s/it]                                                 {'loss': 1.0273, 'learning_rate': 1.7697534445988804e-05, 'epoch': 0.25}
 25%|██▍       | 107/436 [13:57<41:26,  7.56s/it] 25%|██▍       | 108/436 [14:05<41:23,  7.57s/it]                                                 {'loss': 0.9028, 'learning_rate': 1.7649800057422256e-05, 'epoch': 0.25}
 25%|██▍       | 108/436 [14:05<41:23,  7.57s/it] 25%|██▌       | 109/436 [14:12<41:14,  7.57s/it]                                                 {'loss': 1.0586, 'learning_rate': 1.760164171077064e-05, 'epoch': 0.25}
 25%|██▌       | 109/436 [14:13<41:14,  7.57s/it] 25%|██▌       | 110/436 [14:20<40:45,  7.50s/it]                                                 {'loss': 0.7175, 'learning_rate': 1.755306207500834e-05, 'epoch': 0.25}
 25%|██▌       | 110/436 [14:20<40:45,  7.50s/it] 25%|██▌       | 111/436 [14:27<40:38,  7.50s/it]                                                 {'loss': 0.7139, 'learning_rate': 1.750406384245793e-05, 'epoch': 0.25}
 25%|██▌       | 111/436 [14:27<40:38,  7.50s/it] 26%|██▌       | 112/436 [14:35<41:28,  7.68s/it]                                                 {'loss': 0.5583, 'learning_rate': 1.7454649728640944e-05, 'epoch': 0.26}
 26%|██▌       | 112/436 [14:35<41:28,  7.68s/it] 26%|██▌       | 113/436 [14:43<40:59,  7.61s/it]                                                 {'loss': 0.8716, 'learning_rate': 1.7404822472127406e-05, 'epoch': 0.26}
 26%|██▌       | 113/436 [14:43<40:59,  7.61s/it] 26%|██▌       | 114/436 [14:51<40:53,  7.62s/it]                                                 {'loss': 0.9727, 'learning_rate': 1.7354584834384036e-05, 'epoch': 0.26}
 26%|██▌       | 114/436 [14:51<40:53,  7.62s/it] 26%|██▋       | 115/436 [14:58<40:42,  7.61s/it]                                                 {'loss': 0.6951, 'learning_rate': 1.73039395996212e-05, 'epoch': 0.26}
 26%|██▋       | 115/436 [14:58<40:42,  7.61s/it] 27%|██▋       | 116/436 [15:06<40:12,  7.54s/it]                                                 {'loss': 0.5952, 'learning_rate': 1.725288957463864e-05, 'epoch': 0.27}
 27%|██▋       | 116/436 [15:06<40:12,  7.54s/it] 27%|██▋       | 117/436 [15:13<39:52,  7.50s/it]                                                 {'loss': 1.0051, 'learning_rate': 1.720143758866988e-05, 'epoch': 0.27}
 27%|██▋       | 117/436 [15:13<39:52,  7.50s/it] 27%|██▋       | 118/436 [15:20<39:50,  7.52s/it]                                                 {'loss': 0.832, 'learning_rate': 1.7149586493225453e-05, 'epoch': 0.27}
 27%|██▋       | 118/436 [15:20<39:50,  7.52s/it] 27%|██▋       | 119/436 [15:28<39:32,  7.48s/it]                                                 {'loss': 0.571, 'learning_rate': 1.709733916193487e-05, 'epoch': 0.27}
 27%|██▋       | 119/436 [15:28<39:32,  7.48s/it] 28%|██▊       | 120/436 [15:36<39:45,  7.55s/it]                                                 {'loss': 0.8438, 'learning_rate': 1.704469849038734e-05, 'epoch': 0.27}
 28%|██▊       | 120/436 [15:36<39:45,  7.55s/it] 28%|██▊       | 121/436 [15:43<39:21,  7.50s/it]                                                 {'loss': 0.7068, 'learning_rate': 1.6991667395971306e-05, 'epoch': 0.28}
 28%|██▊       | 121/436 [15:43<39:21,  7.50s/it] 28%|██▊       | 122/436 [15:50<38:53,  7.43s/it]                                                 {'loss': 0.7012, 'learning_rate': 1.6938248817712767e-05, 'epoch': 0.28}
 28%|██▊       | 122/436 [15:50<38:53,  7.43s/it] 28%|██▊       | 123/436 [15:58<38:42,  7.42s/it]                                                 {'loss': 0.5677, 'learning_rate': 1.6884445716112388e-05, 'epoch': 0.28}
 28%|██▊       | 123/436 [15:58<38:42,  7.42s/it] 28%|██▊       | 124/436 [16:06<39:30,  7.60s/it]                                                 {'loss': 0.8423, 'learning_rate': 1.6830261072981423e-05, 'epoch': 0.28}
 28%|██▊       | 124/436 [16:06<39:30,  7.60s/it] 29%|██▊       | 125/436 [16:13<38:51,  7.50s/it]                                                 {'loss': 0.7582, 'learning_rate': 1.677569789127647e-05, 'epoch': 0.29}
 29%|██▊       | 125/436 [16:13<38:51,  7.50s/it] 29%|██▉       | 126/436 [16:20<38:44,  7.50s/it]                                                 {'loss': 0.8506, 'learning_rate': 1.6720759194933037e-05, 'epoch': 0.29}
 29%|██▉       | 126/436 [16:20<38:44,  7.50s/it] 29%|██▉       | 127/436 [16:28<38:37,  7.50s/it]                                                 {'loss': 0.6714, 'learning_rate': 1.666544802869796e-05, 'epoch': 0.29}
 29%|██▉       | 127/436 [16:28<38:37,  7.50s/it] 29%|██▉       | 128/436 [16:36<38:39,  7.53s/it]                                                 {'loss': 0.6826, 'learning_rate': 1.660976745796065e-05, 'epoch': 0.29}
 29%|██▉       | 128/436 [16:36<38:39,  7.53s/it] 30%|██▉       | 129/436 [16:43<38:16,  7.48s/it]                                                 {'loss': 0.6422, 'learning_rate': 1.655372056858322e-05, 'epoch': 0.3}
 30%|██▉       | 129/436 [16:43<38:16,  7.48s/it] 30%|██▉       | 130/436 [16:50<38:04,  7.46s/it]                                                 {'loss': 0.9601, 'learning_rate': 1.6497310466729448e-05, 'epoch': 0.3}
 30%|██▉       | 130/436 [16:50<38:04,  7.46s/it] 30%|███       | 131/436 [16:58<38:01,  7.48s/it]                                                 {'loss': 1.0352, 'learning_rate': 1.6440540278692656e-05, 'epoch': 0.3}
 30%|███       | 131/436 [16:58<38:01,  7.48s/it] 30%|███       | 132/436 [17:06<38:19,  7.56s/it]                                                 {'loss': 0.8641, 'learning_rate': 1.6383413150722417e-05, 'epoch': 0.3}
 30%|███       | 132/436 [17:06<38:19,  7.56s/it] 31%|███       | 133/436 [17:13<37:57,  7.52s/it]                                                 {'loss': 0.7756, 'learning_rate': 1.6325932248850206e-05, 'epoch': 0.3}
 31%|███       | 133/436 [17:13<37:57,  7.52s/it] 31%|███       | 134/436 [17:20<37:48,  7.51s/it]                                                 {'loss': 0.9961, 'learning_rate': 1.626810075871394e-05, 'epoch': 0.31}
 31%|███       | 134/436 [17:20<37:48,  7.51s/it] 31%|███       | 135/436 [17:28<37:52,  7.55s/it]                                                 {'loss': 0.9348, 'learning_rate': 1.6209921885381418e-05, 'epoch': 0.31}
 31%|███       | 135/436 [17:28<37:52,  7.55s/it] 31%|███       | 136/436 [17:36<37:41,  7.54s/it]                                                 {'loss': 0.6655, 'learning_rate': 1.615139885317269e-05, 'epoch': 0.31}
 31%|███       | 136/436 [17:36<37:41,  7.54s/it] 31%|███▏      | 137/436 [17:43<37:28,  7.52s/it]                                                 {'loss': 0.8491, 'learning_rate': 1.6092534905481367e-05, 'epoch': 0.31}
 31%|███▏      | 137/436 [17:43<37:28,  7.52s/it] 32%|███▏      | 138/436 [17:51<37:21,  7.52s/it]                                                 {'loss': 0.6422, 'learning_rate': 1.6033333304594886e-05, 'epoch': 0.32}
 32%|███▏      | 138/436 [17:51<37:21,  7.52s/it] 32%|███▏      | 139/436 [17:58<37:12,  7.52s/it]                                                 {'loss': 0.6572, 'learning_rate': 1.5973797331513674e-05, 'epoch': 0.32}
 32%|███▏      | 139/436 [17:58<37:12,  7.52s/it] 32%|███▏      | 140/436 [18:06<37:18,  7.56s/it]                                                 {'loss': 0.6901, 'learning_rate': 1.5913930285769356e-05, 'epoch': 0.32}
 32%|███▏      | 140/436 [18:06<37:18,  7.56s/it] 32%|███▏      | 141/436 [18:14<37:24,  7.61s/it]                                                 {'loss': 0.6902, 'learning_rate': 1.5853735485241858e-05, 'epoch': 0.32}
 32%|███▏      | 141/436 [18:14<37:24,  7.61s/it] 33%|███▎      | 142/436 [18:21<36:58,  7.55s/it]                                                 {'loss': 0.9419, 'learning_rate': 1.579321626597554e-05, 'epoch': 0.33}
 33%|███▎      | 142/436 [18:21<36:58,  7.55s/it] 33%|███▎      | 143/436 [18:28<36:38,  7.50s/it]                                                 {'loss': 0.6411, 'learning_rate': 1.573237598199432e-05, 'epoch': 0.33}
 33%|███▎      | 143/436 [18:28<36:38,  7.50s/it] 33%|███▎      | 144/436 [18:36<36:39,  7.53s/it]                                                 {'loss': 0.6792, 'learning_rate': 1.5671218005115767e-05, 'epoch': 0.33}
 33%|███▎      | 144/436 [18:36<36:39,  7.53s/it] 33%|███▎      | 145/436 [18:43<36:29,  7.52s/it]                                                 {'loss': 0.6091, 'learning_rate': 1.5609745724764264e-05, 'epoch': 0.33}
 33%|███▎      | 145/436 [18:43<36:29,  7.52s/it] 33%|███▎      | 146/436 [18:51<36:22,  7.53s/it]                                                 {'loss': 0.8079, 'learning_rate': 1.5547962547783126e-05, 'epoch': 0.33}
 33%|███▎      | 146/436 [18:51<36:22,  7.53s/it] 34%|███▎      | 147/436 [18:58<36:03,  7.49s/it]                                                 {'loss': 0.613, 'learning_rate': 1.5485871898245824e-05, 'epoch': 0.34}
 34%|███▎      | 147/436 [18:58<36:03,  7.49s/it] 34%|███▍      | 148/436 [19:06<35:52,  7.47s/it]                                                 {'loss': 0.6255, 'learning_rate': 1.54234772172662e-05, 'epoch': 0.34}
 34%|███▍      | 148/436 [19:06<35:52,  7.47s/it] 34%|███▍      | 149/436 [19:13<35:35,  7.44s/it]                                                 {'loss': 0.5535, 'learning_rate': 1.536078196280777e-05, 'epoch': 0.34}
 34%|███▍      | 149/436 [19:13<35:35,  7.44s/it] 34%|███▍      | 150/436 [19:21<35:40,  7.49s/it]                                                 {'loss': 0.7649, 'learning_rate': 1.5297789609492062e-05, 'epoch': 0.34}
 34%|███▍      | 150/436 [19:21<35:40,  7.49s/it] 35%|███▍      | 151/436 [19:28<35:43,  7.52s/it]                                                 {'loss': 0.8386, 'learning_rate': 1.5234503648406075e-05, 'epoch': 0.35}
 35%|███▍      | 151/436 [19:28<35:43,  7.52s/it] 35%|███▍      | 152/436 [19:36<35:36,  7.52s/it]                                                 {'loss': 0.552, 'learning_rate': 1.5170927586908787e-05, 'epoch': 0.35}
 35%|███▍      | 152/436 [19:36<35:36,  7.52s/it] 35%|███▌      | 153/436 [19:44<35:38,  7.56s/it]                                                 {'loss': 0.7693, 'learning_rate': 1.5107064948436758e-05, 'epoch': 0.35}
 35%|███▌      | 153/436 [19:44<35:38,  7.56s/it] 35%|███▌      | 154/436 [19:51<35:13,  7.49s/it]                                                 {'loss': 0.5244, 'learning_rate': 1.5042919272308895e-05, 'epoch': 0.35}
 35%|███▌      | 154/436 [19:51<35:13,  7.49s/it] 36%|███▌      | 155/436 [19:58<35:05,  7.49s/it]                                                 {'loss': 0.6255, 'learning_rate': 1.4978494113530268e-05, 'epoch': 0.36}
 36%|███▌      | 155/436 [19:58<35:05,  7.49s/it] 36%|███▌      | 156/436 [20:06<35:33,  7.62s/it]                                                 {'loss': 0.682, 'learning_rate': 1.4913793042595109e-05, 'epoch': 0.36}
 36%|███▌      | 156/436 [20:06<35:33,  7.62s/it] 36%|███▌      | 157/436 [20:14<35:25,  7.62s/it]                                                 {'loss': 0.738, 'learning_rate': 1.4848819645288915e-05, 'epoch': 0.36}
 36%|███▌      | 157/436 [20:14<35:25,  7.62s/it] 36%|███▌      | 158/436 [20:22<35:19,  7.63s/it]                                                 {'loss': 0.5997, 'learning_rate': 1.4783577522489733e-05, 'epoch': 0.36}
 36%|███▌      | 158/436 [20:22<35:19,  7.63s/it] 36%|███▋      | 159/436 [20:29<34:55,  7.57s/it]                                                 {'loss': 0.6887, 'learning_rate': 1.4718070289968602e-05, 'epoch': 0.36}
 36%|███▋      | 159/436 [20:29<34:55,  7.57s/it] 37%|███▋      | 160/436 [20:37<35:28,  7.71s/it]                                                 {'loss': 0.6643, 'learning_rate': 1.4652301578189141e-05, 'epoch': 0.37}
 37%|███▋      | 160/436 [20:37<35:28,  7.71s/it] 37%|███▋      | 161/436 [20:44<34:54,  7.61s/it]                                                 {'loss': 0.4584, 'learning_rate': 1.4586275032106373e-05, 'epoch': 0.37}
 37%|███▋      | 161/436 [20:44<34:54,  7.61s/it] 37%|███▋      | 162/436 [20:52<35:03,  7.68s/it]                                                 {'loss': 0.6864, 'learning_rate': 1.4519994310964697e-05, 'epoch': 0.37}
 37%|███▋      | 162/436 [20:52<35:03,  7.68s/it] 37%|███▋      | 163/436 [21:00<34:35,  7.60s/it]                                                 {'loss': 0.7258, 'learning_rate': 1.4453463088095108e-05, 'epoch': 0.37}
 37%|███▋      | 163/436 [21:00<34:35,  7.60s/it] 38%|███▊      | 164/436 [21:07<34:39,  7.65s/it]                                                 {'loss': 0.8149, 'learning_rate': 1.4386685050711593e-05, 'epoch': 0.38}
 38%|███▊      | 164/436 [21:07<34:39,  7.65s/it] 38%|███▊      | 165/436 [21:15<34:26,  7.63s/it]                                                 {'loss': 0.6753, 'learning_rate': 1.4319663899706818e-05, 'epoch': 0.38}
 38%|███▊      | 165/436 [21:15<34:26,  7.63s/it] 38%|███▊      | 166/436 [21:23<34:17,  7.62s/it]                                                 {'loss': 0.7793, 'learning_rate': 1.4252403349446986e-05, 'epoch': 0.38}
 38%|███▊      | 166/436 [21:23<34:17,  7.62s/it] 38%|███▊      | 167/436 [21:30<33:52,  7.56s/it]                                                 {'loss': 0.6656, 'learning_rate': 1.4184907127566006e-05, 'epoch': 0.38}
 38%|███▊      | 167/436 [21:30<33:52,  7.56s/it] 39%|███▊      | 168/436 [21:38<34:27,  7.71s/it]                                                 {'loss': 0.4451, 'learning_rate': 1.4117178974758903e-05, 'epoch': 0.38}
 39%|███▊      | 168/436 [21:38<34:27,  7.71s/it] 39%|███▉      | 169/436 [21:45<33:45,  7.59s/it]                                                 {'loss': 0.3541, 'learning_rate': 1.404922264457449e-05, 'epoch': 0.39}
 39%|███▉      | 169/436 [21:45<33:45,  7.59s/it] 39%|███▉      | 170/436 [21:53<33:26,  7.54s/it]                                                 {'loss': 0.7153, 'learning_rate': 1.3981041903207364e-05, 'epoch': 0.39}
 39%|███▉      | 170/436 [21:53<33:26,  7.54s/it] 39%|███▉      | 171/436 [22:00<33:21,  7.55s/it]                                                 {'loss': 0.988, 'learning_rate': 1.3912640529289163e-05, 'epoch': 0.39}
 39%|███▉      | 171/436 [22:00<33:21,  7.55s/it] 39%|███▉      | 172/436 [22:08<33:46,  7.68s/it]                                                 {'loss': 0.6323, 'learning_rate': 1.3844022313679167e-05, 'epoch': 0.39}
 39%|███▉      | 172/436 [22:08<33:46,  7.68s/it] 40%|███▉      | 173/436 [22:16<33:31,  7.65s/it]                                                 {'loss': 0.886, 'learning_rate': 1.3775191059254185e-05, 'epoch': 0.4}
 40%|███▉      | 173/436 [22:16<33:31,  7.65s/it] 40%|███▉      | 174/436 [22:23<33:07,  7.59s/it]                                                 {'loss': 0.8796, 'learning_rate': 1.3706150580697826e-05, 'epoch': 0.4}
 40%|███▉      | 174/436 [22:23<33:07,  7.59s/it] 40%|████      | 175/436 [22:31<32:44,  7.53s/it]                                                 {'loss': 0.6885, 'learning_rate': 1.3636904704289053e-05, 'epoch': 0.4}
 40%|████      | 175/436 [22:31<32:44,  7.53s/it] 40%|████      | 176/436 [22:38<32:42,  7.55s/it]                                                 {'loss': 0.6113, 'learning_rate': 1.3567457267690152e-05, 'epoch': 0.4}
 40%|████      | 176/436 [22:38<32:42,  7.55s/it] 41%|████      | 177/436 [22:46<32:22,  7.50s/it]                                                 {'loss': 0.6902, 'learning_rate': 1.3497812119734037e-05, 'epoch': 0.41}
 41%|████      | 177/436 [22:46<32:22,  7.50s/it] 41%|████      | 178/436 [22:53<32:29,  7.56s/it]                                                 {'loss': 0.834, 'learning_rate': 1.342797312021094e-05, 'epoch': 0.41}
 41%|████      | 178/436 [22:53<32:29,  7.56s/it] 41%|████      | 179/436 [23:01<32:23,  7.56s/it]                                                 {'loss': 0.6375, 'learning_rate': 1.3357944139654508e-05, 'epoch': 0.41}
 41%|████      | 179/436 [23:01<32:23,  7.56s/it] 41%|████▏     | 180/436 [23:09<32:53,  7.71s/it]                                                 {'loss': 0.6675, 'learning_rate': 1.3287729059127288e-05, 'epoch': 0.41}
 41%|████▏     | 180/436 [23:09<32:53,  7.71s/it] 42%|████▏     | 181/436 [23:17<32:30,  7.65s/it]                                                 {'loss': 0.5701, 'learning_rate': 1.3217331770005639e-05, 'epoch': 0.41}
 42%|████▏     | 181/436 [23:17<32:30,  7.65s/it] 42%|████▏     | 182/436 [23:24<32:05,  7.58s/it]                                                 {'loss': 0.4351, 'learning_rate': 1.3146756173764061e-05, 'epoch': 0.42}
 42%|████▏     | 182/436 [23:24<32:05,  7.58s/it] 42%|████▏     | 183/436 [23:32<32:13,  7.64s/it]                                                 {'loss': 0.5515, 'learning_rate': 1.3076006181758989e-05, 'epoch': 0.42}
 42%|████▏     | 183/436 [23:32<32:13,  7.64s/it] 42%|████▏     | 184/436 [23:40<32:39,  7.78s/it]                                                 {'loss': 0.8201, 'learning_rate': 1.3005085715012003e-05, 'epoch': 0.42}
 42%|████▏     | 184/436 [23:40<32:39,  7.78s/it] 42%|████▏     | 185/436 [23:47<32:10,  7.69s/it]                                                 {'loss': 0.6777, 'learning_rate': 1.2933998703992531e-05, 'epoch': 0.42}
 42%|████▏     | 185/436 [23:47<32:10,  7.69s/it] 43%|████▎     | 186/436 [23:55<31:45,  7.62s/it]                                                 {'loss': 0.7467, 'learning_rate': 1.2862749088400026e-05, 'epoch': 0.43}
 43%|████▎     | 186/436 [23:55<31:45,  7.62s/it] 43%|████▎     | 187/436 [24:03<32:10,  7.75s/it]                                                 {'loss': 0.9668, 'learning_rate': 1.279134081694561e-05, 'epoch': 0.43}
 43%|████▎     | 187/436 [24:03<32:10,  7.75s/it] 43%|████▎     | 188/436 [24:11<32:10,  7.78s/it]                                                 {'loss': 0.9886, 'learning_rate': 1.2719777847133241e-05, 'epoch': 0.43}
 43%|████▎     | 188/436 [24:11<32:10,  7.78s/it] 43%|████▎     | 189/436 [24:18<31:52,  7.74s/it]                                                 {'loss': 0.7866, 'learning_rate': 1.2648064145040392e-05, 'epoch': 0.43}
 43%|████▎     | 189/436 [24:18<31:52,  7.74s/it] 44%|████▎     | 190/436 [24:26<31:38,  7.72s/it]                                                 {'loss': 0.8057, 'learning_rate': 1.2576203685098233e-05, 'epoch': 0.44}
 44%|████▎     | 190/436 [24:26<31:38,  7.72s/it] 44%|████▍     | 191/436 [24:35<32:32,  7.97s/it]                                                 {'loss': 0.432, 'learning_rate': 1.2504200449871378e-05, 'epoch': 0.44}
 44%|████▍     | 191/436 [24:35<32:32,  7.97s/it] 44%|████▍     | 192/436 [24:42<31:50,  7.83s/it]                                                 {'loss': 0.6887, 'learning_rate': 1.2432058429837153e-05, 'epoch': 0.44}
 44%|████▍     | 192/436 [24:42<31:50,  7.83s/it] 44%|████▍     | 193/436 [24:50<31:29,  7.77s/it]                                                 {'loss': 0.6289, 'learning_rate': 1.2359781623164465e-05, 'epoch': 0.44}
 44%|████▍     | 193/436 [24:50<31:29,  7.77s/it] 44%|████▍     | 194/436 [24:57<31:06,  7.71s/it]                                                 {'loss': 0.766, 'learning_rate': 1.2287374035492184e-05, 'epoch': 0.44}
 44%|████▍     | 194/436 [24:57<31:06,  7.71s/it] 45%|████▍     | 195/436 [25:05<30:49,  7.68s/it]                                                 {'loss': 0.4388, 'learning_rate': 1.2214839679707193e-05, 'epoch': 0.45}
 45%|████▍     | 195/436 [25:05<30:49,  7.68s/it] 45%|████▍     | 196/436 [25:13<30:45,  7.69s/it]                                                 {'loss': 0.9062, 'learning_rate': 1.2142182575721946e-05, 'epoch': 0.45}
 45%|████▍     | 196/436 [25:13<30:45,  7.69s/it] 45%|████▌     | 197/436 [25:20<30:24,  7.63s/it]                                                 {'loss': 0.661, 'learning_rate': 1.2069406750251713e-05, 'epoch': 0.45}
 45%|████▌     | 197/436 [25:20<30:24,  7.63s/it] 45%|████▌     | 198/436 [25:28<30:11,  7.61s/it]                                                 {'loss': 0.5718, 'learning_rate': 1.1996516236591398e-05, 'epoch': 0.45}
 45%|████▌     | 198/436 [25:28<30:11,  7.61s/it] 46%|████▌     | 199/436 [25:36<30:25,  7.70s/it]                                                 {'loss': 0.7773, 'learning_rate': 1.1923515074392022e-05, 'epoch': 0.46}
 46%|████▌     | 199/436 [25:36<30:25,  7.70s/it] 46%|████▌     | 200/436 [25:43<29:52,  7.59s/it]                                                 {'loss': 0.7239, 'learning_rate': 1.1850407309436831e-05, 'epoch': 0.46}
 46%|████▌     | 200/436 [25:43<29:52,  7.59s/it]/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 46%|████▌     | 201/436 [27:43<2:41:29, 41.23s/it]                                                   {'loss': 0.6125, 'learning_rate': 1.1777196993417087e-05, 'epoch': 0.46}
 46%|████▌     | 201/436 [27:43<2:41:29, 41.23s/it] 46%|████▋     | 202/436 [27:50<2:01:15, 31.09s/it]                                                   {'loss': 0.542, 'learning_rate': 1.1703888183707513e-05, 'epoch': 0.46}
 46%|████▋     | 202/436 [27:50<2:01:15, 31.09s/it] 47%|████▋     | 203/436 [27:58<1:33:14, 24.01s/it]                                                   {'loss': 0.6146, 'learning_rate': 1.1630484943141428e-05, 'epoch': 0.47}
 47%|████▋     | 203/436 [27:58<1:33:14, 24.01s/it] 47%|████▋     | 204/436 [28:05<1:14:00, 19.14s/it]                                                   {'loss': 0.5737, 'learning_rate': 1.1556991339785595e-05, 'epoch': 0.47}
 47%|████▋     | 204/436 [28:05<1:14:00, 19.14s/it] 47%|████▋     | 205/436 [28:13<1:00:28, 15.71s/it]                                                   {'loss': 0.6958, 'learning_rate': 1.1483411446714744e-05, 'epoch': 0.47}
 47%|████▋     | 205/436 [28:13<1:00:28, 15.71s/it] 47%|████▋     | 206/436 [28:21<50:47, 13.25s/it]                                                   {'loss': 0.7671, 'learning_rate': 1.1409749341785859e-05, 'epoch': 0.47}
 47%|████▋     | 206/436 [28:21<50:47, 13.25s/it] 47%|████▋     | 207/436 [28:28<44:07, 11.56s/it]                                                 {'loss': 0.603, 'learning_rate': 1.1336009107412162e-05, 'epoch': 0.47}
 47%|████▋     | 207/436 [28:28<44:07, 11.56s/it] 48%|████▊     | 208/436 [28:36<39:04, 10.28s/it]                                                 {'loss': 0.5775, 'learning_rate': 1.1262194830336888e-05, 'epoch': 0.48}
 48%|████▊     | 208/436 [28:36<39:04, 10.28s/it] 48%|████▊     | 209/436 [28:43<35:41,  9.43s/it]                                                 {'loss': 0.8296, 'learning_rate': 1.118831060140676e-05, 'epoch': 0.48}
 48%|████▊     | 209/436 [28:43<35:41,  9.43s/it] 48%|████▊     | 210/436 [28:50<33:03,  8.78s/it]                                                 {'loss': 0.5101, 'learning_rate': 1.1114360515345301e-05, 'epoch': 0.48}
 48%|████▊     | 210/436 [28:50<33:03,  8.78s/it] 48%|████▊     | 211/436 [28:58<31:35,  8.42s/it]                                                 {'loss': 0.8511, 'learning_rate': 1.1040348670525889e-05, 'epoch': 0.48}
 48%|████▊     | 211/436 [28:58<31:35,  8.42s/it] 49%|████▊     | 212/436 [29:06<30:44,  8.24s/it]                                                 {'loss': 0.5989, 'learning_rate': 1.096627916874461e-05, 'epoch': 0.49}
 49%|████▊     | 212/436 [29:06<30:44,  8.24s/it] 49%|████▉     | 213/436 [29:13<29:46,  8.01s/it]                                                 {'loss': 0.5701, 'learning_rate': 1.0892156114992963e-05, 'epoch': 0.49}
 49%|████▉     | 213/436 [29:13<29:46,  8.01s/it] 49%|████▉     | 214/436 [29:20<28:53,  7.81s/it]                                                 {'loss': 0.6526, 'learning_rate': 1.0817983617230326e-05, 'epoch': 0.49}
 49%|████▉     | 214/436 [29:20<28:53,  7.81s/it] 49%|████▉     | 215/436 [29:28<28:20,  7.69s/it]                                                 {'loss': 0.7168, 'learning_rate': 1.0743765786156313e-05, 'epoch': 0.49}
 49%|████▉     | 215/436 [29:28<28:20,  7.69s/it] 50%|████▉     | 216/436 [29:35<28:05,  7.66s/it]                                                 {'loss': 0.7017, 'learning_rate': 1.066950673498294e-05, 'epoch': 0.49}
 50%|████▉     | 216/436 [29:35<28:05,  7.66s/it] 50%|████▉     | 217/436 [29:43<27:50,  7.63s/it]                                                 {'loss': 0.6938, 'learning_rate': 1.0595210579206676e-05, 'epoch': 0.5}
 50%|████▉     | 217/436 [29:43<27:50,  7.63s/it] 50%|█████     | 218/436 [29:50<27:27,  7.56s/it]                                                 {'loss': 0.627, 'learning_rate': 1.0520881436380366e-05, 'epoch': 0.5}
 50%|█████     | 218/436 [29:50<27:27,  7.56s/it] 50%|█████     | 219/436 [29:58<27:15,  7.53s/it]                                                 {'loss': 0.6196, 'learning_rate': 1.0446523425885008e-05, 'epoch': 0.5}
 50%|█████     | 219/436 [29:58<27:15,  7.53s/it] 50%|█████     | 220/436 [30:06<27:15,  7.57s/it]                                                 {'loss': 0.5952, 'learning_rate': 1.0372140668701483e-05, 'epoch': 0.5}
 50%|█████     | 220/436 [30:06<27:15,  7.57s/it] 51%|█████     | 221/436 [30:13<27:07,  7.57s/it]                                                 {'loss': 0.6713, 'learning_rate': 1.0297737287182144e-05, 'epoch': 0.51}
 51%|█████     | 221/436 [30:13<27:07,  7.57s/it] 51%|█████     | 222/436 [30:20<26:48,  7.52s/it]                                                 {'loss': 0.5016, 'learning_rate': 1.022331740482237e-05, 'epoch': 0.51}
 51%|█████     | 222/436 [30:20<26:48,  7.52s/it] 51%|█████     | 223/436 [30:28<26:41,  7.52s/it]                                                 {'loss': 0.6458, 'learning_rate': 1.014888514603202e-05, 'epoch': 0.51}
 51%|█████     | 223/436 [30:28<26:41,  7.52s/it] 51%|█████▏    | 224/436 [30:36<26:42,  7.56s/it]                                                 {'loss': 0.6199, 'learning_rate': 1.0074444635906875e-05, 'epoch': 0.51}
 51%|█████▏    | 224/436 [30:36<26:42,  7.56s/it] 52%|█████▏    | 225/436 [30:43<26:27,  7.53s/it]                                                 {'loss': 0.7778, 'learning_rate': 1e-05, 'epoch': 0.52}
 52%|█████▏    | 225/436 [30:43<26:27,  7.53s/it] 52%|█████▏    | 226/436 [30:51<26:27,  7.56s/it]                                                 {'loss': 0.4301, 'learning_rate': 9.92555536409313e-06, 'epoch': 0.52}
 52%|█████▏    | 226/436 [30:51<26:27,  7.56s/it] 52%|█████▏    | 227/436 [30:58<26:22,  7.57s/it]                                                 {'loss': 0.674, 'learning_rate': 9.85111485396798e-06, 'epoch': 0.52}
 52%|█████▏    | 227/436 [30:58<26:22,  7.57s/it] 52%|█████▏    | 228/436 [31:06<26:19,  7.60s/it]                                                 {'loss': 0.5758, 'learning_rate': 9.776682595177633e-06, 'epoch': 0.52}
 52%|█████▏    | 228/436 [31:06<26:19,  7.60s/it] 53%|█████▎    | 229/436 [31:13<26:01,  7.54s/it]                                                 {'loss': 0.5234, 'learning_rate': 9.702262712817857e-06, 'epoch': 0.52}
 53%|█████▎    | 229/436 [31:13<26:01,  7.54s/it] 53%|█████▎    | 230/436 [31:21<25:54,  7.55s/it]                                                 {'loss': 0.5942, 'learning_rate': 9.627859331298522e-06, 'epoch': 0.53}
 53%|█████▎    | 230/436 [31:21<25:54,  7.55s/it] 53%|█████▎    | 231/436 [31:28<25:42,  7.53s/it]                                                 {'loss': 0.6022, 'learning_rate': 9.553476574114993e-06, 'epoch': 0.53}
 53%|█████▎    | 231/436 [31:28<25:42,  7.53s/it] 53%|█████▎    | 232/436 [31:36<25:45,  7.58s/it]                                                 {'loss': 0.6299, 'learning_rate': 9.479118563619638e-06, 'epoch': 0.53}
 53%|█████▎    | 232/436 [31:36<25:45,  7.58s/it] 53%|█████▎    | 233/436 [31:44<26:21,  7.79s/it]                                                 {'loss': 0.7566, 'learning_rate': 9.404789420793327e-06, 'epoch': 0.53}
 53%|█████▎    | 233/436 [31:44<26:21,  7.79s/it] 54%|█████▎    | 234/436 [31:52<25:49,  7.67s/it]                                                 {'loss': 0.5209, 'learning_rate': 9.330493265017062e-06, 'epoch': 0.54}
 54%|█████▎    | 234/436 [31:52<25:49,  7.67s/it] 54%|█████▍    | 235/436 [31:59<25:28,  7.60s/it]                                                 {'loss': 0.4127, 'learning_rate': 9.25623421384369e-06, 'epoch': 0.54}
 54%|█████▍    | 235/436 [31:59<25:28,  7.60s/it] 54%|█████▍    | 236/436 [32:07<25:28,  7.64s/it]                                                 {'loss': 0.6973, 'learning_rate': 9.182016382769678e-06, 'epoch': 0.54}
 54%|█████▍    | 236/436 [32:07<25:28,  7.64s/it] 54%|█████▍    | 237/436 [32:15<25:14,  7.61s/it]                                                 {'loss': 0.5995, 'learning_rate': 9.107843885007042e-06, 'epoch': 0.54}
 54%|█████▍    | 237/436 [32:15<25:14,  7.61s/it] 55%|█████▍    | 238/436 [32:22<25:04,  7.60s/it]                                                 {'loss': 0.7513, 'learning_rate': 9.033720831255391e-06, 'epoch': 0.55}
 55%|█████▍    | 238/436 [32:22<25:04,  7.60s/it] 55%|█████▍    | 239/436 [32:30<24:49,  7.56s/it]                                                 {'loss': 0.5229, 'learning_rate': 8.959651329474115e-06, 'epoch': 0.55}
 55%|█████▍    | 239/436 [32:30<24:49,  7.56s/it] 55%|█████▌    | 240/436 [32:37<24:53,  7.62s/it]                                                 {'loss': 0.6528, 'learning_rate': 8.8856394846547e-06, 'epoch': 0.55}
 55%|█████▌    | 240/436 [32:37<24:53,  7.62s/it] 55%|█████▌    | 241/436 [32:45<24:36,  7.57s/it]                                                 {'loss': 0.4888, 'learning_rate': 8.811689398593245e-06, 'epoch': 0.55}
 55%|█████▌    | 241/436 [32:45<24:36,  7.57s/it] 56%|█████▌    | 242/436 [32:53<25:14,  7.81s/it]                                                 {'loss': 0.5342, 'learning_rate': 8.737805169663113e-06, 'epoch': 0.55}
 56%|█████▌    | 242/436 [32:53<25:14,  7.81s/it] 56%|█████▌    | 243/436 [33:01<24:58,  7.76s/it]                                                 {'loss': 0.7058, 'learning_rate': 8.663990892587839e-06, 'epoch': 0.56}
 56%|█████▌    | 243/436 [33:01<24:58,  7.76s/it] 56%|█████▌    | 244/436 [33:09<25:00,  7.81s/it]                                                 {'loss': 0.6104, 'learning_rate': 8.590250658214148e-06, 'epoch': 0.56}
 56%|█████▌    | 244/436 [33:09<25:00,  7.81s/it] 56%|█████▌    | 245/436 [33:16<24:37,  7.74s/it]                                                 {'loss': 0.7395, 'learning_rate': 8.516588553285258e-06, 'epoch': 0.56}
 56%|█████▌    | 245/436 [33:16<24:37,  7.74s/it] 56%|█████▋    | 246/436 [33:24<24:11,  7.64s/it]                                                 {'loss': 0.5918, 'learning_rate': 8.443008660214409e-06, 'epoch': 0.56}
 56%|█████▋    | 246/436 [33:24<24:11,  7.64s/it] 57%|█████▋    | 247/436 [33:32<24:19,  7.72s/it]                                                 {'loss': 0.726, 'learning_rate': 8.369515056858575e-06, 'epoch': 0.57}
 57%|█████▋    | 247/436 [33:32<24:19,  7.72s/it] 57%|█████▋    | 248/436 [33:39<24:10,  7.72s/it]                                                 {'loss': 0.4288, 'learning_rate': 8.296111816292494e-06, 'epoch': 0.57}
 57%|█████▋    | 248/436 [33:39<24:10,  7.72s/it] 57%|█████▋    | 249/436 [33:47<23:49,  7.65s/it]                                                 {'loss': 0.5367, 'learning_rate': 8.222803006582915e-06, 'epoch': 0.57}
 57%|█████▋    | 249/436 [33:47<23:49,  7.65s/it] 57%|█████▋    | 250/436 [33:54<23:29,  7.58s/it]                                                 {'loss': 0.5056, 'learning_rate': 8.149592690563172e-06, 'epoch': 0.57}
 57%|█████▋    | 250/436 [33:54<23:29,  7.58s/it] 58%|█████▊    | 251/436 [34:02<23:45,  7.71s/it]                                                 {'loss': 0.4526, 'learning_rate': 8.076484925607983e-06, 'epoch': 0.58}
 58%|█████▊    | 251/436 [34:02<23:45,  7.71s/it] 58%|█████▊    | 252/436 [34:10<24:08,  7.87s/it]                                                 {'loss': 0.6809, 'learning_rate': 8.003483763408604e-06, 'epoch': 0.58}
 58%|█████▊    | 252/436 [34:11<24:08,  7.87s/it] 58%|█████▊    | 253/436 [34:18<23:34,  7.73s/it]                                                 {'loss': 0.4708, 'learning_rate': 7.930593249748289e-06, 'epoch': 0.58}
 58%|█████▊    | 253/436 [34:18<23:34,  7.73s/it] 58%|█████▊    | 254/436 [34:25<23:09,  7.63s/it]                                                 {'loss': 0.5725, 'learning_rate': 7.857817424278056e-06, 'epoch': 0.58}
 58%|█████▊    | 254/436 [34:25<23:09,  7.63s/it] 58%|█████▊    | 255/436 [34:34<23:37,  7.83s/it]                                                 {'loss': 0.5229, 'learning_rate': 7.785160320292812e-06, 'epoch': 0.58}
 58%|█████▊    | 255/436 [34:34<23:37,  7.83s/it] 59%|█████▊    | 256/436 [34:41<23:03,  7.69s/it]                                                 {'loss': 0.6021, 'learning_rate': 7.712625964507818e-06, 'epoch': 0.59}
 59%|█████▊    | 256/436 [34:41<23:03,  7.69s/it] 59%|█████▉    | 257/436 [34:48<22:42,  7.61s/it]                                                 {'loss': 0.6213, 'learning_rate': 7.64021837683554e-06, 'epoch': 0.59}
 59%|█████▉    | 257/436 [34:48<22:42,  7.61s/it] 59%|█████▉    | 258/436 [34:56<22:25,  7.56s/it]                                                 {'loss': 0.783, 'learning_rate': 7.567941570162849e-06, 'epoch': 0.59}
 59%|█████▉    | 258/436 [34:56<22:25,  7.56s/it] 59%|█████▉    | 259/436 [35:04<22:30,  7.63s/it]                                                 {'loss': 0.7402, 'learning_rate': 7.495799550128625e-06, 'epoch': 0.59}
 59%|█████▉    | 259/436 [35:04<22:30,  7.63s/it] 60%|█████▉    | 260/436 [35:11<22:22,  7.63s/it]                                                 {'loss': 0.5093, 'learning_rate': 7.423796314901769e-06, 'epoch': 0.6}
 60%|█████▉    | 260/436 [35:11<22:22,  7.63s/it] 60%|█████▉    | 261/436 [35:19<22:13,  7.62s/it]                                                 {'loss': 0.7142, 'learning_rate': 7.351935854959608e-06, 'epoch': 0.6}
 60%|█████▉    | 261/436 [35:19<22:13,  7.62s/it] 60%|██████    | 262/436 [35:26<21:57,  7.57s/it]                                                 {'loss': 0.7241, 'learning_rate': 7.2802221528667604e-06, 'epoch': 0.6}
 60%|██████    | 262/436 [35:26<21:57,  7.57s/it] 60%|██████    | 263/436 [35:34<22:08,  7.68s/it]                                                 {'loss': 0.8059, 'learning_rate': 7.208659183054393e-06, 'epoch': 0.6}
 60%|██████    | 263/436 [35:34<22:08,  7.68s/it] 61%|██████    | 264/436 [35:42<21:49,  7.61s/it]                                                 {'loss': 0.5315, 'learning_rate': 7.137250911599978e-06, 'epoch': 0.6}
 61%|██████    | 264/436 [35:42<21:49,  7.61s/it] 61%|██████    | 265/436 [35:49<21:31,  7.55s/it]                                                 {'loss': 0.7781, 'learning_rate': 7.066001296007469e-06, 'epoch': 0.61}
 61%|██████    | 265/436 [35:49<21:31,  7.55s/it] 61%|██████    | 266/436 [35:57<21:24,  7.55s/it]                                                 {'loss': 0.6904, 'learning_rate': 6.9949142849880015e-06, 'epoch': 0.61}
 61%|██████    | 266/436 [35:57<21:24,  7.55s/it] 61%|██████    | 267/436 [36:04<21:24,  7.60s/it]                                                 {'loss': 0.7043, 'learning_rate': 6.9239938182410126e-06, 'epoch': 0.61}
 61%|██████    | 267/436 [36:04<21:24,  7.60s/it] 61%|██████▏   | 268/436 [36:12<21:15,  7.59s/it]                                                 {'loss': 0.5071, 'learning_rate': 6.8532438262359404e-06, 'epoch': 0.61}
 61%|██████▏   | 268/436 [36:12<21:15,  7.59s/it] 62%|██████▏   | 269/436 [36:20<21:12,  7.62s/it]                                                 {'loss': 0.6584, 'learning_rate': 6.7826682299943635e-06, 'epoch': 0.62}
 62%|██████▏   | 269/436 [36:20<21:12,  7.62s/it] 62%|██████▏   | 270/436 [36:28<21:23,  7.73s/it]                                                 {'loss': 0.611, 'learning_rate': 6.712270940872713e-06, 'epoch': 0.62}
 62%|██████▏   | 270/436 [36:28<21:23,  7.73s/it] 62%|██████▏   | 271/436 [36:36<21:36,  7.86s/it]                                                 {'loss': 0.8289, 'learning_rate': 6.642055860345494e-06, 'epoch': 0.62}
 62%|██████▏   | 271/436 [36:36<21:36,  7.86s/it] 62%|██████▏   | 272/436 [36:43<21:16,  7.79s/it]                                                 {'loss': 0.7229, 'learning_rate': 6.572026879789064e-06, 'epoch': 0.62}
 62%|██████▏   | 272/436 [36:43<21:16,  7.79s/it] 63%|██████▎   | 273/436 [36:51<20:51,  7.68s/it]                                                 {'loss': 0.2741, 'learning_rate': 6.502187880265969e-06, 'epoch': 0.63}
 63%|██████▎   | 273/436 [36:51<20:51,  7.68s/it] 63%|██████▎   | 274/436 [36:58<20:38,  7.65s/it]                                                 {'loss': 0.7985, 'learning_rate': 6.43254273230985e-06, 'epoch': 0.63}
 63%|██████▎   | 274/436 [36:58<20:38,  7.65s/it] 63%|██████▎   | 275/436 [37:06<20:24,  7.61s/it]                                                 {'loss': 0.5786, 'learning_rate': 6.36309529571095e-06, 'epoch': 0.63}
 63%|██████▎   | 275/436 [37:06<20:24,  7.61s/it] 63%|██████▎   | 276/436 [37:13<20:07,  7.55s/it]                                                 {'loss': 0.5255, 'learning_rate': 6.293849419302179e-06, 'epoch': 0.63}
 63%|██████▎   | 276/436 [37:13<20:07,  7.55s/it] 64%|██████▎   | 277/436 [37:21<19:48,  7.48s/it]                                                 {'loss': 0.5461, 'learning_rate': 6.224808940745814e-06, 'epoch': 0.63}
 64%|██████▎   | 277/436 [37:21<19:48,  7.48s/it] 64%|██████▍   | 278/436 [37:28<19:40,  7.47s/it]                                                 {'loss': 0.3763, 'learning_rate': 6.155977686320837e-06, 'epoch': 0.64}
 64%|██████▍   | 278/436 [37:28<19:40,  7.47s/it] 64%|██████▍   | 279/436 [37:36<19:40,  7.52s/it]                                                 {'loss': 0.5746, 'learning_rate': 6.087359470710841e-06, 'epoch': 0.64}
 64%|██████▍   | 279/436 [37:36<19:40,  7.52s/it] 64%|██████▍   | 280/436 [37:43<19:42,  7.58s/it]                                                 {'loss': 0.511, 'learning_rate': 6.018958096792642e-06, 'epoch': 0.64}
 64%|██████▍   | 280/436 [37:43<19:42,  7.58s/it] 64%|██████▍   | 281/436 [37:51<19:34,  7.58s/it]                                                 {'loss': 0.5568, 'learning_rate': 5.950777355425511e-06, 'epoch': 0.64}
 64%|██████▍   | 281/436 [37:51<19:34,  7.58s/it] 65%|██████▍   | 282/436 [37:59<19:27,  7.58s/it]                                                 {'loss': 0.4436, 'learning_rate': 5.8828210252411e-06, 'epoch': 0.65}
 65%|██████▍   | 282/436 [37:59<19:27,  7.58s/it] 65%|██████▍   | 283/436 [38:06<19:04,  7.48s/it]                                                 {'loss': 0.5142, 'learning_rate': 5.815092872433994e-06, 'epoch': 0.65}
 65%|██████▍   | 283/436 [38:06<19:04,  7.48s/it] 65%|██████▌   | 284/436 [38:13<18:58,  7.49s/it]                                                 {'loss': 0.6938, 'learning_rate': 5.74759665055302e-06, 'epoch': 0.65}
 65%|██████▌   | 284/436 [38:13<18:58,  7.49s/it] 65%|██████▌   | 285/436 [38:21<18:49,  7.48s/it]                                                 {'loss': 0.6606, 'learning_rate': 5.680336100293182e-06, 'epoch': 0.65}
 65%|██████▌   | 285/436 [38:21<18:49,  7.48s/it] 66%|██████▌   | 286/436 [38:29<18:55,  7.57s/it]                                                 {'loss': 0.6972, 'learning_rate': 5.613314949288409e-06, 'epoch': 0.66}
 66%|██████▌   | 286/436 [38:29<18:55,  7.57s/it] 66%|██████▌   | 287/436 [38:36<18:31,  7.46s/it]                                                 {'loss': 0.434, 'learning_rate': 5.546536911904896e-06, 'epoch': 0.66}
 66%|██████▌   | 287/436 [38:36<18:31,  7.46s/it] 66%|██████▌   | 288/436 [38:43<18:20,  7.44s/it]                                                 {'loss': 0.45, 'learning_rate': 5.4800056890353025e-06, 'epoch': 0.66}
 66%|██████▌   | 288/436 [38:43<18:20,  7.44s/it] 66%|██████▋   | 289/436 [38:51<18:22,  7.50s/it]                                                 {'loss': 0.42, 'learning_rate': 5.4137249678936265e-06, 'epoch': 0.66}
 66%|██████▋   | 289/436 [38:51<18:22,  7.50s/it] 67%|██████▋   | 290/436 [38:58<18:11,  7.48s/it]                                                 {'loss': 0.8101, 'learning_rate': 5.347698421810861e-06, 'epoch': 0.66}
 67%|██████▋   | 290/436 [38:58<18:11,  7.48s/it] 67%|██████▋   | 291/436 [39:07<18:41,  7.74s/it]                                                 {'loss': 0.6056, 'learning_rate': 5.2819297100314e-06, 'epoch': 0.67}
 67%|██████▋   | 291/436 [39:07<18:41,  7.74s/it] 67%|██████▋   | 292/436 [39:14<18:21,  7.65s/it]                                                 {'loss': 0.5645, 'learning_rate': 5.216422477510267e-06, 'epoch': 0.67}
 67%|██████▋   | 292/436 [39:14<18:21,  7.65s/it] 67%|██████▋   | 293/436 [39:22<18:10,  7.63s/it]                                                 {'loss': 0.601, 'learning_rate': 5.151180354711087e-06, 'epoch': 0.67}
 67%|██████▋   | 293/436 [39:22<18:10,  7.63s/it] 67%|██████▋   | 294/436 [39:29<17:56,  7.58s/it]                                                 {'loss': 0.6508, 'learning_rate': 5.0862069574048956e-06, 'epoch': 0.67}
 67%|██████▋   | 294/436 [39:29<17:56,  7.58s/it] 68%|██████▊   | 295/436 [39:37<18:05,  7.70s/it]                                                 {'loss': 0.491, 'learning_rate': 5.021505886469733e-06, 'epoch': 0.68}
 68%|██████▊   | 295/436 [39:37<18:05,  7.70s/it] 68%|██████▊   | 296/436 [39:45<17:50,  7.65s/it]                                                 {'loss': 0.4868, 'learning_rate': 4.957080727691107e-06, 'epoch': 0.68}
 68%|██████▊   | 296/436 [39:45<17:50,  7.65s/it] 68%|██████▊   | 297/436 [39:52<17:36,  7.60s/it]                                                 {'loss': 0.7949, 'learning_rate': 4.892935051563243e-06, 'epoch': 0.68}
 68%|██████▊   | 297/436 [39:52<17:36,  7.60s/it] 68%|██████▊   | 298/436 [40:00<17:41,  7.69s/it]                                                 {'loss': 0.3844, 'learning_rate': 4.829072413091219e-06, 'epoch': 0.68}
 68%|██████▊   | 298/436 [40:00<17:41,  7.69s/it] 69%|██████▊   | 299/436 [40:08<17:56,  7.85s/it]                                                 {'loss': 0.7988, 'learning_rate': 4.765496351593927e-06, 'epoch': 0.68}
 69%|██████▊   | 299/436 [40:08<17:56,  7.85s/it] 69%|██████▉   | 300/436 [40:16<17:45,  7.84s/it]                                                 {'loss': 0.8674, 'learning_rate': 4.7022103905079405e-06, 'epoch': 0.69}
 69%|██████▉   | 300/436 [40:16<17:45,  7.84s/it] 69%|██████▉   | 301/436 [40:24<17:31,  7.79s/it]                                                 {'loss': 0.5513, 'learning_rate': 4.639218037192235e-06, 'epoch': 0.69}
 69%|██████▉   | 301/436 [40:24<17:31,  7.79s/it] 69%|██████▉   | 302/436 [40:31<17:14,  7.72s/it]                                                 {'loss': 0.5499, 'learning_rate': 4.576522782733802e-06, 'epoch': 0.69}
 69%|██████▉   | 302/436 [40:31<17:14,  7.72s/it] 69%|██████▉   | 303/436 [40:39<17:07,  7.72s/it]                                                 {'loss': 0.5215, 'learning_rate': 4.514128101754183e-06, 'epoch': 0.69}
 69%|██████▉   | 303/436 [40:39<17:07,  7.72s/it] 70%|██████▉   | 304/436 [40:47<17:00,  7.73s/it]                                                 {'loss': 0.52, 'learning_rate': 4.45203745221688e-06, 'epoch': 0.7}
 70%|██████▉   | 304/436 [40:47<17:00,  7.73s/it] 70%|██████▉   | 305/436 [40:54<16:42,  7.65s/it]                                                 {'loss': 0.4468, 'learning_rate': 4.3902542752357415e-06, 'epoch': 0.7}
 70%|██████▉   | 305/436 [40:54<16:42,  7.65s/it] 70%|███████   | 306/436 [41:02<16:40,  7.70s/it]                                                 {'loss': 0.6269, 'learning_rate': 4.3287819948842334e-06, 'epoch': 0.7}
 70%|███████   | 306/436 [41:02<16:40,  7.70s/it] 70%|███████   | 307/436 [41:10<16:44,  7.79s/it]                                                 {'loss': 0.7806, 'learning_rate': 4.267624018005686e-06, 'epoch': 0.7}
 70%|███████   | 307/436 [41:10<16:44,  7.79s/it] 71%|███████   | 308/436 [41:18<16:38,  7.80s/it]                                                 {'loss': 0.5978, 'learning_rate': 4.206783734024463e-06, 'epoch': 0.71}
 71%|███████   | 308/436 [41:18<16:38,  7.80s/it] 71%|███████   | 309/436 [41:25<16:25,  7.76s/it]                                                 {'loss': 0.6119, 'learning_rate': 4.1462645147581456e-06, 'epoch': 0.71}
 71%|███████   | 309/436 [41:25<16:25,  7.76s/it] 71%|███████   | 310/436 [41:34<16:31,  7.87s/it]                                                 {'loss': 0.8501, 'learning_rate': 4.086069714230646e-06, 'epoch': 0.71}
 71%|███████   | 310/436 [41:34<16:31,  7.87s/it] 71%|███████▏  | 311/436 [41:41<16:11,  7.77s/it]                                                 {'loss': 0.6698, 'learning_rate': 4.0262026684863295e-06, 'epoch': 0.71}
 71%|███████▏  | 311/436 [41:41<16:11,  7.77s/it] 72%|███████▏  | 312/436 [41:49<15:56,  7.71s/it]                                                 {'loss': 0.623, 'learning_rate': 3.96666669540512e-06, 'epoch': 0.71}
 72%|███████▏  | 312/436 [41:49<15:56,  7.71s/it] 72%|███████▏  | 313/436 [41:56<15:41,  7.65s/it]                                                 {'loss': 0.6534, 'learning_rate': 3.907465094518636e-06, 'epoch': 0.72}
 72%|███████▏  | 313/436 [41:56<15:41,  7.65s/it] 72%|███████▏  | 314/436 [42:04<15:54,  7.82s/it]                                                 {'loss': 0.7539, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72}
 72%|███████▏  | 314/436 [42:04<15:54,  7.82s/it] 72%|███████▏  | 315/436 [42:12<15:35,  7.73s/it]                                                 {'loss': 0.4182, 'learning_rate': 3.790078114618586e-06, 'epoch': 0.72}
 72%|███████▏  | 315/436 [42:12<15:35,  7.73s/it] 72%|███████▏  | 316/436 [42:20<15:28,  7.74s/it]                                                 {'loss': 0.4941, 'learning_rate': 3.731899241286061e-06, 'epoch': 0.72}
 72%|███████▏  | 316/436 [42:20<15:28,  7.74s/it] 73%|███████▎  | 317/436 [42:27<15:14,  7.69s/it]                                                 {'loss': 0.792, 'learning_rate': 3.6740677511497958e-06, 'epoch': 0.73}
 73%|███████▎  | 317/436 [42:27<15:14,  7.69s/it] 73%|███████▎  | 318/436 [42:35<14:58,  7.61s/it]                                                 {'loss': 0.3807, 'learning_rate': 3.616586849277587e-06, 'epoch': 0.73}
 73%|███████▎  | 318/436 [42:35<14:58,  7.61s/it] 73%|███████▎  | 319/436 [42:42<14:37,  7.50s/it]                                                 {'loss': 0.3849, 'learning_rate': 3.559459721307349e-06, 'epoch': 0.73}
 73%|███████▎  | 319/436 [42:42<14:37,  7.50s/it] 73%|███████▎  | 320/436 [42:49<14:30,  7.51s/it]                                                 {'loss': 0.5055, 'learning_rate': 3.5026895332705504e-06, 'epoch': 0.73}
 73%|███████▎  | 320/436 [42:49<14:30,  7.51s/it] 74%|███████▎  | 321/436 [42:57<14:23,  7.50s/it]                                                 {'loss': 0.3914, 'learning_rate': 3.4462794314167846e-06, 'epoch': 0.74}
 74%|███████▎  | 321/436 [42:57<14:23,  7.50s/it] 74%|███████▍  | 322/436 [43:05<14:19,  7.54s/it]                                                 {'loss': 0.5931, 'learning_rate': 3.390232542039352e-06, 'epoch': 0.74}
 74%|███████▍  | 322/436 [43:05<14:19,  7.54s/it] 74%|███████▍  | 323/436 [43:12<14:13,  7.55s/it]                                                 {'loss': 0.5183, 'learning_rate': 3.3345519713020445e-06, 'epoch': 0.74}
 74%|███████▍  | 323/436 [43:12<14:13,  7.55s/it] 74%|███████▍  | 324/436 [43:20<14:06,  7.56s/it]                                                 {'loss': 0.5669, 'learning_rate': 3.2792408050669634e-06, 'epoch': 0.74}
 74%|███████▍  | 324/436 [43:20<14:06,  7.56s/it] 75%|███████▍  | 325/436 [43:27<13:55,  7.53s/it]                                                 {'loss': 0.6245, 'learning_rate': 3.2243021087235336e-06, 'epoch': 0.74}
 75%|███████▍  | 325/436 [43:27<13:55,  7.53s/it] 75%|███████▍  | 326/436 [43:35<13:51,  7.56s/it]                                                 {'loss': 0.505, 'learning_rate': 3.16973892701858e-06, 'epoch': 0.75}
 75%|███████▍  | 326/436 [43:35<13:51,  7.56s/it] 75%|███████▌  | 327/436 [43:43<13:50,  7.61s/it]                                                 {'loss': 0.7007, 'learning_rate': 3.115554283887614e-06, 'epoch': 0.75}
 75%|███████▌  | 327/436 [43:43<13:50,  7.61s/it] 75%|███████▌  | 328/436 [43:50<13:31,  7.52s/it]                                                 {'loss': 0.5483, 'learning_rate': 3.0617511822872337e-06, 'epoch': 0.75}
 75%|███████▌  | 328/436 [43:50<13:31,  7.52s/it] 75%|███████▌  | 329/436 [43:57<13:23,  7.51s/it]                                                 {'loss': 0.5467, 'learning_rate': 3.0083326040286977e-06, 'epoch': 0.75}
 75%|███████▌  | 329/436 [43:57<13:23,  7.51s/it] 76%|███████▌  | 330/436 [44:05<13:21,  7.56s/it]                                                 {'loss': 0.5421, 'learning_rate': 2.9553015096126638e-06, 'epoch': 0.76}
 76%|███████▌  | 330/436 [44:05<13:21,  7.56s/it] 76%|███████▌  | 331/436 [44:12<13:08,  7.51s/it]                                                 {'loss': 0.4762, 'learning_rate': 2.902660838065131e-06, 'epoch': 0.76}
 76%|███████▌  | 331/436 [44:12<13:08,  7.51s/it] 76%|███████▌  | 332/436 [44:20<13:04,  7.54s/it]                                                 {'loss': 0.5938, 'learning_rate': 2.8504135067745463e-06, 'epoch': 0.76}
 76%|███████▌  | 332/436 [44:20<13:04,  7.54s/it] 76%|███████▋  | 333/436 [44:27<12:52,  7.50s/it]                                                 {'loss': 0.4651, 'learning_rate': 2.798562411330126e-06, 'epoch': 0.76}
 76%|███████▋  | 333/436 [44:27<12:52,  7.50s/it] 77%|███████▋  | 334/436 [44:36<13:04,  7.70s/it]                                                 {'loss': 0.6957, 'learning_rate': 2.7471104253613645e-06, 'epoch': 0.77}
 77%|███████▋  | 334/436 [44:36<13:04,  7.70s/it] 77%|███████▋  | 335/436 [44:43<12:57,  7.70s/it]                                                 {'loss': 0.3929, 'learning_rate': 2.6960604003788014e-06, 'epoch': 0.77}
 77%|███████▋  | 335/436 [44:43<12:57,  7.70s/it] 77%|███████▋  | 336/436 [44:51<12:41,  7.61s/it]                                                 {'loss': 0.8394, 'learning_rate': 2.6454151656159666e-06, 'epoch': 0.77}
 77%|███████▋  | 336/436 [44:51<12:41,  7.61s/it] 77%|███████▋  | 337/436 [44:58<12:33,  7.61s/it]                                                 {'loss': 0.3896, 'learning_rate': 2.5951775278725956e-06, 'epoch': 0.77}
 77%|███████▋  | 337/436 [44:58<12:33,  7.61s/it] 78%|███████▊  | 338/436 [45:06<12:37,  7.73s/it]                                                 {'loss': 0.5126, 'learning_rate': 2.545350271359055e-06, 'epoch': 0.77}
 78%|███████▊  | 338/436 [45:06<12:37,  7.73s/it] 78%|███████▊  | 339/436 [45:14<12:28,  7.72s/it]                                                 {'loss': 0.6935, 'learning_rate': 2.495936157542074e-06, 'epoch': 0.78}
 78%|███████▊  | 339/436 [45:14<12:28,  7.72s/it] 78%|███████▊  | 340/436 [45:22<12:22,  7.74s/it]                                                 {'loss': 0.899, 'learning_rate': 2.4469379249916614e-06, 'epoch': 0.78}
 78%|███████▊  | 340/436 [45:22<12:22,  7.74s/it] 78%|███████▊  | 341/436 [45:29<12:09,  7.68s/it]                                                 {'loss': 0.5923, 'learning_rate': 2.3983582892293642e-06, 'epoch': 0.78}
 78%|███████▊  | 341/436 [45:29<12:09,  7.68s/it] 78%|███████▊  | 342/436 [45:37<12:04,  7.71s/it]                                                 {'loss': 0.5265, 'learning_rate': 2.3501999425777433e-06, 'epoch': 0.78}
 78%|███████▊  | 342/436 [45:37<12:04,  7.71s/it] 79%|███████▊  | 343/436 [45:45<11:50,  7.64s/it]                                                 {'loss': 0.4846, 'learning_rate': 2.3024655540111984e-06, 'epoch': 0.79}
 79%|███████▊  | 343/436 [45:45<11:50,  7.64s/it] 79%|███████▉  | 344/436 [45:52<11:43,  7.64s/it]                                                 {'loss': 0.5231, 'learning_rate': 2.255157769008011e-06, 'epoch': 0.79}
 79%|███████▉  | 344/436 [45:52<11:43,  7.64s/it] 79%|███████▉  | 345/436 [46:00<11:33,  7.62s/it]                                                 {'loss': 0.6746, 'learning_rate': 2.2082792094037585e-06, 'epoch': 0.79}
 79%|███████▉  | 345/436 [46:00<11:33,  7.62s/it] 79%|███████▉  | 346/436 [46:07<11:27,  7.63s/it]                                                 {'loss': 0.5432, 'learning_rate': 2.1618324732459993e-06, 'epoch': 0.79}
 79%|███████▉  | 346/436 [46:07<11:27,  7.63s/it] 80%|███████▉  | 347/436 [46:15<11:20,  7.65s/it]                                                 {'loss': 0.4727, 'learning_rate': 2.1158201346502927e-06, 'epoch': 0.79}
 80%|███████▉  | 347/436 [46:15<11:20,  7.65s/it] 80%|███████▉  | 348/436 [46:23<11:08,  7.60s/it]                                                 {'loss': 0.5767, 'learning_rate': 2.0702447436575223e-06, 'epoch': 0.8}
 80%|███████▉  | 348/436 [46:23<11:08,  7.60s/it] 80%|████████  | 349/436 [46:30<10:59,  7.58s/it]                                                 {'loss': 0.5858, 'learning_rate': 2.0251088260925967e-06, 'epoch': 0.8}
 80%|████████  | 349/436 [46:30<10:59,  7.58s/it] 80%|████████  | 350/436 [46:38<11:02,  7.70s/it]                                                 {'loss': 0.5173, 'learning_rate': 1.9804148834244465e-06, 'epoch': 0.8}
 80%|████████  | 350/436 [46:38<11:02,  7.70s/it] 81%|████████  | 351/436 [46:46<10:52,  7.68s/it]                                                 {'loss': 0.7319, 'learning_rate': 1.9361653926274016e-06, 'epoch': 0.8}
 81%|████████  | 351/436 [46:46<10:52,  7.68s/it] 81%|████████  | 352/436 [46:53<10:38,  7.60s/it]                                                 {'loss': 0.5065, 'learning_rate': 1.8923628060439037e-06, 'epoch': 0.81}
 81%|████████  | 352/436 [46:53<10:38,  7.60s/it] 81%|████████  | 353/436 [47:01<10:27,  7.56s/it]                                                 {'loss': 0.6709, 'learning_rate': 1.8490095512486072e-06, 'epoch': 0.81}
 81%|████████  | 353/436 [47:01<10:27,  7.56s/it] 81%|████████  | 354/436 [47:09<10:41,  7.83s/it]                                                 {'loss': 0.3436, 'learning_rate': 1.8061080309138379e-06, 'epoch': 0.81}
 81%|████████  | 354/436 [47:09<10:41,  7.83s/it] 81%|████████▏ | 355/436 [47:17<10:28,  7.76s/it]                                                 {'loss': 0.6803, 'learning_rate': 1.7636606226764353e-06, 'epoch': 0.81}
 81%|████████▏ | 355/436 [47:17<10:28,  7.76s/it] 82%|████████▏ | 356/436 [47:24<10:16,  7.70s/it]                                                 {'loss': 0.7877, 'learning_rate': 1.7216696790059718e-06, 'epoch': 0.82}
 82%|████████▏ | 356/436 [47:24<10:16,  7.70s/it] 82%|████████▏ | 357/436 [47:32<10:10,  7.73s/it]                                                 {'loss': 0.7275, 'learning_rate': 1.6801375270743925e-06, 'epoch': 0.82}
 82%|████████▏ | 357/436 [47:32<10:10,  7.73s/it] 82%|████████▏ | 358/436 [47:40<10:00,  7.70s/it]                                                 {'loss': 0.434, 'learning_rate': 1.6390664686270342e-06, 'epoch': 0.82}
 82%|████████▏ | 358/436 [47:40<10:00,  7.70s/it] 82%|████████▏ | 359/436 [47:47<09:46,  7.62s/it]                                                 {'loss': 0.5621, 'learning_rate': 1.5984587798550633e-06, 'epoch': 0.82}
 82%|████████▏ | 359/436 [47:47<09:46,  7.62s/it] 83%|████████▎ | 360/436 [47:55<09:37,  7.60s/it]                                                 {'loss': 0.7888, 'learning_rate': 1.5583167112693153e-06, 'epoch': 0.82}
 83%|████████▎ | 360/436 [47:55<09:37,  7.60s/it] 83%|████████▎ | 361/436 [48:03<09:45,  7.80s/it]                                                 {'loss': 0.4135, 'learning_rate': 1.518642487575591e-06, 'epoch': 0.83}
 83%|████████▎ | 361/436 [48:03<09:45,  7.80s/it] 83%|████████▎ | 362/436 [48:10<09:30,  7.71s/it]                                                 {'loss': 0.5996, 'learning_rate': 1.4794383075513453e-06, 'epoch': 0.83}
 83%|████████▎ | 362/436 [48:10<09:30,  7.71s/it] 83%|████████▎ | 363/436 [48:18<09:19,  7.67s/it]                                                 {'loss': 0.5479, 'learning_rate': 1.4407063439238333e-06, 'epoch': 0.83}
 83%|████████▎ | 363/436 [48:18<09:19,  7.67s/it] 83%|████████▎ | 364/436 [48:26<09:10,  7.65s/it]                                                 {'loss': 0.9241, 'learning_rate': 1.4024487432497013e-06, 'epoch': 0.83}
 83%|████████▎ | 364/436 [48:26<09:10,  7.65s/it] 84%|████████▎ | 365/436 [48:34<09:09,  7.74s/it]                                                 {'loss': 0.5863, 'learning_rate': 1.36466762579601e-06, 'epoch': 0.84}
 84%|████████▎ | 365/436 [48:34<09:09,  7.74s/it] 84%|████████▍ | 366/436 [48:41<08:57,  7.68s/it]                                                 {'loss': 0.316, 'learning_rate': 1.3273650854227438e-06, 'epoch': 0.84}
 84%|████████▍ | 366/436 [48:41<08:57,  7.68s/it] 84%|████████▍ | 367/436 [48:49<08:45,  7.62s/it]                                                 {'loss': 0.4772, 'learning_rate': 1.2905431894667552e-06, 'epoch': 0.84}
 84%|████████▍ | 367/436 [48:49<08:45,  7.62s/it] 84%|████████▍ | 368/436 [48:56<08:34,  7.57s/it]                                                 {'loss': 0.5902, 'learning_rate': 1.2542039786272008e-06, 'epoch': 0.84}
 84%|████████▍ | 368/436 [48:56<08:34,  7.57s/it] 85%|████████▍ | 369/436 [49:04<08:41,  7.79s/it]                                                 {'loss': 0.6643, 'learning_rate': 1.218349466852432e-06, 'epoch': 0.85}
 85%|████████▍ | 369/436 [49:04<08:41,  7.79s/it] 85%|████████▍ | 370/436 [49:12<08:27,  7.68s/it]                                                 {'loss': 0.6406, 'learning_rate': 1.1829816412283912e-06, 'epoch': 0.85}
 85%|████████▍ | 370/436 [49:12<08:27,  7.68s/it] 85%|████████▌ | 371/436 [49:19<08:14,  7.61s/it]                                                 {'loss': 0.4042, 'learning_rate': 1.1481024618684821e-06, 'epoch': 0.85}
 85%|████████▌ | 371/436 [49:19<08:14,  7.61s/it] 85%|████████▌ | 372/436 [49:27<08:03,  7.56s/it]                                                 {'loss': 0.45, 'learning_rate': 1.1137138618049403e-06, 'epoch': 0.85}
 85%|████████▌ | 372/436 [49:27<08:03,  7.56s/it] 86%|████████▌ | 373/436 [49:34<07:56,  7.56s/it]                                                 {'loss': 0.4978, 'learning_rate': 1.079817746881696e-06, 'epoch': 0.85}
 86%|████████▌ | 373/436 [49:34<07:56,  7.56s/it] 86%|████████▌ | 374/436 [49:42<07:47,  7.54s/it]                                                 {'loss': 0.5317, 'learning_rate': 1.0464159956487596e-06, 'epoch': 0.86}
 86%|████████▌ | 374/436 [49:42<07:47,  7.54s/it] 86%|████████▌ | 375/436 [49:49<07:39,  7.54s/it]                                                 {'loss': 0.5685, 'learning_rate': 1.013510459258108e-06, 'epoch': 0.86}
 86%|████████▌ | 375/436 [49:49<07:39,  7.54s/it] 86%|████████▌ | 376/436 [49:57<07:28,  7.47s/it]                                                 {'loss': 0.5529, 'learning_rate': 9.811029613610913e-07, 'epoch': 0.86}
 86%|████████▌ | 376/436 [49:57<07:28,  7.47s/it] 86%|████████▋ | 377/436 [50:04<07:22,  7.51s/it]                                                 {'loss': 0.4918, 'learning_rate': 9.491952980073604e-07, 'epoch': 0.86}
 86%|████████▋ | 377/436 [50:04<07:22,  7.51s/it] 87%|████████▋ | 378/436 [50:12<07:19,  7.59s/it]                                                 {'loss': 0.5009, 'learning_rate': 9.177892375453413e-07, 'epoch': 0.87}
 87%|████████▋ | 378/436 [50:12<07:19,  7.59s/it] 87%|████████▋ | 379/436 [50:20<07:12,  7.58s/it]                                                 {'loss': 0.4711, 'learning_rate': 8.86886520524216e-07, 'epoch': 0.87}
 87%|████████▋ | 379/436 [50:20<07:12,  7.58s/it] 87%|████████▋ | 380/436 [50:27<07:03,  7.56s/it]                                                 {'loss': 0.3954, 'learning_rate': 8.564888595974718e-07, 'epoch': 0.87}
 87%|████████▋ | 380/436 [50:27<07:03,  7.56s/it] 87%|████████▋ | 381/436 [50:35<06:54,  7.54s/it]                                                 {'loss': 0.579, 'learning_rate': 8.265979394279732e-07, 'epoch': 0.87}
 87%|████████▋ | 381/436 [50:35<06:54,  7.54s/it] 88%|████████▊ | 382/436 [50:42<06:45,  7.52s/it]                                                 {'loss': 0.4573, 'learning_rate': 7.972154165946155e-07, 'epoch': 0.88}
 88%|████████▊ | 382/436 [50:42<06:45,  7.52s/it] 88%|████████▊ | 383/436 [50:50<06:42,  7.60s/it]                                                 {'loss': 0.4508, 'learning_rate': 7.683429195004932e-07, 'epoch': 0.88}
 88%|████████▊ | 383/436 [50:50<06:42,  7.60s/it] 88%|████████▊ | 384/436 [50:57<06:32,  7.56s/it]                                                 {'loss': 0.6062, 'learning_rate': 7.399820482826692e-07, 'epoch': 0.88}
 88%|████████▊ | 384/436 [50:57<06:32,  7.56s/it] 88%|████████▊ | 385/436 [51:05<06:24,  7.54s/it]                                                 {'loss': 0.5509, 'learning_rate': 7.12134374723481e-07, 'epoch': 0.88}
 88%|████████▊ | 385/436 [51:05<06:24,  7.54s/it] 89%|████████▊ | 386/436 [51:12<06:15,  7.51s/it]                                                 {'loss': 0.688, 'learning_rate': 6.848014421634497e-07, 'epoch': 0.88}
 89%|████████▊ | 386/436 [51:12<06:15,  7.51s/it] 89%|████████▉ | 387/436 [51:20<06:09,  7.54s/it]                                                 {'loss': 0.8525, 'learning_rate': 6.579847654157234e-07, 'epoch': 0.89}
 89%|████████▉ | 387/436 [51:20<06:09,  7.54s/it] 89%|████████▉ | 388/436 [51:27<06:01,  7.53s/it]                                                 {'loss': 0.3931, 'learning_rate': 6.316858306821449e-07, 'epoch': 0.89}
 89%|████████▉ | 388/436 [51:27<06:01,  7.53s/it] 89%|████████▉ | 389/436 [51:35<05:53,  7.53s/it]                                                 {'loss': 0.5291, 'learning_rate': 6.05906095470874e-07, 'epoch': 0.89}
 89%|████████▉ | 389/436 [51:35<05:53,  7.53s/it] 89%|████████▉ | 390/436 [51:42<05:43,  7.48s/it]                                                 {'loss': 0.4323, 'learning_rate': 5.806469885156163e-07, 'epoch': 0.89}
 89%|████████▉ | 390/436 [51:42<05:43,  7.48s/it] 90%|████████▉ | 391/436 [51:50<05:37,  7.49s/it]                                                 {'loss': 0.6562, 'learning_rate': 5.55909909696436e-07, 'epoch': 0.9}
 90%|████████▉ | 391/436 [51:50<05:37,  7.49s/it] 90%|████████▉ | 392/436 [51:57<05:28,  7.47s/it]                                                 {'loss': 0.4813, 'learning_rate': 5.316962299621808e-07, 'epoch': 0.9}
 90%|████████▉ | 392/436 [51:57<05:28,  7.47s/it] 90%|█████████ | 393/436 [52:05<05:21,  7.48s/it]                                                 {'loss': 0.5037, 'learning_rate': 5.080072912544987e-07, 'epoch': 0.9}
 90%|█████████ | 393/436 [52:05<05:21,  7.48s/it] 90%|█████████ | 394/436 [52:12<05:15,  7.52s/it]                                                 {'loss': 0.4259, 'learning_rate': 4.848444064334679e-07, 'epoch': 0.9}
 90%|█████████ | 394/436 [52:12<05:15,  7.52s/it] 91%|█████████ | 395/436 [52:20<05:09,  7.54s/it]                                                 {'loss': 0.3702, 'learning_rate': 4.6220885920483014e-07, 'epoch': 0.9}
 91%|█████████ | 395/436 [52:20<05:09,  7.54s/it] 91%|█████████ | 396/436 [52:27<05:00,  7.51s/it]                                                 {'loss': 0.4297, 'learning_rate': 4.401019040488652e-07, 'epoch': 0.91}
 91%|█████████ | 396/436 [52:27<05:00,  7.51s/it] 91%|█████████ | 397/436 [52:36<05:02,  7.75s/it]                                                 {'loss': 0.3851, 'learning_rate': 4.1852476615083957e-07, 'epoch': 0.91}
 91%|█████████ | 397/436 [52:36<05:02,  7.75s/it]slurmstepd: error: *** JOB 6326222 ON gcn59 CANCELLED AT 2024-05-22T10:51:41 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: open failed for /slurm/6326222/.ns: No such file or directory
slurmstepd: error: container_g_join(6326222): No such file or directory

JOB STATISTICS
==============
Job ID: 6326222
Cluster: snellius
User/Group: scur0405/scur0405
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 72
CPU Utilized: 00:00:02
CPU Efficiency: 0.00% of 2-23:10:48 core-walltime
Job Wall-clock time: 00:59:19
Memory Utilized: 239.23 GB
Memory Efficiency: 49.84% of 480.00 GB
