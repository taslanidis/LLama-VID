============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[2024-05-23 04:20:53,108] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:20:58,777] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-05-23 04:20:58,778] [INFO] [runner.py:555:main] cmd = /home/scur0405/.conda/envs/llamavid/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llamavid/train/train_mem.py --deepspeed ./scripts/zero2_offload.json --model_name_or_path ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 --version imgsp_v1 --data_path ./data/LLaMA-VID-Finetune/animal-grounding/train_grounding_animals.json --image_folder ./data/LLaMA-VID-Finetune --video_folder ./data/LLaMA-VID-Finetune --vision_tower ./model_zoo/LAVIS/eva_vit_g.pth --image_processor ./llamavid/processor/clip-patch14-224 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length False --video_fps 6 --video_token 2 --bert_type qformer_pretrain_freeze_all --num_query 32 --compress_type mean --bf16 True --output_dir ./work_dirs/finetuning-grounding-animals-scratch-fps-6-ckpt --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 200 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 65536 --gradient_checkpointing True --dataloader_num_workers 1 --lazy_preprocess True --report_to wandb
[2024-05-23 04:21:02,182] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:07,010] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-23 04:21:07,011] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-23 04:21:07,011] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-23 04:21:07,011] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-23 04:21:07,011] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-23 04:21:14,502] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:14,659] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:14,660] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-23 04:21:14,667] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
no smdistributed.modelparallel.torch
[2024-05-23 04:21:16,942] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:16,942] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:16,943] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-23 04:21:17,090] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:17,090] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:17,094] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:17,094] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-23 04:21:17,147] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-23 04:21:17,147] [INFO] [comm.py:594:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.89s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.73s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_val_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.77s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_key_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_projector.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_query', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.96s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vlm_att_query', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vlm_att_projector.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vlm_att_ln.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_key_projector.weight', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vlm_att_val_projector.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vlm_att_val_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.45s/it]
Some weights of the model checkpoint at ./work_dirs/llama-vid/llama-vid-7b-full-224-video-fps-1 were not used when initializing LlavaLlamaAttForCausalLM: ['model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.21.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.25.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.29.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.vision_tower.vision_tower.blocks.34.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.norm1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.28.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.norm2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.attn.q_bias', 'model.vision_tower.vision_tower.blocks.36.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.attn.v_bias', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.32.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.30.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vlm_att_encoder.bert.embeddings.word_embeddings.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.dense.bias', 'model.vlm_att_encoder.bert.embeddings.position_ids', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.blocks.35.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.30.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output.dense.weight', 'model.vision_tower.vision_tower.blocks.30.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.norm1.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.26.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.q_bias', 'model.vision_tower.vision_tower.blocks.34.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.28.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.33.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.36.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.bias', 'model.vision_tower.vision_tower.blocks.27.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.27.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.28.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.weight', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.vision_tower.vision_tower.blocks.24.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.32.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.29.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.31.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.vision_tower.vision_tower.blocks.36.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.dense.bias', 'model.vision_tower.vision_tower.blocks.14.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.vision_tower.vision_tower.blocks.34.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.32.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'model.vlm_att_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.weight', 'model.vision_tower.vision_tower.blocks.34.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.weight', 'model.vision_tower.vision_tower.blocks.38.attn.v_bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.34.norm1.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.26.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.34.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.query.weight', 'model.vlm_att_query', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.32.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.query.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.29.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.bias', 'model.vision_tower.vision_tower.blocks.24.attn.q_bias', 'model.vision_tower.vision_tower.blocks.28.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.32.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output.dense.bias', 'model.vlm_att_val_projector.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.38.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.37.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.34.norm1.weight', 'model.vlm_att_val_projector.weight', 'model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.25.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.30.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.bias', 'model.vision_tower.vision_tower.cls_token', 'model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.key.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.bias', 'model.vision_tower.vision_tower.blocks.33.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.24.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.26.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.37.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.36.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.30.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.37.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.38.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.norm1.bias', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.32.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.attn.v_bias', 'model.vlm_att_ln.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.38.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.35.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.vision_tower.vision_tower.blocks.27.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.34.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.27.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.fc2.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.29.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.26.norm2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.27.norm2.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.key.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.31.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.26.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.38.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.self.query.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.31.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.bias', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.29.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.29.norm1.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.29.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.36.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.5.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.23.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.query.bias', 'model.vlm_att_key_projector.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.38.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.36.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.33.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.38.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.25.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.blocks.20.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.34.attn.q_bias', 'model.vision_tower.vision_tower.blocks.35.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.27.norm1.weight', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.output.dense.bias', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.36.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.output_query.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.6.crossattention.self.value.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.32.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.27.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.value.weight', 'model.vision_tower.vision_tower.blocks.25.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.key.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.bias', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.11.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.13.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.fc2.bias', 'model.vlm_att_encoder.bert.embeddings.position_embeddings.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.weight', 'model.vision_tower.vision_tower.blocks.31.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.self.query.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.4.output.dense.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.35.attn.q_bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.crossattention.self.query.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.fc1.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.25.norm2.bias', 'model.vision_tower.vision_tower.blocks.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.bias', 'model.vlm_att_key_projector.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.norm1.weight', 'model.vlm_att_encoder.bert.encoder.layer.10.output.dense.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.25.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.blocks.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.query.weight', 'model.vision_tower.vision_tower.blocks.31.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output.dense.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.vlm_att_encoder.bert.embeddings.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.self.value.weight', 'model.vlm_att_encoder.bert.encoder.layer.3.attention.self.value.bias', 'model.vision_tower.vision_tower.blocks.33.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.attention.self.value.bias', 'model.vlm_att_projector.weight', 'model.vision_tower.vision_tower.blocks.37.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.24.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.vision_tower.vision_tower.blocks.15.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.query.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.value.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.25.mlp.fc2.weight', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.self.key.weight', 'model.vlm_att_encoder.bert.encoder.layer.0.attention.self.value.bias', 'model.vlm_att_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vlm_att_ln.bias', 'model.vision_tower.vision_tower.blocks.38.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.16.attn.qkv.weight', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.26.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vlm_att_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'model.vision_tower.vision_tower.blocks.33.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.33.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.37.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.35.norm1.weight', 'model.vision_tower.vision_tower.blocks.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.blocks.24.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.output_query.dense.weight', 'model.vlm_att_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.30.norm2.weight', 'model.vision_tower.vision_tower.blocks.35.mlp.fc1.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.attention.self.key.weight', 'model.vision_tower.vision_tower.blocks.36.attn.qkv.weight', 'model.vision_tower.vision_tower.blocks.28.attn.proj.weight', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.vlm_att_encoder.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.vision_tower.vision_tower.blocks.35.attn.v_bias', 'model.vision_tower.vision_tower.blocks.37.attn.proj.bias', 'model.vlm_att_encoder.bert.encoder.layer.1.output.dense.bias', 'model.vision_tower.vision_tower.blocks.28.norm2.bias', 'model.vlm_att_encoder.bert.encoder.layer.7.attention.self.key.bias', 'model.vlm_att_encoder.bert.encoder.layer.4.crossattention.output.dense.weight']
- This IS expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaAttForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
_IncompatibleKeys(missing_keys=[], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.q_bias', 'blocks.39.attn.v_bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias'])
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Freezing all qformer weights...
Loading pretrained weights...
Loading pretrained weights...
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Loading pretrained weights...
Loading vlm_att_query weights...
Loading vlm_att_ln weights...
Formatting inputs...Skip in lazy mode
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.0346651077270508 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.01228928565979 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.013733148574829 seconds
Using /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home4/scur0405/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.0239925384521484 seconds
Rank: 1 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 0 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 3 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
Rank: 2 partition count [4, 4] and sizes[(1690240000, False), (2048, False)] 
wandb: Currently logged in as: antonios-tragoudaras (tonytragoudaras). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /gpfs/home4/scur0405/LLaMA-VID/wandb/run-20240523_042603-evmumks0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-mountain-25
wandb: ⭐️ View project at https://wandb.ai/tonytragoudaras/huggingface
wandb: 🚀 View run at https://wandb.ai/tonytragoudaras/huggingface/runs/evmumks0
  0%|          | 0/436 [00:00<?, ?it/s]  0%|          | 1/436 [00:42<5:09:17, 42.66s/it]                                                 {'loss': 4.0156, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.0}
  0%|          | 1/436 [00:42<5:09:17, 42.66s/it]  0%|          | 2/436 [00:53<2:53:30, 23.99s/it]                                                 {'loss': 3.584, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.0}
  0%|          | 2/436 [00:53<2:53:30, 23.99s/it]  1%|          | 3/436 [01:02<2:02:58, 17.04s/it]                                                 {'loss': 3.4316, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.01}
  1%|          | 3/436 [01:02<2:02:58, 17.04s/it]  1%|          | 4/436 [01:21<2:08:38, 17.87s/it]                                                 {'loss': 3.1992, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.01}
  1%|          | 4/436 [01:21<2:08:38, 17.87s/it]  1%|          | 5/436 [01:43<2:18:10, 19.24s/it]                                                 {'loss': 3.3457, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.01}
  1%|          | 5/436 [01:43<2:18:10, 19.24s/it]  1%|▏         | 6/436 [01:54<1:57:50, 16.44s/it]                                                 {'loss': 2.7812, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.01}
  1%|▏         | 6/436 [01:54<1:57:50, 16.44s/it]  2%|▏         | 7/436 [02:11<1:59:37, 16.73s/it]                                                 {'loss': 2.3594, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|▏         | 7/436 [02:11<1:59:37, 16.73s/it]  2%|▏         | 8/436 [02:26<1:54:52, 16.10s/it]                                                 {'loss': 1.8652, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.02}
  2%|▏         | 8/436 [02:26<1:54:52, 16.10s/it]  2%|▏         | 9/436 [02:38<1:45:08, 14.77s/it]                                                 {'loss': 1.6787, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.02}
  2%|▏         | 9/436 [02:38<1:45:08, 14.77s/it]  2%|▏         | 10/436 [02:48<1:34:35, 13.32s/it]                                                  {'loss': 1.7871, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.02}
  2%|▏         | 10/436 [02:48<1:34:35, 13.32s/it]  3%|▎         | 11/436 [03:02<1:36:51, 13.67s/it]                                                  {'loss': 1.6362, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.03}
  3%|▎         | 11/436 [03:02<1:36:51, 13.67s/it]  3%|▎         | 12/436 [03:13<1:29:57, 12.73s/it]                                                  {'loss': 1.2676, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.03}
  3%|▎         | 12/436 [03:13<1:29:57, 12.73s/it]  3%|▎         | 13/436 [03:22<1:21:55, 11.62s/it]                                                  {'loss': 1.2058, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.03}
  3%|▎         | 13/436 [03:22<1:21:55, 11.62s/it]  3%|▎         | 14/436 [03:38<1:30:40, 12.89s/it]                                                  {'loss': 1.0874, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|▎         | 14/436 [03:38<1:30:40, 12.89s/it]  3%|▎         | 15/436 [03:46<1:21:34, 11.63s/it]                                                  {'loss': 1.3677, 'learning_rate': 1.9999722895969904e-05, 'epoch': 0.03}
  3%|▎         | 15/436 [03:46<1:21:34, 11.63s/it]  4%|▎         | 16/436 [04:13<1:53:35, 16.23s/it]                                                  {'loss': 1.124, 'learning_rate': 1.999889159923694e-05, 'epoch': 0.04}
  4%|▎         | 16/436 [04:13<1:53:35, 16.23s/it]  4%|▍         | 17/436 [04:25<1:43:43, 14.85s/it]                                                  {'loss': 1.5088, 'learning_rate': 1.9997506155872246e-05, 'epoch': 0.04}
  4%|▍         | 17/436 [04:25<1:43:43, 14.85s/it]  4%|▍         | 18/436 [04:41<1:46:15, 15.25s/it]                                                  {'loss': 1.1853, 'learning_rate': 1.9995566642658208e-05, 'epoch': 0.04}
  4%|▍         | 18/436 [04:41<1:46:15, 15.25s/it]  4%|▍         | 19/436 [04:59<1:52:05, 16.13s/it]                                                  {'loss': 1.1504, 'learning_rate': 1.999307316708421e-05, 'epoch': 0.04}
  4%|▍         | 19/436 [04:59<1:52:05, 16.13s/it]  5%|▍         | 20/436 [05:10<1:40:41, 14.52s/it]                                                  {'loss': 1.1123, 'learning_rate': 1.9990025867340683e-05, 'epoch': 0.05}
  5%|▍         | 20/436 [05:10<1:40:41, 14.52s/it]  5%|▍         | 21/436 [05:29<1:48:58, 15.76s/it]                                                  {'loss': 1.3335, 'learning_rate': 1.998642491231143e-05, 'epoch': 0.05}
  5%|▍         | 21/436 [05:29<1:48:58, 15.76s/it]  5%|▌         | 22/436 [05:46<1:51:58, 16.23s/it]                                                  {'loss': 1.0701, 'learning_rate': 1.9982270501564286e-05, 'epoch': 0.05}
  5%|▌         | 22/436 [05:46<1:51:58, 16.23s/it]  5%|▌         | 23/436 [05:57<1:40:12, 14.56s/it]                                                  {'loss': 1.0093, 'learning_rate': 1.997756286534004e-05, 'epoch': 0.05}
  5%|▌         | 23/436 [05:57<1:40:12, 14.56s/it]  6%|▌         | 24/436 [06:14<1:46:08, 15.46s/it]                                                  {'loss': 0.9253, 'learning_rate': 1.9972302264539686e-05, 'epoch': 0.05}
  6%|▌         | 24/436 [06:14<1:46:08, 15.46s/it]  6%|▌         | 25/436 [06:24<1:34:47, 13.84s/it]                                                  {'loss': 1.2012, 'learning_rate': 1.996648899070996e-05, 'epoch': 0.06}
  6%|▌         | 25/436 [06:24<1:34:47, 13.84s/it]  6%|▌         | 26/436 [06:40<1:38:59, 14.49s/it]                                                  {'loss': 0.9644, 'learning_rate': 1.9960123366027187e-05, 'epoch': 0.06}
  6%|▌         | 26/436 [06:40<1:38:59, 14.49s/it]  6%|▌         | 27/436 [06:53<1:34:47, 13.91s/it]                                                  {'loss': 1.1211, 'learning_rate': 1.995320574327941e-05, 'epoch': 0.06}
  6%|▌         | 27/436 [06:53<1:34:47, 13.91s/it]  6%|▋         | 28/436 [07:06<1:32:40, 13.63s/it]                                                  {'loss': 1.3486, 'learning_rate': 1.9945736505846866e-05, 'epoch': 0.06}
  6%|▋         | 28/436 [07:06<1:32:40, 13.63s/it]  7%|▋         | 29/436 [07:16<1:25:51, 12.66s/it]                                                  {'loss': 1.3884, 'learning_rate': 1.9937716067680712e-05, 'epoch': 0.07}
  7%|▋         | 29/436 [07:16<1:25:51, 12.66s/it]  7%|▋         | 30/436 [07:35<1:38:38, 14.58s/it]                                                  {'loss': 1.1484, 'learning_rate': 1.9929144873280092e-05, 'epoch': 0.07}
  7%|▋         | 30/436 [07:35<1:38:38, 14.58s/it]  7%|▋         | 31/436 [07:46<1:29:43, 13.29s/it]                                                  {'loss': 1.1963, 'learning_rate': 1.992002339766751e-05, 'epoch': 0.07}
  7%|▋         | 31/436 [07:46<1:29:43, 13.29s/it]  7%|▋         | 32/436 [08:04<1:40:56, 14.99s/it]                                                  {'loss': 0.8389, 'learning_rate': 1.99103521463625e-05, 'epoch': 0.07}
  7%|▋         | 32/436 [08:05<1:40:56, 14.99s/it]  8%|▊         | 33/436 [08:24<1:49:01, 16.23s/it]                                                  {'loss': 1.1672, 'learning_rate': 1.9900131655353597e-05, 'epoch': 0.08}
  8%|▊         | 33/436 [08:24<1:49:01, 16.23s/it]  8%|▊         | 34/436 [08:39<1:47:16, 16.01s/it]                                                  {'loss': 1.05, 'learning_rate': 1.9889362491068658e-05, 'epoch': 0.08}
  8%|▊         | 34/436 [08:39<1:47:16, 16.01s/it]  8%|▊         | 35/436 [08:50<1:36:53, 14.50s/it]                                                  {'loss': 1.0493, 'learning_rate': 1.9878045250343445e-05, 'epoch': 0.08}
  8%|▊         | 35/436 [08:50<1:36:53, 14.50s/it]  8%|▊         | 36/436 [09:03<1:32:50, 13.93s/it]                                                  {'loss': 0.9495, 'learning_rate': 1.986618056038856e-05, 'epoch': 0.08}
  8%|▊         | 36/436 [09:03<1:32:50, 13.93s/it]  8%|▊         | 37/436 [09:18<1:35:38, 14.38s/it]                                                  {'loss': 1.2866, 'learning_rate': 1.9853769078754685e-05, 'epoch': 0.08}
  8%|▊         | 37/436 [09:18<1:35:38, 14.38s/it]  9%|▊         | 38/436 [09:27<1:25:27, 12.88s/it]                                                  {'loss': 0.801, 'learning_rate': 1.9840811493296134e-05, 'epoch': 0.09}
  9%|▊         | 38/436 [09:27<1:25:27, 12.88s/it]  9%|▉         | 39/436 [09:37<1:18:31, 11.87s/it]                                                  {'loss': 1.0312, 'learning_rate': 1.982730852213274e-05, 'epoch': 0.09}
  9%|▉         | 39/436 [09:37<1:18:31, 11.87s/it]  9%|▉         | 40/436 [09:56<1:33:26, 14.16s/it]                                                  {'loss': 0.9047, 'learning_rate': 1.9813260913610048e-05, 'epoch': 0.09}
  9%|▉         | 40/436 [09:56<1:33:26, 14.16s/it]  9%|▉         | 41/436 [10:08<1:27:02, 13.22s/it]                                                  {'loss': 0.8289, 'learning_rate': 1.9798669446257844e-05, 'epoch': 0.09}
  9%|▉         | 41/436 [10:08<1:27:02, 13.22s/it] 10%|▉         | 42/436 [10:18<1:20:31, 12.26s/it]                                                  {'loss': 0.8201, 'learning_rate': 1.9783534928747006e-05, 'epoch': 0.1}
 10%|▉         | 42/436 [10:18<1:20:31, 12.26s/it] 10%|▉         | 43/436 [10:30<1:20:08, 12.24s/it]                                                  {'loss': 0.9119, 'learning_rate': 1.9767858199844697e-05, 'epoch': 0.1}
 10%|▉         | 43/436 [10:30<1:20:08, 12.24s/it] 10%|█         | 44/436 [10:44<1:24:09, 12.88s/it]                                                  {'loss': 1.1499, 'learning_rate': 1.9751640128367872e-05, 'epoch': 0.1}
 10%|█         | 44/436 [10:44<1:24:09, 12.88s/it] 10%|█         | 45/436 [10:57<1:23:01, 12.74s/it]                                                  {'loss': 1.1274, 'learning_rate': 1.973488161313512e-05, 'epoch': 0.1}
 10%|█         | 45/436 [10:57<1:23:01, 12.74s/it] 11%|█         | 46/436 [11:16<1:36:41, 14.88s/it]                                                  {'loss': 0.8036, 'learning_rate': 1.9717583582916862e-05, 'epoch': 0.11}
 11%|█         | 46/436 [11:16<1:36:41, 14.88s/it] 11%|█         | 47/436 [11:27<1:27:24, 13.48s/it]                                                  {'loss': 1.2131, 'learning_rate': 1.969974699638388e-05, 'epoch': 0.11}
 11%|█         | 47/436 [11:27<1:27:24, 13.48s/it] 11%|█         | 48/436 [11:44<1:33:56, 14.53s/it]                                                  {'loss': 1.1292, 'learning_rate': 1.968137284205417e-05, 'epoch': 0.11}
 11%|█         | 48/436 [11:44<1:33:56, 14.53s/it] 11%|█         | 49/436 [12:06<1:49:19, 16.95s/it]                                                  {'loss': 1.0339, 'learning_rate': 1.966246213823818e-05, 'epoch': 0.11}
 11%|█         | 49/436 [12:06<1:49:19, 16.95s/it] 11%|█▏        | 50/436 [12:22<1:46:48, 16.60s/it]                                                  {'loss': 0.947, 'learning_rate': 1.9643015932982355e-05, 'epoch': 0.11}
 11%|█▏        | 50/436 [12:22<1:46:48, 16.60s/it] 12%|█▏        | 51/436 [12:37<1:43:19, 16.10s/it]                                                  {'loss': 1.0474, 'learning_rate': 1.9623035304011062e-05, 'epoch': 0.12}
 12%|█▏        | 51/436 [12:37<1:43:19, 16.10s/it] 12%|█▏        | 52/436 [12:46<1:29:27, 13.98s/it]                                                  {'loss': 1.0593, 'learning_rate': 1.960252135866687e-05, 'epoch': 0.12}
 12%|█▏        | 52/436 [12:46<1:29:27, 13.98s/it] 12%|█▏        | 53/436 [13:00<1:30:07, 14.12s/it]                                                  {'loss': 0.8926, 'learning_rate': 1.9581475233849165e-05, 'epoch': 0.12}
 12%|█▏        | 53/436 [13:00<1:30:07, 14.12s/it] 12%|█▏        | 54/436 [13:12<1:25:16, 13.39s/it]                                                  {'loss': 1.2202, 'learning_rate': 1.9559898095951137e-05, 'epoch': 0.12}
 12%|█▏        | 54/436 [13:12<1:25:16, 13.39s/it] 13%|█▎        | 55/436 [13:25<1:24:07, 13.25s/it]                                                  {'loss': 0.9478, 'learning_rate': 1.953779114079517e-05, 'epoch': 0.13}
 13%|█▎        | 55/436 [13:25<1:24:07, 13.25s/it] 13%|█▎        | 56/436 [13:47<1:40:23, 15.85s/it]                                                  {'loss': 1.1477, 'learning_rate': 1.9515155593566536e-05, 'epoch': 0.13}
 13%|█▎        | 56/436 [13:47<1:40:23, 15.85s/it] 13%|█▎        | 57/436 [13:57<1:29:05, 14.11s/it]                                                  {'loss': 0.9653, 'learning_rate': 1.9491992708745502e-05, 'epoch': 0.13}
 13%|█▎        | 57/436 [13:57<1:29:05, 14.11s/it] 13%|█▎        | 58/436 [14:12<1:31:07, 14.47s/it]                                                  {'loss': 1.189, 'learning_rate': 1.946830377003782e-05, 'epoch': 0.13}
 13%|█▎        | 58/436 [14:12<1:31:07, 14.47s/it] 14%|█▎        | 59/436 [14:33<1:43:17, 16.44s/it]                                                  {'loss': 0.814, 'learning_rate': 1.9444090090303567e-05, 'epoch': 0.14}
 14%|█▎        | 59/436 [14:33<1:43:17, 16.44s/it] 14%|█▍        | 60/436 [14:44<1:31:18, 14.57s/it]                                                  {'loss': 0.8438, 'learning_rate': 1.941935301148439e-05, 'epoch': 0.14}
 14%|█▍        | 60/436 [14:44<1:31:18, 14.57s/it] 14%|█▍        | 61/436 [15:08<1:49:02, 17.45s/it]                                                  {'loss': 0.8533, 'learning_rate': 1.939409390452913e-05, 'epoch': 0.14}
 14%|█▍        | 61/436 [15:08<1:49:02, 17.45s/it] 14%|█▍        | 62/436 [15:19<1:37:42, 15.68s/it]                                                  {'loss': 0.7764, 'learning_rate': 1.9368314169317858e-05, 'epoch': 0.14}
 14%|█▍        | 62/436 [15:19<1:37:42, 15.68s/it] 14%|█▍        | 63/436 [15:42<1:50:31, 17.78s/it]                                                  {'loss': 1.063, 'learning_rate': 1.9342015234584277e-05, 'epoch': 0.14}
 14%|█▍        | 63/436 [15:42<1:50:31, 17.78s/it] 15%|█▍        | 64/436 [15:59<1:48:57, 17.57s/it]                                                  {'loss': 0.9644, 'learning_rate': 1.9315198557836555e-05, 'epoch': 0.15}
 15%|█▍        | 64/436 [15:59<1:48:57, 17.57s/it] 15%|█▍        | 65/436 [16:17<1:48:38, 17.57s/it]                                                  {'loss': 0.9722, 'learning_rate': 1.928786562527652e-05, 'epoch': 0.15}
 15%|█▍        | 65/436 [16:17<1:48:38, 17.57s/it] 15%|█▌        | 66/436 [16:34<1:47:21, 17.41s/it]                                                  {'loss': 0.7866, 'learning_rate': 1.9260017951717334e-05, 'epoch': 0.15}
 15%|█▌        | 66/436 [16:34<1:47:21, 17.41s/it] 15%|█▌        | 67/436 [16:44<1:34:42, 15.40s/it]                                                  {'loss': 0.8828, 'learning_rate': 1.9231657080499507e-05, 'epoch': 0.15}
 15%|█▌        | 67/436 [16:44<1:34:42, 15.40s/it] 16%|█▌        | 68/436 [17:01<1:36:08, 15.68s/it]                                                  {'loss': 1.0276, 'learning_rate': 1.9202784583405386e-05, 'epoch': 0.16}
 16%|█▌        | 68/436 [17:01<1:36:08, 15.68s/it] 16%|█▌        | 69/436 [17:12<1:27:51, 14.36s/it]                                                  {'loss': 0.9109, 'learning_rate': 1.9173402060572028e-05, 'epoch': 0.16}
 16%|█▌        | 69/436 [17:12<1:27:51, 14.36s/it] 16%|█▌        | 70/436 [17:27<1:29:32, 14.68s/it]                                                  {'loss': 0.7656, 'learning_rate': 1.9143511140402532e-05, 'epoch': 0.16}
 16%|█▌        | 70/436 [17:27<1:29:32, 14.68s/it] 16%|█▋        | 71/436 [17:38<1:21:57, 13.47s/it]                                                  {'loss': 0.6931, 'learning_rate': 1.9113113479475784e-05, 'epoch': 0.16}
 16%|█▋        | 71/436 [17:38<1:21:57, 13.47s/it] 17%|█▋        | 72/436 [17:53<1:24:35, 13.94s/it]                                                  {'loss': 1.2593, 'learning_rate': 1.908221076245466e-05, 'epoch': 0.16}
 17%|█▋        | 72/436 [17:53<1:24:35, 13.94s/it] 17%|█▋        | 73/436 [18:06<1:23:07, 13.74s/it]                                                  {'loss': 1.0483, 'learning_rate': 1.905080470199264e-05, 'epoch': 0.17}
 17%|█▋        | 73/436 [18:06<1:23:07, 13.74s/it] 17%|█▋        | 74/436 [18:22<1:25:56, 14.24s/it]                                                  {'loss': 0.7356, 'learning_rate': 1.901889703863891e-05, 'epoch': 0.17}
 17%|█▋        | 74/436 [18:22<1:25:56, 14.24s/it] 17%|█▋        | 75/436 [18:40<1:33:45, 15.58s/it]                                                  {'loss': 0.7603, 'learning_rate': 1.8986489540741895e-05, 'epoch': 0.17}
 17%|█▋        | 75/436 [18:40<1:33:45, 15.58s/it] 17%|█▋        | 76/436 [19:01<1:42:38, 17.11s/it]                                                  {'loss': 0.7058, 'learning_rate': 1.8953584004351243e-05, 'epoch': 0.17}
 17%|█▋        | 76/436 [19:01<1:42:38, 17.11s/it] 18%|█▊        | 77/436 [19:28<1:59:42, 20.01s/it]                                                  {'loss': 0.8809, 'learning_rate': 1.892018225311831e-05, 'epoch': 0.18}
 18%|█▊        | 77/436 [19:28<1:59:42, 20.01s/it] 18%|█▊        | 78/436 [19:40<1:44:55, 17.59s/it]                                                  {'loss': 0.6206, 'learning_rate': 1.8886286138195063e-05, 'epoch': 0.18}
 18%|█▊        | 78/436 [19:40<1:44:55, 17.59s/it] 18%|█▊        | 79/436 [19:52<1:35:00, 15.97s/it]                                                  {'loss': 0.7485, 'learning_rate': 1.885189753813152e-05, 'epoch': 0.18}
 18%|█▊        | 79/436 [19:52<1:35:00, 15.97s/it] 18%|█▊        | 80/436 [20:09<1:37:23, 16.41s/it]                                                  {'loss': 0.7793, 'learning_rate': 1.8817018358771612e-05, 'epoch': 0.18}
 18%|█▊        | 80/436 [20:09<1:37:23, 16.41s/it] 19%|█▊        | 81/436 [20:19<1:24:37, 14.30s/it]                                                  {'loss': 0.8615, 'learning_rate': 1.8781650533147572e-05, 'epoch': 0.19}
 19%|█▊        | 81/436 [20:19<1:24:37, 14.30s/it] 19%|█▉        | 82/436 [20:31<1:21:06, 13.75s/it]                                                  {'loss': 0.8933, 'learning_rate': 1.87457960213728e-05, 'epoch': 0.19}
 19%|█▉        | 82/436 [20:31<1:21:06, 13.75s/it] 19%|█▉        | 83/436 [20:51<1:31:40, 15.58s/it]                                                  {'loss': 0.495, 'learning_rate': 1.8709456810533248e-05, 'epoch': 0.19}
 19%|█▉        | 83/436 [20:51<1:31:40, 15.58s/it] 19%|█▉        | 84/436 [21:01<1:21:18, 13.86s/it]                                                  {'loss': 0.8066, 'learning_rate': 1.867263491457726e-05, 'epoch': 0.19}
 19%|█▉        | 84/436 [21:01<1:21:18, 13.86s/it] 19%|█▉        | 85/436 [21:14<1:20:15, 13.72s/it]                                                  {'loss': 1.0647, 'learning_rate': 1.8635332374203993e-05, 'epoch': 0.19}
 19%|█▉        | 85/436 [21:14<1:20:15, 13.72s/it] 20%|█▉        | 86/436 [21:31<1:25:07, 14.59s/it]                                                  {'loss': 0.835, 'learning_rate': 1.85975512567503e-05, 'epoch': 0.2}
 20%|█▉        | 86/436 [21:31<1:25:07, 14.59s/it] 20%|█▉        | 87/436 [21:41<1:16:55, 13.23s/it]                                                  {'loss': 1.0754, 'learning_rate': 1.8559293656076167e-05, 'epoch': 0.2}
 20%|█▉        | 87/436 [21:41<1:16:55, 13.23s/it] 20%|██        | 88/436 [22:00<1:26:25, 14.90s/it]                                                  {'loss': 0.9839, 'learning_rate': 1.8520561692448655e-05, 'epoch': 0.2}
 20%|██        | 88/436 [22:00<1:26:25, 14.90s/it] 20%|██        | 89/436 [22:19<1:33:09, 16.11s/it]                                                  {'loss': 1.0396, 'learning_rate': 1.848135751242441e-05, 'epoch': 0.2}
 20%|██        | 89/436 [22:19<1:33:09, 16.11s/it] 21%|██        | 90/436 [22:32<1:27:18, 15.14s/it]                                                  {'loss': 0.6722, 'learning_rate': 1.8441683288730686e-05, 'epoch': 0.21}
 21%|██        | 90/436 [22:32<1:27:18, 15.14s/it] 21%|██        | 91/436 [22:49<1:30:55, 15.81s/it]                                                  {'loss': 0.6647, 'learning_rate': 1.840154122014494e-05, 'epoch': 0.21}
 21%|██        | 91/436 [22:49<1:30:55, 15.81s/it] 21%|██        | 92/436 [22:59<1:21:20, 14.19s/it]                                                  {'loss': 0.8645, 'learning_rate': 1.836093353137297e-05, 'epoch': 0.21}
 21%|██        | 92/436 [22:59<1:21:20, 14.19s/it] 21%|██▏       | 93/436 [23:12<1:18:31, 13.74s/it]                                                  {'loss': 0.887, 'learning_rate': 1.831986247292561e-05, 'epoch': 0.21}
 21%|██▏       | 93/436 [23:12<1:18:31, 13.74s/it] 22%|██▏       | 94/436 [23:22<1:12:01, 12.64s/it]                                                  {'loss': 0.8518, 'learning_rate': 1.8278330320994035e-05, 'epoch': 0.22}
 22%|██▏       | 94/436 [23:22<1:12:01, 12.64s/it] 22%|██▏       | 95/436 [23:36<1:13:56, 13.01s/it]                                                  {'loss': 0.939, 'learning_rate': 1.823633937732357e-05, 'epoch': 0.22}
 22%|██▏       | 95/436 [23:36<1:13:56, 13.01s/it] 22%|██▏       | 96/436 [23:50<1:14:35, 13.16s/it]                                                  {'loss': 0.875, 'learning_rate': 1.8193891969086164e-05, 'epoch': 0.22}
 22%|██▏       | 96/436 [23:50<1:14:35, 13.16s/it] 22%|██▏       | 97/436 [24:02<1:12:24, 12.82s/it]                                                  {'loss': 0.6907, 'learning_rate': 1.8150990448751393e-05, 'epoch': 0.22}
 22%|██▏       | 97/436 [24:02<1:12:24, 12.82s/it] 22%|██▏       | 98/436 [24:14<1:11:37, 12.71s/it]                                                  {'loss': 1.1179, 'learning_rate': 1.8107637193956102e-05, 'epoch': 0.22}
 22%|██▏       | 98/436 [24:14<1:11:37, 12.71s/it] 23%|██▎       | 99/436 [24:26<1:10:29, 12.55s/it]                                                  {'loss': 0.8494, 'learning_rate': 1.8063834607372603e-05, 'epoch': 0.23}
 23%|██▎       | 99/436 [24:26<1:10:29, 12.55s/it] 23%|██▎       | 100/436 [24:45<1:20:54, 14.45s/it]                                                   {'loss': 0.7949, 'learning_rate': 1.8019585116575554e-05, 'epoch': 0.23}
 23%|██▎       | 100/436 [24:45<1:20:54, 14.45s/it] 23%|██▎       | 101/436 [24:56<1:14:31, 13.35s/it]                                                   {'loss': 0.7559, 'learning_rate': 1.7974891173907406e-05, 'epoch': 0.23}
 23%|██▎       | 101/436 [24:56<1:14:31, 13.35s/it] 23%|██▎       | 102/436 [25:11<1:18:04, 14.02s/it]                                                   {'loss': 1.0691, 'learning_rate': 1.792975525634248e-05, 'epoch': 0.23}
 23%|██▎       | 102/436 [25:11<1:18:04, 14.02s/it] 24%|██▎       | 103/436 [25:30<1:25:19, 15.37s/it]                                                   {'loss': 0.6826, 'learning_rate': 1.7884179865349713e-05, 'epoch': 0.24}
 24%|██▎       | 103/436 [25:30<1:25:19, 15.37s/it] 24%|██▍       | 104/436 [25:43<1:21:54, 14.80s/it]                                                   {'loss': 0.8638, 'learning_rate': 1.7838167526754002e-05, 'epoch': 0.24}
 24%|██▍       | 104/436 [25:43<1:21:54, 14.80s/it] 24%|██▍       | 105/436 [25:54<1:14:26, 13.49s/it]                                                   {'loss': 0.7057, 'learning_rate': 1.7791720790596242e-05, 'epoch': 0.24}
 24%|██▍       | 105/436 [25:54<1:14:26, 13.49s/it] 24%|██▍       | 106/436 [26:13<1:22:49, 15.06s/it]                                                   {'loss': 0.8499, 'learning_rate': 1.774484223099199e-05, 'epoch': 0.24}
 24%|██▍       | 106/436 [26:13<1:22:49, 15.06s/it] 25%|██▍       | 107/436 [26:27<1:20:48, 14.74s/it]                                                   {'loss': 1.0039, 'learning_rate': 1.7697534445988804e-05, 'epoch': 0.25}
 25%|██▍       | 107/436 [26:27<1:20:48, 14.74s/it] 25%|██▍       | 108/436 [26:47<1:29:16, 16.33s/it]                                                   {'loss': 0.9536, 'learning_rate': 1.7649800057422256e-05, 'epoch': 0.25}
 25%|██▍       | 108/436 [26:47<1:29:16, 16.33s/it] 25%|██▌       | 109/436 [26:57<1:19:38, 14.61s/it]                                                   {'loss': 1.0278, 'learning_rate': 1.760164171077064e-05, 'epoch': 0.25}
 25%|██▌       | 109/436 [26:57<1:19:38, 14.61s/it] 25%|██▌       | 110/436 [27:08<1:12:57, 13.43s/it]                                                   {'loss': 0.7041, 'learning_rate': 1.755306207500834e-05, 'epoch': 0.25}
 25%|██▌       | 110/436 [27:08<1:12:57, 13.43s/it] 25%|██▌       | 111/436 [27:24<1:17:30, 14.31s/it]                                                   {'loss': 0.6985, 'learning_rate': 1.750406384245793e-05, 'epoch': 0.25}
 25%|██▌       | 111/436 [27:24<1:17:30, 14.31s/it] 26%|██▌       | 112/436 [27:37<1:15:23, 13.96s/it]                                                   {'loss': 0.5391, 'learning_rate': 1.7454649728640944e-05, 'epoch': 0.26}
 26%|██▌       | 112/436 [27:37<1:15:23, 13.96s/it] 26%|██▌       | 113/436 [27:51<1:13:58, 13.74s/it]                                                   {'loss': 0.8152, 'learning_rate': 1.7404822472127406e-05, 'epoch': 0.26}
 26%|██▌       | 113/436 [27:51<1:13:58, 13.74s/it] 26%|██▌       | 114/436 [28:04<1:12:38, 13.54s/it]                                                   {'loss': 0.9673, 'learning_rate': 1.7354584834384036e-05, 'epoch': 0.26}
 26%|██▌       | 114/436 [28:04<1:12:38, 13.54s/it] 26%|██▋       | 115/436 [28:18<1:13:53, 13.81s/it]                                                   {'loss': 0.6787, 'learning_rate': 1.73039395996212e-05, 'epoch': 0.26}
 26%|██▋       | 115/436 [28:18<1:13:53, 13.81s/it] 27%|██▋       | 116/436 [28:29<1:09:11, 12.97s/it]                                                   {'loss': 0.5842, 'learning_rate': 1.725288957463864e-05, 'epoch': 0.27}
 27%|██▋       | 116/436 [28:29<1:09:11, 12.97s/it] 27%|██▋       | 117/436 [28:47<1:16:04, 14.31s/it]                                                   {'loss': 0.9854, 'learning_rate': 1.720143758866988e-05, 'epoch': 0.27}
 27%|██▋       | 117/436 [28:47<1:16:04, 14.31s/it] 27%|██▋       | 118/436 [29:04<1:20:07, 15.12s/it]                                                   {'loss': 0.866, 'learning_rate': 1.7149586493225453e-05, 'epoch': 0.27}
 27%|██▋       | 118/436 [29:04<1:20:07, 15.12s/it] 27%|██▋       | 119/436 [29:17<1:17:30, 14.67s/it]                                                   {'loss': 0.5566, 'learning_rate': 1.709733916193487e-05, 'epoch': 0.27}
 27%|██▋       | 119/436 [29:17<1:17:30, 14.67s/it] 28%|██▊       | 120/436 [29:26<1:07:46, 12.87s/it]                                                   {'loss': 0.8542, 'learning_rate': 1.704469849038734e-05, 'epoch': 0.27}
 28%|██▊       | 120/436 [29:26<1:07:46, 12.87s/it] 28%|██▊       | 121/436 [29:40<1:09:33, 13.25s/it]                                                   {'loss': 0.7107, 'learning_rate': 1.6991667395971306e-05, 'epoch': 0.28}
 28%|██▊       | 121/436 [29:40<1:09:33, 13.25s/it] 28%|██▊       | 122/436 [29:50<1:04:00, 12.23s/it]                                                   {'loss': 0.7698, 'learning_rate': 1.6938248817712767e-05, 'epoch': 0.28}
 28%|██▊       | 122/436 [29:50<1:04:00, 12.23s/it] 28%|██▊       | 123/436 [30:07<1:10:48, 13.57s/it]                                                   {'loss': 0.6005, 'learning_rate': 1.6884445716112388e-05, 'epoch': 0.28}
 28%|██▊       | 123/436 [30:07<1:10:48, 13.57s/it] 28%|██▊       | 124/436 [30:17<1:05:04, 12.51s/it]                                                   {'loss': 0.8774, 'learning_rate': 1.6830261072981423e-05, 'epoch': 0.28}
 28%|██▊       | 124/436 [30:17<1:05:04, 12.51s/it] 29%|██▊       | 125/436 [30:26<59:25, 11.46s/it]                                                   {'loss': 0.7894, 'learning_rate': 1.677569789127647e-05, 'epoch': 0.29}
 29%|██▊       | 125/436 [30:26<59:25, 11.46s/it] 29%|██▉       | 126/436 [30:51<1:21:01, 15.68s/it]                                                   {'loss': 0.8542, 'learning_rate': 1.6720759194933037e-05, 'epoch': 0.29}
 29%|██▉       | 126/436 [30:51<1:21:01, 15.68s/it] 29%|██▉       | 127/436 [31:02<1:13:49, 14.34s/it]                                                   {'loss': 0.72, 'learning_rate': 1.666544802869796e-05, 'epoch': 0.29}
 29%|██▉       | 127/436 [31:02<1:13:49, 14.34s/it] 29%|██▉       | 128/436 [31:13<1:08:19, 13.31s/it]                                                   {'loss': 0.7654, 'learning_rate': 1.660976745796065e-05, 'epoch': 0.29}
 29%|██▉       | 128/436 [31:13<1:08:19, 13.31s/it] 30%|██▉       | 129/436 [31:22<1:01:30, 12.02s/it]                                                   {'loss': 0.7089, 'learning_rate': 1.655372056858322e-05, 'epoch': 0.3}
 30%|██▉       | 129/436 [31:22<1:01:30, 12.02s/it] 30%|██▉       | 130/436 [31:37<1:04:50, 12.71s/it]                                                   {'loss': 0.9685, 'learning_rate': 1.6497310466729448e-05, 'epoch': 0.3}
 30%|██▉       | 130/436 [31:37<1:04:50, 12.71s/it] 30%|███       | 131/436 [32:04<1:26:25, 17.00s/it]                                                   {'loss': 0.9951, 'learning_rate': 1.6440540278692656e-05, 'epoch': 0.3}
 30%|███       | 131/436 [32:04<1:26:25, 17.00s/it] 30%|███       | 132/436 [32:14<1:16:09, 15.03s/it]                                                   {'loss': 0.8436, 'learning_rate': 1.6383413150722417e-05, 'epoch': 0.3}
 30%|███       | 132/436 [32:14<1:16:09, 15.03s/it] 31%|███       | 133/436 [32:23<1:06:52, 13.24s/it]                                                   {'loss': 0.8026, 'learning_rate': 1.6325932248850206e-05, 'epoch': 0.3}
 31%|███       | 133/436 [32:23<1:06:52, 13.24s/it] 31%|███       | 134/436 [32:40<1:12:37, 14.43s/it]                                                   {'loss': 0.9697, 'learning_rate': 1.626810075871394e-05, 'epoch': 0.31}
 31%|███       | 134/436 [32:40<1:12:37, 14.43s/it] 31%|███       | 135/436 [32:55<1:12:01, 14.36s/it]                                                   {'loss': 0.9849, 'learning_rate': 1.6209921885381418e-05, 'epoch': 0.31}
 31%|███       | 135/436 [32:55<1:12:01, 14.36s/it] 31%|███       | 136/436 [33:15<1:20:44, 16.15s/it]                                                   {'loss': 0.6792, 'learning_rate': 1.615139885317269e-05, 'epoch': 0.31}
 31%|███       | 136/436 [33:15<1:20:44, 16.15s/it] 31%|███▏      | 137/436 [33:28<1:16:22, 15.33s/it]                                                   {'loss': 0.8574, 'learning_rate': 1.6092534905481367e-05, 'epoch': 0.31}
 31%|███▏      | 137/436 [33:28<1:16:22, 15.33s/it] 32%|███▏      | 138/436 [33:39<1:09:04, 13.91s/it]                                                   {'loss': 0.6134, 'learning_rate': 1.6033333304594886e-05, 'epoch': 0.32}
 32%|███▏      | 138/436 [33:39<1:09:04, 13.91s/it] 32%|███▏      | 139/436 [33:54<1:11:04, 14.36s/it]                                                   {'loss': 0.6785, 'learning_rate': 1.5973797331513674e-05, 'epoch': 0.32}
 32%|███▏      | 139/436 [33:54<1:11:04, 14.36s/it] 32%|███▏      | 140/436 [34:14<1:18:55, 16.00s/it]                                                   {'loss': 0.6989, 'learning_rate': 1.5913930285769356e-05, 'epoch': 0.32}
 32%|███▏      | 140/436 [34:14<1:18:55, 16.00s/it] 32%|███▏      | 141/436 [34:38<1:30:05, 18.32s/it]                                                   {'loss': 0.7034, 'learning_rate': 1.5853735485241858e-05, 'epoch': 0.32}
 32%|███▏      | 141/436 [34:38<1:30:05, 18.32s/it] 33%|███▎      | 142/436 [34:48<1:17:57, 15.91s/it]                                                   {'loss': 0.9722, 'learning_rate': 1.579321626597554e-05, 'epoch': 0.33}
 33%|███▎      | 142/436 [34:48<1:17:57, 15.91s/it] 33%|███▎      | 143/436 [35:02<1:14:24, 15.24s/it]                                                   {'loss': 0.6418, 'learning_rate': 1.573237598199432e-05, 'epoch': 0.33}
 33%|███▎      | 143/436 [35:02<1:14:24, 15.24s/it] 33%|███▎      | 144/436 [35:25<1:26:15, 17.72s/it]                                                   {'loss': 0.6578, 'learning_rate': 1.5671218005115767e-05, 'epoch': 0.33}
 33%|███▎      | 144/436 [35:25<1:26:15, 17.72s/it] 33%|███▎      | 145/436 [35:35<1:14:41, 15.40s/it]                                                   {'loss': 0.5742, 'learning_rate': 1.5609745724764264e-05, 'epoch': 0.33}
 33%|███▎      | 145/436 [35:35<1:14:41, 15.40s/it] 33%|███▎      | 146/436 [36:00<1:28:09, 18.24s/it]                                                   {'loss': 0.7998, 'learning_rate': 1.5547962547783126e-05, 'epoch': 0.33}
 33%|███▎      | 146/436 [36:00<1:28:09, 18.24s/it] 34%|███▎      | 147/436 [36:16<1:24:19, 17.51s/it]                                                   {'loss': 0.6528, 'learning_rate': 1.5485871898245824e-05, 'epoch': 0.34}
 34%|███▎      | 147/436 [36:16<1:24:19, 17.51s/it] 34%|███▍      | 148/436 [36:35<1:26:29, 18.02s/it]                                                   {'loss': 0.6663, 'learning_rate': 1.54234772172662e-05, 'epoch': 0.34}
 34%|███▍      | 148/436 [36:35<1:26:29, 18.02s/it] 34%|███▍      | 149/436 [36:45<1:14:08, 15.50s/it]                                                   {'loss': 0.5322, 'learning_rate': 1.536078196280777e-05, 'epoch': 0.34}
 34%|███▍      | 149/436 [36:45<1:14:08, 15.50s/it] 34%|███▍      | 150/436 [37:01<1:14:28, 15.63s/it]                                                   {'loss': 0.7798, 'learning_rate': 1.5297789609492062e-05, 'epoch': 0.34}
 34%|███▍      | 150/436 [37:01<1:14:28, 15.63s/it] 35%|███▍      | 151/436 [37:16<1:14:18, 15.65s/it]                                                   {'loss': 0.8306, 'learning_rate': 1.5234503648406075e-05, 'epoch': 0.35}
 35%|███▍      | 151/436 [37:16<1:14:18, 15.65s/it] 35%|███▍      | 152/436 [37:27<1:06:53, 14.13s/it]                                                   {'loss': 0.5698, 'learning_rate': 1.5170927586908787e-05, 'epoch': 0.35}
 35%|███▍      | 152/436 [37:27<1:06:53, 14.13s/it] 35%|███▌      | 153/436 [37:42<1:08:27, 14.51s/it]                                                   {'loss': 0.7781, 'learning_rate': 1.5107064948436758e-05, 'epoch': 0.35}
 35%|███▌      | 153/436 [37:42<1:08:27, 14.51s/it] 35%|███▌      | 154/436 [37:55<1:05:39, 13.97s/it]                                                   {'loss': 0.5165, 'learning_rate': 1.5042919272308895e-05, 'epoch': 0.35}
 35%|███▌      | 154/436 [37:55<1:05:39, 13.97s/it] 36%|███▌      | 155/436 [38:05<1:00:12, 12.86s/it]                                                   {'loss': 0.6123, 'learning_rate': 1.4978494113530268e-05, 'epoch': 0.36}
 36%|███▌      | 155/436 [38:05<1:00:12, 12.86s/it] 36%|███▌      | 156/436 [38:22<1:04:37, 13.85s/it]                                                   {'loss': 0.6885, 'learning_rate': 1.4913793042595109e-05, 'epoch': 0.36}
 36%|███▌      | 156/436 [38:22<1:04:37, 13.85s/it] 36%|███▌      | 157/436 [38:33<1:01:24, 13.21s/it]                                                   {'loss': 0.7549, 'learning_rate': 1.4848819645288915e-05, 'epoch': 0.36}
 36%|███▌      | 157/436 [38:33<1:01:24, 13.21s/it] 36%|███▌      | 158/436 [38:48<1:03:51, 13.78s/it]                                                   {'loss': 0.5994, 'learning_rate': 1.4783577522489733e-05, 'epoch': 0.36}
 36%|███▌      | 158/436 [38:48<1:03:51, 13.78s/it] 36%|███▋      | 159/436 [39:03<1:04:58, 14.07s/it]                                                   {'loss': 0.6738, 'learning_rate': 1.4718070289968602e-05, 'epoch': 0.36}
 36%|███▋      | 159/436 [39:03<1:04:58, 14.07s/it] 37%|███▋      | 160/436 [39:13<58:53, 12.80s/it]                                                   {'loss': 0.6375, 'learning_rate': 1.4652301578189141e-05, 'epoch': 0.37}
 37%|███▋      | 160/436 [39:13<58:53, 12.80s/it] 37%|███▋      | 161/436 [39:28<1:02:19, 13.60s/it]                                                   {'loss': 0.4291, 'learning_rate': 1.4586275032106373e-05, 'epoch': 0.37}
 37%|███▋      | 161/436 [39:28<1:02:19, 13.60s/it] 37%|███▋      | 162/436 [39:43<1:03:17, 13.86s/it]                                                   {'loss': 0.6619, 'learning_rate': 1.4519994310964697e-05, 'epoch': 0.37}
 37%|███▋      | 162/436 [39:43<1:03:17, 13.86s/it] 37%|███▋      | 163/436 [39:55<1:01:02, 13.42s/it]                                                   {'loss': 0.7441, 'learning_rate': 1.4453463088095108e-05, 'epoch': 0.37}
 37%|███▋      | 163/436 [39:55<1:01:02, 13.42s/it] 38%|███▊      | 164/436 [40:17<1:12:07, 15.91s/it]                                                   {'loss': 0.809, 'learning_rate': 1.4386685050711593e-05, 'epoch': 0.38}
 38%|███▊      | 164/436 [40:17<1:12:07, 15.91s/it] 38%|███▊      | 165/436 [40:36<1:16:22, 16.91s/it]                                                   {'loss': 0.6653, 'learning_rate': 1.4319663899706818e-05, 'epoch': 0.38}
 38%|███▊      | 165/436 [40:36<1:16:22, 16.91s/it] 38%|███▊      | 166/436 [40:47<1:07:59, 15.11s/it]                                                   {'loss': 0.7998, 'learning_rate': 1.4252403349446986e-05, 'epoch': 0.38}
 38%|███▊      | 166/436 [40:47<1:07:59, 15.11s/it] 38%|███▊      | 167/436 [40:57<1:00:35, 13.51s/it]                                                   {'loss': 0.6394, 'learning_rate': 1.4184907127566006e-05, 'epoch': 0.38}
 38%|███▊      | 167/436 [40:57<1:00:35, 13.51s/it] 39%|███▊      | 168/436 [41:11<1:00:38, 13.58s/it]                                                   {'loss': 0.4349, 'learning_rate': 1.4117178974758903e-05, 'epoch': 0.38}
 39%|███▊      | 168/436 [41:11<1:00:38, 13.58s/it] 39%|███▉      | 169/436 [41:23<58:24, 13.13s/it]                                                   {'loss': 0.4048, 'learning_rate': 1.404922264457449e-05, 'epoch': 0.39}
 39%|███▉      | 169/436 [41:23<58:24, 13.13s/it] 39%|███▉      | 170/436 [41:38<1:00:33, 13.66s/it]                                                   {'loss': 0.7239, 'learning_rate': 1.3981041903207364e-05, 'epoch': 0.39}
 39%|███▉      | 170/436 [41:38<1:00:33, 13.66s/it] 39%|███▉      | 171/436 [41:51<1:00:24, 13.68s/it]                                                   {'loss': 0.9673, 'learning_rate': 1.3912640529289163e-05, 'epoch': 0.39}
 39%|███▉      | 171/436 [41:51<1:00:24, 13.68s/it] 39%|███▉      | 172/436 [42:02<56:31, 12.85s/it]                                                   {'loss': 0.6768, 'learning_rate': 1.3844022313679167e-05, 'epoch': 0.39}
 39%|███▉      | 172/436 [42:02<56:31, 12.85s/it] 40%|███▉      | 173/436 [42:14<55:25, 12.65s/it]                                                 {'loss': 0.8835, 'learning_rate': 1.3775191059254185e-05, 'epoch': 0.4}
 40%|███▉      | 173/436 [42:14<55:25, 12.65s/it] 40%|███▉      | 174/436 [42:31<1:00:09, 13.78s/it]                                                   {'loss': 0.9211, 'learning_rate': 1.3706150580697826e-05, 'epoch': 0.4}
 40%|███▉      | 174/436 [42:31<1:00:09, 13.78s/it] 40%|████      | 175/436 [42:41<54:50, 12.61s/it]                                                   {'loss': 0.7078, 'learning_rate': 1.3636904704289053e-05, 'epoch': 0.4}
 40%|████      | 175/436 [42:41<54:50, 12.61s/it] 40%|████      | 176/436 [42:54<56:04, 12.94s/it]                                                 {'loss': 0.582, 'learning_rate': 1.3567457267690152e-05, 'epoch': 0.4}
 40%|████      | 176/436 [42:54<56:04, 12.94s/it] 41%|████      | 177/436 [43:13<1:02:39, 14.51s/it]                                                   {'loss': 0.6735, 'learning_rate': 1.3497812119734037e-05, 'epoch': 0.41}
 41%|████      | 177/436 [43:13<1:02:39, 14.51s/it] 41%|████      | 178/436 [43:35<1:12:28, 16.86s/it]                                                   {'loss': 0.8591, 'learning_rate': 1.342797312021094e-05, 'epoch': 0.41}
 41%|████      | 178/436 [43:35<1:12:28, 16.86s/it] 41%|████      | 179/436 [43:53<1:14:17, 17.34s/it]                                                   {'loss': 0.6534, 'learning_rate': 1.3357944139654508e-05, 'epoch': 0.41}
 41%|████      | 179/436 [43:53<1:14:17, 17.34s/it] 41%|████▏     | 180/436 [44:04<1:05:05, 15.26s/it]                                                   {'loss': 0.6792, 'learning_rate': 1.3287729059127288e-05, 'epoch': 0.41}
 41%|████▏     | 180/436 [44:04<1:05:05, 15.26s/it] 42%|████▏     | 181/436 [44:17<1:01:34, 14.49s/it]                                                   {'loss': 0.5815, 'learning_rate': 1.3217331770005639e-05, 'epoch': 0.41}
 42%|████▏     | 181/436 [44:17<1:01:34, 14.49s/it] 42%|████▏     | 182/436 [44:26<54:33, 12.89s/it]                                                   {'loss': 0.4109, 'learning_rate': 1.3146756173764061e-05, 'epoch': 0.42}
 42%|████▏     | 182/436 [44:26<54:33, 12.89s/it] 42%|████▏     | 183/436 [44:39<55:15, 13.10s/it]                                                 {'loss': 0.5428, 'learning_rate': 1.3076006181758989e-05, 'epoch': 0.42}
 42%|████▏     | 183/436 [44:39<55:15, 13.10s/it] 42%|████▏     | 184/436 [44:52<54:20, 12.94s/it]                                                 {'loss': 0.7837, 'learning_rate': 1.3005085715012003e-05, 'epoch': 0.42}
 42%|████▏     | 184/436 [44:52<54:20, 12.94s/it] 42%|████▏     | 185/436 [45:03<51:21, 12.28s/it]                                                 {'loss': 0.6467, 'learning_rate': 1.2933998703992531e-05, 'epoch': 0.42}
 42%|████▏     | 185/436 [45:03<51:21, 12.28s/it] 43%|████▎     | 186/436 [45:17<53:12, 12.77s/it]                                                 {'loss': 0.7571, 'learning_rate': 1.2862749088400026e-05, 'epoch': 0.43}
 43%|████▎     | 186/436 [45:17<53:12, 12.77s/it] 43%|████▎     | 187/436 [45:30<53:49, 12.97s/it]                                                 {'loss': 0.9224, 'learning_rate': 1.279134081694561e-05, 'epoch': 0.43}
 43%|████▎     | 187/436 [45:30<53:49, 12.97s/it] 43%|████▎     | 188/436 [45:53<1:06:05, 15.99s/it]                                                   {'loss': 0.9795, 'learning_rate': 1.2719777847133241e-05, 'epoch': 0.43}
 43%|████▎     | 188/436 [45:53<1:06:05, 15.99s/it] 43%|████▎     | 189/436 [46:06<1:02:17, 15.13s/it]                                                   {'loss': 0.7732, 'learning_rate': 1.2648064145040392e-05, 'epoch': 0.43}
 43%|████▎     | 189/436 [46:06<1:02:17, 15.13s/it] 44%|████▎     | 190/436 [46:24<1:05:16, 15.92s/it]                                                   {'loss': 0.7974, 'learning_rate': 1.2576203685098233e-05, 'epoch': 0.44}
 44%|████▎     | 190/436 [46:24<1:05:16, 15.92s/it] 44%|████▍     | 191/436 [46:35<58:47, 14.40s/it]                                                   {'loss': 0.4324, 'learning_rate': 1.2504200449871378e-05, 'epoch': 0.44}
 44%|████▍     | 191/436 [46:35<58:47, 14.40s/it] 44%|████▍     | 192/436 [46:52<1:02:00, 15.25s/it]                                                   {'loss': 0.7041, 'learning_rate': 1.2432058429837153e-05, 'epoch': 0.44}
 44%|████▍     | 192/436 [46:52<1:02:00, 15.25s/it] 44%|████▍     | 193/436 [47:02<55:45, 13.77s/it]                                                   {'loss': 0.6316, 'learning_rate': 1.2359781623164465e-05, 'epoch': 0.44}
 44%|████▍     | 193/436 [47:02<55:45, 13.77s/it] 44%|████▍     | 194/436 [47:13<52:16, 12.96s/it]                                                 {'loss': 0.7854, 'learning_rate': 1.2287374035492184e-05, 'epoch': 0.44}
 44%|████▍     | 194/436 [47:13<52:16, 12.96s/it] 45%|████▍     | 195/436 [47:34<1:01:53, 15.41s/it]                                                   {'loss': 0.4517, 'learning_rate': 1.2214839679707193e-05, 'epoch': 0.45}
 45%|████▍     | 195/436 [47:34<1:01:53, 15.41s/it] 45%|████▍     | 196/436 [47:50<1:01:57, 15.49s/it]                                                   {'loss': 0.922, 'learning_rate': 1.2142182575721946e-05, 'epoch': 0.45}
 45%|████▍     | 196/436 [47:50<1:01:57, 15.49s/it] 45%|████▌     | 197/436 [48:02<56:53, 14.28s/it]                                                   {'loss': 0.6685, 'learning_rate': 1.2069406750251713e-05, 'epoch': 0.45}
 45%|████▌     | 197/436 [48:02<56:53, 14.28s/it] 45%|████▌     | 198/436 [48:12<52:35, 13.26s/it]                                                 {'loss': 0.5793, 'learning_rate': 1.1996516236591398e-05, 'epoch': 0.45}
 45%|████▌     | 198/436 [48:12<52:35, 13.26s/it] 46%|████▌     | 199/436 [48:28<55:25, 14.03s/it]                                                 {'loss': 0.7571, 'learning_rate': 1.1923515074392022e-05, 'epoch': 0.46}
 46%|████▌     | 199/436 [48:28<55:25, 14.03s/it] 46%|████▌     | 200/436 [48:42<54:36, 13.88s/it]                                                 {'loss': 0.7107, 'learning_rate': 1.1850407309436831e-05, 'epoch': 0.46}
 46%|████▌     | 200/436 [48:42<54:36, 13.88s/it]/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/scur0405/.conda/envs/llamavid/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 46%|████▌     | 201/436 [50:40<2:56:47, 45.14s/it]                                                   {'loss': 0.6047, 'learning_rate': 1.1777196993417087e-05, 'epoch': 0.46}
 46%|████▌     | 201/436 [50:40<2:56:47, 45.14s/it] 46%|████▋     | 202/436 [50:50<2:15:22, 34.71s/it]                                                   {'loss': 0.5658, 'learning_rate': 1.1703888183707513e-05, 'epoch': 0.46}
 46%|████▋     | 202/436 [50:50<2:15:22, 34.71s/it] 47%|████▋     | 203/436 [51:04<1:50:50, 28.54s/it]                                                   {'loss': 0.6143, 'learning_rate': 1.1630484943141428e-05, 'epoch': 0.47}
 47%|████▋     | 203/436 [51:04<1:50:50, 28.54s/it] 47%|████▋     | 204/436 [51:16<1:31:03, 23.55s/it]                                                   {'loss': 0.5654, 'learning_rate': 1.1556991339785595e-05, 'epoch': 0.47}
 47%|████▋     | 204/436 [51:16<1:31:03, 23.55s/it] 47%|████▋     | 205/436 [51:37<1:26:44, 22.53s/it]                                                   {'loss': 0.6906, 'learning_rate': 1.1483411446714744e-05, 'epoch': 0.47}
 47%|████▋     | 205/436 [51:37<1:26:44, 22.53s/it] 47%|████▋     | 206/436 [51:47<1:12:41, 18.96s/it]                                                   {'loss': 0.7917, 'learning_rate': 1.1409749341785859e-05, 'epoch': 0.47}
 47%|████▋     | 206/436 [51:47<1:12:41, 18.96s/it] 47%|████▋     | 207/436 [52:04<1:09:48, 18.29s/it]                                                   {'loss': 0.5808, 'learning_rate': 1.1336009107412162e-05, 'epoch': 0.47}
 47%|████▋     | 207/436 [52:04<1:09:48, 18.29s/it] 48%|████▊     | 208/436 [52:15<1:01:13, 16.11s/it]                                                   {'loss': 0.5812, 'learning_rate': 1.1262194830336888e-05, 'epoch': 0.48}
 48%|████▊     | 208/436 [52:15<1:01:13, 16.11s/it] 48%|████▊     | 209/436 [52:30<59:47, 15.80s/it]                                                   {'loss': 0.8318, 'learning_rate': 1.118831060140676e-05, 'epoch': 0.48}
 48%|████▊     | 209/436 [52:30<59:47, 15.80s/it] 48%|████▊     | 210/436 [52:46<1:00:14, 15.99s/it]                                                   {'loss': 0.5092, 'learning_rate': 1.1114360515345301e-05, 'epoch': 0.48}
 48%|████▊     | 210/436 [52:46<1:00:14, 15.99s/it] 48%|████▊     | 211/436 [52:58<54:51, 14.63s/it]                                                   {'loss': 0.9077, 'learning_rate': 1.1040348670525889e-05, 'epoch': 0.48}
 48%|████▊     | 211/436 [52:58<54:51, 14.63s/it] 49%|████▊     | 212/436 [53:13<55:26, 14.85s/it]                                                 {'loss': 0.6085, 'learning_rate': 1.096627916874461e-05, 'epoch': 0.49}
 49%|████▊     | 212/436 [53:13<55:26, 14.85s/it] 49%|████▉     | 213/436 [53:29<56:46, 15.28s/it]                                                 {'loss': 0.5602, 'learning_rate': 1.0892156114992963e-05, 'epoch': 0.49}
 49%|████▉     | 213/436 [53:30<56:46, 15.28s/it] 49%|████▉     | 214/436 [53:40<51:39, 13.96s/it]                                                 {'loss': 0.6526, 'learning_rate': 1.0817983617230326e-05, 'epoch': 0.49}
 49%|████▉     | 214/436 [53:40<51:39, 13.96s/it] 49%|████▉     | 215/436 [53:55<52:14, 14.18s/it]                                                 {'loss': 0.7219, 'learning_rate': 1.0743765786156313e-05, 'epoch': 0.49}
 49%|████▉     | 215/436 [53:55<52:14, 14.18s/it] 50%|████▉     | 216/436 [54:15<58:10, 15.86s/it]                                                 {'loss': 0.6714, 'learning_rate': 1.066950673498294e-05, 'epoch': 0.49}
 50%|████▉     | 216/436 [54:15<58:10, 15.86s/it] 50%|████▉     | 217/436 [54:25<51:41, 14.16s/it]                                                 {'loss': 0.6829, 'learning_rate': 1.0595210579206676e-05, 'epoch': 0.5}
 50%|████▉     | 217/436 [54:25<51:41, 14.16s/it] 50%|█████     | 218/436 [54:40<52:37, 14.49s/it]                                                 {'loss': 0.6321, 'learning_rate': 1.0520881436380366e-05, 'epoch': 0.5}
 50%|█████     | 218/436 [54:40<52:37, 14.49s/it] 50%|█████     | 219/436 [54:52<49:39, 13.73s/it]                                                 {'loss': 0.6331, 'learning_rate': 1.0446523425885008e-05, 'epoch': 0.5}
 50%|█████     | 219/436 [54:52<49:39, 13.73s/it] 50%|█████     | 220/436 [55:08<51:13, 14.23s/it]                                                 {'loss': 0.5916, 'learning_rate': 1.0372140668701483e-05, 'epoch': 0.5}
 50%|█████     | 220/436 [55:08<51:13, 14.23s/it] 51%|█████     | 221/436 [55:25<54:37, 15.24s/it]                                                 {'loss': 0.676, 'learning_rate': 1.0297737287182144e-05, 'epoch': 0.51}
 51%|█████     | 221/436 [55:25<54:37, 15.24s/it] 51%|█████     | 222/436 [55:35<48:46, 13.68s/it]                                                 {'loss': 0.4986, 'learning_rate': 1.022331740482237e-05, 'epoch': 0.51}
 51%|█████     | 222/436 [55:35<48:46, 13.68s/it] 51%|█████     | 223/436 [55:51<51:08, 14.40s/it]                                                 {'loss': 0.6562, 'learning_rate': 1.014888514603202e-05, 'epoch': 0.51}
 51%|█████     | 223/436 [55:51<51:08, 14.40s/it] 51%|█████▏    | 224/436 [56:09<54:22, 15.39s/it]                                                 {'loss': 0.6926, 'learning_rate': 1.0074444635906875e-05, 'epoch': 0.51}
 51%|█████▏    | 224/436 [56:09<54:22, 15.39s/it] 52%|█████▏    | 225/436 [56:21<50:18, 14.30s/it]                                                 {'loss': 0.787, 'learning_rate': 1e-05, 'epoch': 0.52}
 52%|█████▏    | 225/436 [56:21<50:18, 14.30s/it] 52%|█████▏    | 226/436 [56:36<51:16, 14.65s/it]                                                 {'loss': 0.4294, 'learning_rate': 9.92555536409313e-06, 'epoch': 0.52}
 52%|█████▏    | 226/436 [56:36<51:16, 14.65s/it] 52%|█████▏    | 227/436 [56:55<55:21, 15.89s/it]                                                 {'loss': 0.6578, 'learning_rate': 9.85111485396798e-06, 'epoch': 0.52}
 52%|█████▏    | 227/436 [56:55<55:21, 15.89s/it] 52%|█████▏    | 228/436 [57:06<50:03, 14.44s/it]                                                 {'loss': 0.5848, 'learning_rate': 9.776682595177633e-06, 'epoch': 0.52}
 52%|█████▏    | 228/436 [57:06<50:03, 14.44s/it] 53%|█████▎    | 229/436 [57:21<50:11, 14.55s/it]                                                 {'loss': 0.5387, 'learning_rate': 9.702262712817857e-06, 'epoch': 0.52}
 53%|█████▎    | 229/436 [57:21<50:11, 14.55s/it] 53%|█████▎    | 230/436 [57:40<54:18, 15.82s/it]                                                 {'loss': 0.6267, 'learning_rate': 9.627859331298522e-06, 'epoch': 0.53}
 53%|█████▎    | 230/436 [57:40<54:18, 15.82s/it] 53%|█████▎    | 231/436 [57:50<48:25, 14.17s/it]                                                 {'loss': 0.5939, 'learning_rate': 9.553476574114993e-06, 'epoch': 0.53}
 53%|█████▎    | 231/436 [57:50<48:25, 14.17s/it] 53%|█████▎    | 232/436 [58:06<49:37, 14.60s/it]                                                 {'loss': 0.6538, 'learning_rate': 9.479118563619638e-06, 'epoch': 0.53}
 53%|█████▎    | 232/436 [58:06<49:37, 14.60s/it] 53%|█████▎    | 233/436 [58:24<53:26, 15.80s/it]                                                 {'loss': 0.741, 'learning_rate': 9.404789420793327e-06, 'epoch': 0.53}
 53%|█████▎    | 233/436 [58:24<53:26, 15.80s/it] 54%|█████▎    | 234/436 [58:34<47:29, 14.10s/it]                                                 {'loss': 0.5243, 'learning_rate': 9.330493265017062e-06, 'epoch': 0.54}
 54%|█████▎    | 234/436 [58:34<47:29, 14.10s/it] 54%|█████▍    | 235/436 [58:51<49:54, 14.90s/it]                                                 {'loss': 0.4243, 'learning_rate': 9.25623421384369e-06, 'epoch': 0.54}
 54%|█████▍    | 235/436 [58:51<49:54, 14.90s/it] 54%|█████▍    | 236/436 [59:07<50:21, 15.11s/it]                                                 {'loss': 0.7073, 'learning_rate': 9.182016382769678e-06, 'epoch': 0.54}
 54%|█████▍    | 236/436 [59:07<50:21, 15.11s/it] 54%|█████▍    | 237/436 [59:25<52:59, 15.98s/it]                                                 {'loss': 0.6091, 'learning_rate': 9.107843885007042e-06, 'epoch': 0.54}
 54%|█████▍    | 237/436 [59:25<52:59, 15.98s/it] 55%|█████▍    | 238/436 [59:36<47:34, 14.42s/it]                                                 {'loss': 0.7599, 'learning_rate': 9.033720831255391e-06, 'epoch': 0.55}
 55%|█████▍    | 238/436 [59:36<47:34, 14.42s/it] 55%|█████▍    | 239/436 [1:00:00<56:54, 17.33s/it]                                                   {'loss': 0.5312, 'learning_rate': 8.959651329474115e-06, 'epoch': 0.55}
 55%|█████▍    | 239/436 [1:00:00<56:54, 17.33s/it] 55%|█████▌    | 240/436 [1:00:12<51:18, 15.70s/it]                                                   {'loss': 0.6226, 'learning_rate': 8.8856394846547e-06, 'epoch': 0.55}
 55%|█████▌    | 240/436 [1:00:12<51:18, 15.70s/it] 55%|█████▌    | 241/436 [1:00:23<46:25, 14.28s/it]                                                   {'loss': 0.5397, 'learning_rate': 8.811689398593245e-06, 'epoch': 0.55}
 55%|█████▌    | 241/436 [1:00:23<46:25, 14.28s/it] 56%|█████▌    | 242/436 [1:00:44<52:49, 16.34s/it]                                                   {'loss': 0.5227, 'learning_rate': 8.737805169663113e-06, 'epoch': 0.55}
 56%|█████▌    | 242/436 [1:00:44<52:49, 16.34s/it] 56%|█████▌    | 243/436 [1:00:58<50:57, 15.84s/it]                                                   {'loss': 0.6709, 'learning_rate': 8.663990892587839e-06, 'epoch': 0.56}
 56%|█████▌    | 243/436 [1:00:58<50:57, 15.84s/it] 56%|█████▌    | 244/436 [1:01:18<54:33, 17.05s/it]                                                   {'loss': 0.595, 'learning_rate': 8.590250658214148e-06, 'epoch': 0.56}
 56%|█████▌    | 244/436 [1:01:18<54:33, 17.05s/it] 56%|█████▌    | 245/436 [1:01:30<48:50, 15.34s/it]                                                   {'loss': 0.7476, 'learning_rate': 8.516588553285258e-06, 'epoch': 0.56}
 56%|█████▌    | 245/436 [1:01:30<48:50, 15.34s/it] 56%|█████▋    | 246/436 [1:01:43<46:54, 14.81s/it]                                                   {'loss': 0.613, 'learning_rate': 8.443008660214409e-06, 'epoch': 0.56}
 56%|█████▋    | 246/436 [1:01:43<46:54, 14.81s/it] 57%|█████▋    | 247/436 [1:01:59<47:55, 15.22s/it]                                                   {'loss': 0.7303, 'learning_rate': 8.369515056858575e-06, 'epoch': 0.57}
 57%|█████▋    | 247/436 [1:01:59<47:55, 15.22s/it] 57%|█████▋    | 248/436 [1:02:17<49:47, 15.89s/it]                                                   {'loss': 0.434, 'learning_rate': 8.296111816292494e-06, 'epoch': 0.57}
 57%|█████▋    | 248/436 [1:02:17<49:47, 15.89s/it] 57%|█████▋    | 249/436 [1:02:33<50:02, 16.06s/it]                                                   {'loss': 0.5317, 'learning_rate': 8.222803006582915e-06, 'epoch': 0.57}
 57%|█████▋    | 249/436 [1:02:33<50:02, 16.06s/it] 57%|█████▋    | 250/436 [1:02:44<44:59, 14.51s/it]                                                   {'loss': 0.5216, 'learning_rate': 8.149592690563172e-06, 'epoch': 0.57}
 57%|█████▋    | 250/436 [1:02:44<44:59, 14.51s/it] 58%|█████▊    | 251/436 [1:03:01<47:11, 15.30s/it]                                                   {'loss': 0.4604, 'learning_rate': 8.076484925607983e-06, 'epoch': 0.58}
 58%|█████▊    | 251/436 [1:03:01<47:11, 15.30s/it] 58%|█████▊    | 252/436 [1:03:18<47:48, 15.59s/it]                                                   {'loss': 0.6897, 'learning_rate': 8.003483763408604e-06, 'epoch': 0.58}
 58%|█████▊    | 252/436 [1:03:18<47:48, 15.59s/it] 58%|█████▊    | 253/436 [1:03:32<46:07, 15.12s/it]                                                   {'loss': 0.4905, 'learning_rate': 7.930593249748289e-06, 'epoch': 0.58}
 58%|█████▊    | 253/436 [1:03:32<46:07, 15.12s/it] 58%|█████▊    | 254/436 [1:03:42<41:23, 13.65s/it]                                                   {'loss': 0.5637, 'learning_rate': 7.857817424278056e-06, 'epoch': 0.58}
 58%|█████▊    | 254/436 [1:03:42<41:23, 13.65s/it] 58%|█████▊    | 255/436 [1:04:00<45:09, 14.97s/it]                                                   {'loss': 0.5285, 'learning_rate': 7.785160320292812e-06, 'epoch': 0.58}
 58%|█████▊    | 255/436 [1:04:00<45:09, 14.97s/it] 59%|█████▊    | 256/436 [1:04:09<39:42, 13.24s/it]                                                   {'loss': 0.6208, 'learning_rate': 7.712625964507818e-06, 'epoch': 0.59}
 59%|█████▊    | 256/436 [1:04:09<39:42, 13.24s/it] 59%|█████▉    | 257/436 [1:04:30<46:22, 15.54s/it]                                                   {'loss': 0.6465, 'learning_rate': 7.64021837683554e-06, 'epoch': 0.59}
 59%|█████▉    | 257/436 [1:04:30<46:22, 15.54s/it] 59%|█████▉    | 258/436 [1:04:40<41:32, 14.00s/it]                                                   {'loss': 0.7795, 'learning_rate': 7.567941570162849e-06, 'epoch': 0.59}
 59%|█████▉    | 258/436 [1:04:40<41:32, 14.00s/it] 59%|█████▉    | 259/436 [1:04:56<42:18, 14.34s/it]                                                   {'loss': 0.7583, 'learning_rate': 7.495799550128625e-06, 'epoch': 0.59}
 59%|█████▉    | 259/436 [1:04:56<42:18, 14.34s/it] 60%|█████▉    | 260/436 [1:05:14<45:29, 15.51s/it]                                                   {'loss': 0.4771, 'learning_rate': 7.423796314901769e-06, 'epoch': 0.6}
 60%|█████▉    | 260/436 [1:05:14<45:29, 15.51s/it] 60%|█████▉    | 261/436 [1:05:24<40:57, 14.04s/it]                                                   {'loss': 0.6937, 'learning_rate': 7.351935854959608e-06, 'epoch': 0.6}
 60%|█████▉    | 261/436 [1:05:24<40:57, 14.04s/it] 60%|██████    | 262/436 [1:05:44<45:31, 15.70s/it]                                                   {'loss': 0.7571, 'learning_rate': 7.2802221528667604e-06, 'epoch': 0.6}
 60%|██████    | 262/436 [1:05:44<45:31, 15.70s/it] 60%|██████    | 263/436 [1:05:57<42:49, 14.85s/it]                                                   {'loss': 0.8042, 'learning_rate': 7.208659183054393e-06, 'epoch': 0.6}
 60%|██████    | 263/436 [1:05:57<42:49, 14.85s/it] 61%|██████    | 264/436 [1:06:13<43:24, 15.14s/it]                                                   {'loss': 0.5179, 'learning_rate': 7.137250911599978e-06, 'epoch': 0.6}
 61%|██████    | 264/436 [1:06:13<43:24, 15.14s/it] 61%|██████    | 265/436 [1:06:23<39:16, 13.78s/it]                                                   {'loss': 0.7744, 'learning_rate': 7.066001296007469e-06, 'epoch': 0.61}
 61%|██████    | 265/436 [1:06:23<39:16, 13.78s/it] 61%|██████    | 266/436 [1:06:41<42:25, 14.98s/it]                                                   {'loss': 0.6875, 'learning_rate': 6.9949142849880015e-06, 'epoch': 0.61}
 61%|██████    | 266/436 [1:06:41<42:25, 14.98s/it] 61%|██████    | 267/436 [1:06:52<38:40, 13.73s/it]                                                   {'loss': 0.7112, 'learning_rate': 6.9239938182410126e-06, 'epoch': 0.61}
 61%|██████    | 267/436 [1:06:52<38:40, 13.73s/it] 61%|██████▏   | 268/436 [1:07:09<41:11, 14.71s/it]                                                   {'loss': 0.5079, 'learning_rate': 6.8532438262359404e-06, 'epoch': 0.61}
 61%|██████▏   | 268/436 [1:07:09<41:11, 14.71s/it] 62%|██████▏   | 269/436 [1:07:34<49:36, 17.83s/it]                                                   {'loss': 0.6653, 'learning_rate': 6.7826682299943635e-06, 'epoch': 0.62}
 62%|██████▏   | 269/436 [1:07:34<49:36, 17.83s/it] 62%|██████▏   | 270/436 [1:07:50<48:05, 17.39s/it]                                                   {'loss': 0.623, 'learning_rate': 6.712270940872713e-06, 'epoch': 0.62}
 62%|██████▏   | 270/436 [1:07:50<48:05, 17.39s/it] 62%|██████▏   | 271/436 [1:08:04<44:32, 16.20s/it]                                                   {'loss': 0.8313, 'learning_rate': 6.642055860345494e-06, 'epoch': 0.62}
 62%|██████▏   | 271/436 [1:08:04<44:32, 16.20s/it] 62%|██████▏   | 272/436 [1:08:20<44:34, 16.30s/it]                                                   {'loss': 0.7178, 'learning_rate': 6.572026879789064e-06, 'epoch': 0.62}
 62%|██████▏   | 272/436 [1:08:20<44:34, 16.30s/it] 63%|██████▎   | 273/436 [1:08:43<49:57, 18.39s/it]                                                   {'loss': 0.284, 'learning_rate': 6.502187880265969e-06, 'epoch': 0.63}
 63%|██████▎   | 273/436 [1:08:43<49:57, 18.39s/it] 63%|██████▎   | 274/436 [1:08:54<43:12, 16.00s/it]                                                   {'loss': 0.7708, 'learning_rate': 6.43254273230985e-06, 'epoch': 0.63}
 63%|██████▎   | 274/436 [1:08:54<43:12, 16.00s/it] 63%|██████▎   | 275/436 [1:09:14<46:03, 17.16s/it]                                                   {'loss': 0.5582, 'learning_rate': 6.36309529571095e-06, 'epoch': 0.63}
 63%|██████▎   | 275/436 [1:09:14<46:03, 17.16s/it] 63%|██████▎   | 276/436 [1:09:29<43:59, 16.50s/it]                                                   {'loss': 0.5205, 'learning_rate': 6.293849419302179e-06, 'epoch': 0.63}
 63%|██████▎   | 276/436 [1:09:29<43:59, 16.50s/it] 64%|██████▎   | 277/436 [1:09:40<39:56, 15.07s/it]                                                   {'loss': 0.5334, 'learning_rate': 6.224808940745814e-06, 'epoch': 0.63}
 64%|██████▎   | 277/436 [1:09:40<39:56, 15.07s/it] 64%|██████▍   | 278/436 [1:09:53<37:16, 14.16s/it]                                                   {'loss': 0.3602, 'learning_rate': 6.155977686320837e-06, 'epoch': 0.64}
 64%|██████▍   | 278/436 [1:09:53<37:16, 14.16s/it] 64%|██████▍   | 279/436 [1:10:01<32:39, 12.48s/it]                                                   {'loss': 0.5656, 'learning_rate': 6.087359470710841e-06, 'epoch': 0.64}
 64%|██████▍   | 279/436 [1:10:01<32:39, 12.48s/it] 64%|██████▍   | 280/436 [1:10:15<33:17, 12.81s/it]                                                   {'loss': 0.5065, 'learning_rate': 6.018958096792642e-06, 'epoch': 0.64}
 64%|██████▍   | 280/436 [1:10:15<33:17, 12.81s/it] 64%|██████▍   | 281/436 [1:10:35<39:00, 15.10s/it]                                                   {'loss': 0.5322, 'learning_rate': 5.950777355425511e-06, 'epoch': 0.64}
 64%|██████▍   | 281/436 [1:10:35<39:00, 15.10s/it] 65%|██████▍   | 282/436 [1:10:47<36:40, 14.29s/it]                                                   {'loss': 0.4357, 'learning_rate': 5.8828210252411e-06, 'epoch': 0.65}
 65%|██████▍   | 282/436 [1:10:48<36:40, 14.29s/it] 65%|██████▍   | 283/436 [1:10:57<32:29, 12.74s/it]                                                   {'loss': 0.4429, 'learning_rate': 5.815092872433994e-06, 'epoch': 0.65}
 65%|██████▍   | 283/436 [1:10:57<32:29, 12.74s/it] 65%|██████▌   | 284/436 [1:11:20<40:05, 15.83s/it]                                                   {'loss': 0.6968, 'learning_rate': 5.74759665055302e-06, 'epoch': 0.65}
 65%|██████▌   | 284/436 [1:11:20<40:05, 15.83s/it] 65%|██████▌   | 285/436 [1:11:30<35:48, 14.23s/it]                                                   {'loss': 0.652, 'learning_rate': 5.680336100293182e-06, 'epoch': 0.65}
 65%|██████▌   | 285/436 [1:11:30<35:48, 14.23s/it] 66%|██████▌   | 286/436 [1:11:40<32:36, 13.04s/it]                                                   {'loss': 0.7177, 'learning_rate': 5.613314949288409e-06, 'epoch': 0.66}
 66%|██████▌   | 286/436 [1:11:40<32:36, 13.04s/it] 66%|██████▌   | 287/436 [1:11:49<29:05, 11.71s/it]                                                   {'loss': 0.4415, 'learning_rate': 5.546536911904896e-06, 'epoch': 0.66}
 66%|██████▌   | 287/436 [1:11:49<29:05, 11.71s/it] 66%|██████▌   | 288/436 [1:12:06<32:47, 13.29s/it]                                                   {'loss': 0.4493, 'learning_rate': 5.4800056890353025e-06, 'epoch': 0.66}
 66%|██████▌   | 288/436 [1:12:06<32:47, 13.29s/it] 66%|██████▋   | 289/436 [1:12:17<31:02, 12.67s/it]                                                   {'loss': 0.4161, 'learning_rate': 5.4137249678936265e-06, 'epoch': 0.66}
 66%|██████▋   | 289/436 [1:12:17<31:02, 12.67s/it] 67%|██████▋   | 290/436 [1:12:27<28:21, 11.65s/it]                                                   {'loss': 0.8125, 'learning_rate': 5.347698421810861e-06, 'epoch': 0.66}
 67%|██████▋   | 290/436 [1:12:27<28:21, 11.65s/it] 67%|██████▋   | 291/436 [1:12:54<39:48, 16.47s/it]                                                   {'loss': 0.6042, 'learning_rate': 5.2819297100314e-06, 'epoch': 0.67}
 67%|██████▋   | 291/436 [1:12:54<39:48, 16.47s/it] 67%|██████▋   | 292/436 [1:13:10<38:50, 16.18s/it]                                                   {'loss': 0.5817, 'learning_rate': 5.216422477510267e-06, 'epoch': 0.67}
 67%|██████▋   | 292/436 [1:13:10<38:50, 16.18s/it] 67%|██████▋   | 293/436 [1:13:20<34:02, 14.28s/it]                                                   {'loss': 0.6022, 'learning_rate': 5.151180354711087e-06, 'epoch': 0.67}
 67%|██████▋   | 293/436 [1:13:20<34:02, 14.28s/it] 67%|██████▋   | 294/436 [1:13:34<33:44, 14.25s/it]                                                   {'loss': 0.6332, 'learning_rate': 5.0862069574048956e-06, 'epoch': 0.67}
 67%|██████▋   | 294/436 [1:13:34<33:44, 14.25s/it] 68%|██████▊   | 295/436 [1:13:51<35:26, 15.08s/it]                                                   {'loss': 0.4871, 'learning_rate': 5.021505886469733e-06, 'epoch': 0.68}
 68%|██████▊   | 295/436 [1:13:51<35:26, 15.08s/it] 68%|██████▊   | 296/436 [1:14:10<38:23, 16.46s/it]                                                   {'loss': 0.4856, 'learning_rate': 4.957080727691107e-06, 'epoch': 0.68}
 68%|██████▊   | 296/436 [1:14:10<38:23, 16.46s/it] 68%|██████▊   | 297/436 [1:14:21<33:51, 14.62s/it]                                                   {'loss': 0.7998, 'learning_rate': 4.892935051563243e-06, 'epoch': 0.68}
 68%|██████▊   | 297/436 [1:14:21<33:51, 14.62s/it] 68%|██████▊   | 298/436 [1:14:38<35:10, 15.29s/it]                                                   {'loss': 0.3613, 'learning_rate': 4.829072413091219e-06, 'epoch': 0.68}
 68%|██████▊   | 298/436 [1:14:38<35:10, 15.29s/it] 69%|██████▊   | 299/436 [1:14:57<37:44, 16.53s/it]                                                   {'loss': 0.7731, 'learning_rate': 4.765496351593927e-06, 'epoch': 0.68}
 69%|██████▊   | 299/436 [1:14:57<37:44, 16.53s/it] 69%|██████▉   | 300/436 [1:15:08<33:36, 14.83s/it]                                                   {'loss': 0.8674, 'learning_rate': 4.7022103905079405e-06, 'epoch': 0.69}
 69%|██████▉   | 300/436 [1:15:08<33:36, 14.83s/it] 69%|██████▉   | 301/436 [1:15:19<30:49, 13.70s/it]                                                   {'loss': 0.5535, 'learning_rate': 4.639218037192235e-06, 'epoch': 0.69}
 69%|██████▉   | 301/436 [1:15:19<30:49, 13.70s/it] 69%|██████▉   | 302/436 [1:15:33<31:00, 13.89s/it]                                                   {'loss': 0.5604, 'learning_rate': 4.576522782733802e-06, 'epoch': 0.69}
 69%|██████▉   | 302/436 [1:15:34<31:00, 13.89s/it] 69%|██████▉   | 303/436 [1:15:47<30:44, 13.87s/it]                                                   {'loss': 0.5197, 'learning_rate': 4.514128101754183e-06, 'epoch': 0.69}
 69%|██████▉   | 303/436 [1:15:47<30:44, 13.87s/it] 70%|██████▉   | 304/436 [1:15:58<28:50, 13.11s/it]                                                   {'loss': 0.5296, 'learning_rate': 4.45203745221688e-06, 'epoch': 0.7}
 70%|██████▉   | 304/436 [1:15:58<28:50, 13.11s/it] 70%|██████▉   | 305/436 [1:16:15<30:55, 14.16s/it]                                                   {'loss': 0.424, 'learning_rate': 4.3902542752357415e-06, 'epoch': 0.7}
 70%|██████▉   | 305/436 [1:16:15<30:55, 14.16s/it] 70%|███████   | 306/436 [1:16:28<29:37, 13.68s/it]                                                   {'loss': 0.6229, 'learning_rate': 4.3287819948842334e-06, 'epoch': 0.7}
 70%|███████   | 306/436 [1:16:28<29:37, 13.68s/it] 70%|███████   | 307/436 [1:16:39<27:53, 12.97s/it]                                                   {'loss': 0.8114, 'learning_rate': 4.267624018005686e-06, 'epoch': 0.7}
 70%|███████   | 307/436 [1:16:39<27:53, 12.97s/it] 71%|███████   | 308/436 [1:16:54<28:54, 13.55s/it]                                                   {'loss': 0.5831, 'learning_rate': 4.206783734024463e-06, 'epoch': 0.71}
 71%|███████   | 308/436 [1:16:54<28:54, 13.55s/it] 71%|███████   | 309/436 [1:17:05<26:58, 12.75s/it]                                                   {'loss': 0.6132, 'learning_rate': 4.1462645147581456e-06, 'epoch': 0.71}
 71%|███████   | 309/436 [1:17:05<26:58, 12.75s/it] 71%|███████   | 310/436 [1:17:23<30:09, 14.36s/it]                                                   {'loss': 0.8481, 'learning_rate': 4.086069714230646e-06, 'epoch': 0.71}
 71%|███████   | 310/436 [1:17:23<30:09, 14.36s/it] 71%|███████▏  | 311/436 [1:17:40<31:46, 15.25s/it]                                                   {'loss': 0.6624, 'learning_rate': 4.0262026684863295e-06, 'epoch': 0.71}
 71%|███████▏  | 311/436 [1:17:40<31:46, 15.25s/it] 72%|███████▏  | 312/436 [1:17:51<28:45, 13.91s/it]                                                   {'loss': 0.6189, 'learning_rate': 3.96666669540512e-06, 'epoch': 0.71}
 72%|███████▏  | 312/436 [1:17:51<28:45, 13.91s/it] 72%|███████▏  | 313/436 [1:18:02<26:44, 13.04s/it]                                                   {'loss': 0.6389, 'learning_rate': 3.907465094518636e-06, 'epoch': 0.72}
 72%|███████▏  | 313/436 [1:18:02<26:44, 13.04s/it] 72%|███████▏  | 314/436 [1:18:19<29:08, 14.33s/it]                                                   {'loss': 0.7551, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72}
 72%|███████▏  | 314/436 [1:18:19<29:08, 14.33s/it] 72%|███████▏  | 315/436 [1:18:33<28:17, 14.03s/it]                                                   {'loss': 0.3844, 'learning_rate': 3.790078114618586e-06, 'epoch': 0.72}
 72%|███████▏  | 315/436 [1:18:33<28:17, 14.03s/it] 72%|███████▏  | 316/436 [1:18:46<27:34, 13.79s/it]                                                   {'loss': 0.4878, 'learning_rate': 3.731899241286061e-06, 'epoch': 0.72}
 72%|███████▏  | 316/436 [1:18:46<27:34, 13.79s/it] 73%|███████▎  | 317/436 [1:18:57<25:49, 13.02s/it]                                                   {'loss': 0.7965, 'learning_rate': 3.6740677511497958e-06, 'epoch': 0.73}
 73%|███████▎  | 317/436 [1:18:57<25:49, 13.02s/it] 73%|███████▎  | 318/436 [1:19:07<23:58, 12.19s/it]                                                   {'loss': 0.3923, 'learning_rate': 3.616586849277587e-06, 'epoch': 0.73}
 73%|███████▎  | 318/436 [1:19:07<23:58, 12.19s/it] 73%|███████▎  | 319/436 [1:19:24<26:17, 13.48s/it]                                                   {'loss': 0.39, 'learning_rate': 3.559459721307349e-06, 'epoch': 0.73}
 73%|███████▎  | 319/436 [1:19:24<26:17, 13.48s/it] 73%|███████▎  | 320/436 [1:19:46<31:12, 16.14s/it]                                                   {'loss': 0.5173, 'learning_rate': 3.5026895332705504e-06, 'epoch': 0.73}
 73%|███████▎  | 320/436 [1:19:46<31:12, 16.14s/it] 74%|███████▎  | 321/436 [1:19:57<27:39, 14.43s/it]                                                   {'loss': 0.3848, 'learning_rate': 3.4462794314167846e-06, 'epoch': 0.74}
 74%|███████▎  | 321/436 [1:19:57<27:39, 14.43s/it] 74%|███████▍  | 322/436 [1:20:16<30:12, 15.90s/it]                                                   {'loss': 0.6017, 'learning_rate': 3.390232542039352e-06, 'epoch': 0.74}
 74%|███████▍  | 322/436 [1:20:16<30:12, 15.90s/it] 74%|███████▍  | 323/436 [1:20:32<30:14, 16.05s/it]                                                   {'loss': 0.4771, 'learning_rate': 3.3345519713020445e-06, 'epoch': 0.74}
 74%|███████▍  | 323/436 [1:20:32<30:14, 16.05s/it] 74%|███████▍  | 324/436 [1:20:43<26:51, 14.39s/it]                                                   {'loss': 0.5613, 'learning_rate': 3.2792408050669634e-06, 'epoch': 0.74}
 74%|███████▍  | 324/436 [1:20:43<26:51, 14.39s/it] 75%|███████▍  | 325/436 [1:20:54<24:35, 13.29s/it]                                                   {'loss': 0.6398, 'learning_rate': 3.2243021087235336e-06, 'epoch': 0.74}
 75%|███████▍  | 325/436 [1:20:54<24:35, 13.29s/it] 75%|███████▍  | 326/436 [1:21:17<29:39, 16.18s/it]                                                   {'loss': 0.4822, 'learning_rate': 3.16973892701858e-06, 'epoch': 0.75}
 75%|███████▍  | 326/436 [1:21:17<29:39, 16.18s/it] 75%|███████▌  | 327/436 [1:21:29<27:26, 15.10s/it]                                                   {'loss': 0.7108, 'learning_rate': 3.115554283887614e-06, 'epoch': 0.75}
 75%|███████▌  | 327/436 [1:21:29<27:26, 15.10s/it] 75%|███████▌  | 328/436 [1:21:38<24:04, 13.38s/it]                                                   {'loss': 0.544, 'learning_rate': 3.0617511822872337e-06, 'epoch': 0.75}
 75%|███████▌  | 328/436 [1:21:38<24:04, 13.38s/it] 75%|███████▌  | 329/436 [1:21:54<25:10, 14.12s/it]                                                   {'loss': 0.5236, 'learning_rate': 3.0083326040286977e-06, 'epoch': 0.75}
 75%|███████▌  | 329/436 [1:21:54<25:10, 14.12s/it] 76%|███████▌  | 330/436 [1:22:11<26:17, 14.88s/it]                                                   {'loss': 0.5519, 'learning_rate': 2.9553015096126638e-06, 'epoch': 0.76}
 76%|███████▌  | 330/436 [1:22:11<26:17, 14.88s/it] 76%|███████▌  | 331/436 [1:22:22<23:56, 13.68s/it]                                                   {'loss': 0.462, 'learning_rate': 2.902660838065131e-06, 'epoch': 0.76}
 76%|███████▌  | 331/436 [1:22:22<23:56, 13.68s/it] 76%|███████▌  | 332/436 [1:22:44<28:13, 16.28s/it]                                                   {'loss': 0.6068, 'learning_rate': 2.8504135067745463e-06, 'epoch': 0.76}
 76%|███████▌  | 332/436 [1:22:44<28:13, 16.28s/it] 76%|███████▋  | 333/436 [1:22:56<25:49, 15.04s/it]                                                   {'loss': 0.4744, 'learning_rate': 2.798562411330126e-06, 'epoch': 0.76}
 76%|███████▋  | 333/436 [1:22:56<25:49, 15.04s/it] 77%|███████▋  | 334/436 [1:23:21<30:40, 18.04s/it]                                                   {'loss': 0.702, 'learning_rate': 2.7471104253613645e-06, 'epoch': 0.77}
 77%|███████▋  | 334/436 [1:23:21<30:40, 18.04s/it] 77%|███████▋  | 335/436 [1:23:32<26:46, 15.91s/it]                                                   {'loss': 0.4047, 'learning_rate': 2.6960604003788014e-06, 'epoch': 0.77}
 77%|███████▋  | 335/436 [1:23:32<26:46, 15.91s/it] 77%|███████▋  | 336/436 [1:23:45<24:42, 14.82s/it]                                                   {'loss': 0.832, 'learning_rate': 2.6454151656159666e-06, 'epoch': 0.77}
 77%|███████▋  | 336/436 [1:23:45<24:42, 14.82s/it] 77%|███████▋  | 337/436 [1:23:57<23:16, 14.11s/it]                                                   {'loss': 0.3765, 'learning_rate': 2.5951775278725956e-06, 'epoch': 0.77}
 77%|███████▋  | 337/436 [1:23:57<23:16, 14.11s/it] 78%|███████▊  | 338/436 [1:24:16<25:20, 15.52s/it]                                                   {'loss': 0.5181, 'learning_rate': 2.545350271359055e-06, 'epoch': 0.77}
 78%|███████▊  | 338/436 [1:24:16<25:20, 15.52s/it]slurmstepd: error: *** JOB 6344696 ON gcn72 CANCELLED AT 2024-05-23T05:50:40 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: open failed for /slurm/6344696/.ns: No such file or directory
slurmstepd: error: container_g_join(6344696): No such file or directory

JOB STATISTICS
==============
Job ID: 6344696
Cluster: snellius
User/Group: scur0405/scur0405
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 72
CPU Utilized: 00:00:02
CPU Efficiency: 0.00% of 4-12:04:48 core-walltime
Job Wall-clock time: 01:30:04
Memory Utilized: 245.92 GB
Memory Efficiency: 51.23% of 480.00 GB
