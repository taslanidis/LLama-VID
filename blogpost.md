# Seeking the limitations of "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"

### E.G. Lionis, A. Tragoudaras, T. Aslanidis, V. Karlis, O. Neut

---

In this blog post, we discuss, reproduce, and extend on the findings of the paper titled ["LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"](https://arxiv.org/abs/2311.17043).

## Introduction:


### VLMs LLaMA-VID section:

With the rise in popularity of transformer based LLMs e.g. Llama-3, GPT4... the performance for visual language models increase in similar fashion. VLMs are a class of deep learning models that combine both visual and natural language aspects. Common tasks of these models consist of image captioning and visual question-answering. The existing multimodal models in this space already prove to be quite good (Instructblip, GPT4).  However, when considering video, these models are required extrapolate from these single image reasoning capabilities to reason about sequences of frames. Consequently, making it harder for these models to run inference since the input is much bigger. Current VLMs represent an image with sizes from 32 tokens in BLIP up to 256 tokens in LLaVa. The LLaMA-VID model proposed by Li et al. [2023] eliminates the issue posed by scale of videos by representing an image with solely 2 tokens. Firstly, a context token that represents the image based on user input. Secondly, the content token represents the visual cues in the image. This 2 token approach shows to be a more efficient approach while beating most of the video and image based benchmarks.

### LLaMA-VID architecture:
The LLaMA-VID framework consists of many steps, all of which can be boiled down by the figure (ref fig). To generate the context and content tokens, the model processes both the sequence of videos and the query provided by the user. Firstly, every image of the video sequence is passed to a vision transformer (ViT). Since this is a pretrained model and serves as a backbone to the LLaMA-VID model, it is frozen during training. The visual embeddings generated by the vision transformer, are used in 2 parts. Firstly, the text decoder generates a text-guided query using the visual embeddings and the query given by the user. Subsequently, the context attention module generates a context token using the visual embeddings and the text-guided query. Finally, both the content and context tokens are projected down using an MLP. The tokens are interpreted by the LLM alongside the initial user query to anser the question with the provided context and content tokens.

### LLaMA-VID training (Antonis):

### Related work section:
Vision-language models are the product of advancements in both computer vision and natural language processing. The purpose of these models is to align both the natural language space and the vision space to enable cross modality understanding and reasoning. Key advancements in this field have been marked by the inception of large scale models like CLIP and ALIGN (cite CLIP, ALIGN). These models have been trained using a large scale datasets of image-text pairs. After these models, the work in transformer based language models like GPT and Llama have known great advancements [Radford et al., 2021]\[Jia et al., 2021]. The autoregressive nature of their training pipeline enables them to train on vast amounts of data. Pushing the boundaries of LLMs even further has been accomplished by models like Vicuna, that perform fine tuning on Llama with instruction pairs [Chiang et al., 2023]. Considering these leaps in LLMs, the performance of VLMs can be evidently increased by leveraging the LLMs power. This opportunity made space for Flamingo and BLIP-2, that utilize both web-scale image-text pairs for their cross modality alignment and LLMs. More modern versions have been proposed such as 
LLaVA is a multi modal model that uses an MLP to project the encodings to the space of Llama.

Alltough the VLMs are performing well on images, videos pose a new problem for multi modality alignment. Videos contain many frames, and models like LLaVA or BLIP-2 require a considerable amount of tokens to represent them. Several popular frameworks have tackled this issue and proposed unique solutions which consist of Video-LLaMA, VideoChat, Video-ChatGPT and Valley. However, these VLMs still face difficulties for long videos, since the amount of tokens explodes. The current implementations are capped by a video length of 1 hour, making the use of long videos like movies not possible. This is where the 2-token approach from LLaMA-VID enters the stage.

## Reproducibility: LLaMA-VID

For the reproducibility of LLaMA-VID, we had to run inference for several benchmarks that consisted of image-only and video benchmarks. We attempted to reproduce the results reported in the original LLaMA-VID paper and achieved similar outcomes. This successful replication strengthens the validity of the original findings and provides a solid foundation for our further investigation into the model's ability to capture detailed information in complex video frames.


### Image Only 
| LLM | Res. | Model | GQA | MMB | POPE | SQA-Image | VizWiz | VQA v2 |
|----------|----------|-----------|---|---|---|---|---|---| 
Vicuna-7B | 336 | [ckpt](https://huggingface.co/YanweiLi/llama-vid-7b-full-336) | 64.38 | 65.63 | 86.1 | 68.22 | 55.18 | 79.25 | 


### Video
| LLM | Res. | Model | MSVD-QA \w [Llama3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) as QA expert|
|----------|----------|-----------|---|
Vicuna-7B | 224 | [ckpt](https://huggingface.co/YanweiLi/llama-vid-7b-full-224-video-fps-1) | 59.17 |

## Exposition:
In recent advancements of vision-language models, LLaMA-VID represents a significant leap forward by efficiently encoding video frames using only two tokens: a context token and a content token. This innovative approach aims to balance the trade-off between computational efficiency and model performance, particularly for long video sequences. However, this token reduction strategy raises crucial questions about the model's expressiveness, especially in capturing fine-grained details within video frames.

Our primary motivation stems from the trade-off between model efficiency and the amount of tokens used to represent frames. By compressing each frame into just two tokens, LLaMA-VID reduces the available representational capacity per frame, potentially limiting the model's ability to capture intricate details. Detailed visual scenes, rich with subtle movements, small objects, and complex textures, often require a larger representation to preserve and convey the nuanced information effectively.

To rigorously test our hypothesis, we propose evaluating LLaMA-VID on two new datasets specifically designed to assess its ability to capture fine-grained details:

- [Crime Dataset](https://paperswithcode.com/dataset/ucf-crime): This dataset contains security camera footage to detect whether a crime is taking place. The task demands the model to identify subtle and often rapidly occurring actions indicative of criminal activity.
- [Animal Dataset](https://sutdcv.github.io/Animal-Kingdom/): This dataset consists of videos featuring various animals, requiring the model to accurately detect and classify the animals present. This task necessitates capturing detailed visual features unique to each species.

Furthermore, we will experiment with fine-tuning LLaMA-VID on the crime dataset to evaluate whether model performance improves with additional task-specific training. Fine-tuning aims to enhance the model's ability to capture task specific details by updating its weights more closely to the characteristics of the dataset.

weakness:
- not be able to capture details either by big objects like how many animals are in a picture or small objects like clothes because they utilize only 1 content token thus lossing the ability to capture details.

strength:
- that it can handle (long) videos efficently. 

-----
Exposition of its weaknesses/strengths/potential which triggered your group to come up with a response.


## Contribution:

Describe your novel contribution.


## Results:

Results of your work (link that part with the code in the jupyter notebook)

## Conclusion:

## Author's contributions:

Close the notebook with a description of each student's contribution.

## Bibliography:

- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gon- zalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chat- bot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision- language representation learning with noisy text supervision. In International conference on machine learning, pages 4904–4916. PMLR, 2021.
- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730–19742. PMLR, 2023a.
- KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understand- ing. arXiv preprint arXiv:2305.06355, 2023b.
- Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023c.
- Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.
- Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.
- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language su- pervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.
- Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction- tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023.
