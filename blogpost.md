# Seeking the limitations of "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"

### A. Tragoudaras, E.G. Lionis, O. Neut, T. Aslanidis, V. Karlis.

---

In this blog post, we discuss, reproduce, and extend the findings of the paper titled ["LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"](https://arxiv.org/abs/2311.17043). The authors introduce a Visual Language Model that aims to tackle the computational issue that arises with long video understanding, and more specifically, the excess of visual tokens needed for such videos.

The purpose of this blog post is:
-	To help researchers understand LLaMA-VID
-	To reproduce the results provided by the authors
-	Check the model’s performance in different datasets both in zero-shot and in fine-tuning context

## VLMs & LLaMA-VID:

With the rise in popularity of transformer-based LLMs [Touvron et al., 2023, Achiam et al., 2023],  the performance gain for visual language models followed similarly. VLMs are a class of deep learning models that combine both visual and natural language aspects. Common tasks of these models consist of image captioning and visual question-answering. The existing multimodal models in this space already prove to be quite good (Instructblip, GPT4).  However, when considering video, these models are required to extrapolate from these single-image reasoning capabilities to reason about sequences of frames. Consequently, making it harder for these models to run inference since the input is much bigger. Current VLMs represent an image with sizes from 32 tokens in BLIP up to 256 tokens in LLaVa. The LLaMA-VID model proposed by Li et al. [2023] eliminates the issue posed by the scale of videos by representing an image with solely 2 tokens. Firstly, a context token that represents the image based on user input. Secondly, the content token represents the visual cues in the image. This 2 token approach has shown to be a more efficient approach while beating most of the video and image-based benchmarks.

## Related work section:
Vision-language models are the product of advancements in both computer vision and natural language processing. The purpose of these models is to align both the natural language space and the vision space to enable cross-modality understanding and reasoning. Key advancements in this field have been marked by the inception of large-scale models like CLIP and ALIGN (cite CLIP, ALIGN). These models have been trained using large-scale datasets of image-text pairs. After these models, the work in transformer-based language models like GPT and Llama has known great advancements [Radford et al., 2021]\[Jia et al., 2021]. The autoregressive nature of their training pipeline enables them to train on vast amounts of data. Pushing the boundaries of LLMs even further has been accomplished by models like Vicuna, which perform fine-tuning on Llama with instruction pairs [Chiang et al., 2023]. Considering these leaps in LLMs, the performance of VLMs can be evidently increased by leveraging the LLMs power. This opportunity made space for Flamingo and BLIP-2, which utilize both web-scale image-text pairs for their cross-modality alignment and LLMs. More modern versions have been proposed such as 
LLaVA is a multi-modal model that uses an MLP to project the encodings to the space of Llama.

Although the VLMs are performing well on images, videos pose a new problem for multi-modality alignment. Videos contain many frames, and models like LLaVA or BLIP-2 require a considerable amount of tokens to represent them. Several popular frameworks have tackled this issue and proposed unique solutions which consist of Video-LLaMA, VideoChat, Video-ChatGPT, and Valley. However, these VLMs still face difficulties for long videos, since the amount of tokens explodes. The current implementations are capped by a video length of 1 hour, making the use of long videos like movies not possible. This is where the 2-token approach from LLaMA-VID enters the stage.

## LLaMA-VID architecture:
The LLaMA-VID framework consists of many steps, all of which can be boiled down in Figure 1. To generate the context and content tokens, the model processes both the sequence of videos and the query provided by the user. Firstly, every image of the video sequence is passed to a vision transformer (ViT). Since this is a pre-trained model and serves as a backbone to the LLaMA-VID model, it is frozen during training. The visual embeddings generated by the vision transformer, are used in 2 parts. Firstly, the text decoder generates a text-guided query using the visual embeddings and the query given by the user. Subsequently, the context attention module generates a context token using the visual embeddings and the text-guided query. Finally, both the content and context tokens are projected down using an MLP. The tokens are interpreted by the LLM alongside the initial user query to answer the question with the provided context and content tokens.

<table align="center"> <tr align="center"> <td><img src="figures/architecture.png" width=800></td> </tr> <tr align="center"> <td colspan=2><b>Figure 1.</b> LLaMa-VID overall architecture.</td> </tr> </table>

## Vision and Text Encoder:
To prepare the model for achieving cross-modality alignment, the frames of the video need to be encoded. The initial size of the frames is $\boldsymbol V_t \in \mathbb{R}^{H \times W \times 3}$. Then a transformer-based visual encoder produces the visual embedding $\boldsymbol X_t \in \mathbb{N \times C}$ where the dimensions $N$ and $C$ denote the number of image patches and embedding channels respectively. Finally, the visual embedding $\boldsymbol X_t$ is processed by the text decoder (either a QFormer or a BERT model) in combination with the user query to generate the text-guided query $Q_t \in \mathbb{R}^{M \times C}$. Note, that $M$ denotes the number of queries.

​​## Token generation:
Understanding the generation of both context and content tokens is intrinsic to understanding the LLaMA-VID model. These tokens form the input to the LLM at the end of the inference pipeline. The visual embedding produced by the visual encoder $\boldsymbol X_t$ and the text query $\boldsymbol Q_t$ are used to generate the tokens. At first, the context attention module in the LLaMA-VID pipeline is used to aggregate text-related visual features into the context token $\boldsymbol E_t$.

$$ \boldsymbol{E}_t = \text{Mean}(\text{Softmax}(\boldsymbol{Q}_t \times \boldsymbol{X}_t^T) \times \boldsymbol{X}_t) $$

This approach succeeds in capturing the most crucial visual cues in the context token with the help of the text query. Secondly, the content token reaches its final state by performing adaptive pooling on the visual embedding $\boldsymbol X_t$. This adaptive pooling allows the content token $\boldsymbol E_t^V \in \mathbb{R}^{n \times C} \text{where} n \in [1, N]$ (where $N$ is the number of image patches from the vision encoder) to be of varying sizes, depending on the video length.
Subsequently, a linear projector module transforms the $\boldsymbol E_t^T$ and $\boldsymbol E_t^V$ to align with the LLM space. This entire video sequence is translated into tokens that can be interpreted by the LLMs to generate an answer to the user query.




## LLaMA-VID training:
LLMs and VLMs require careful consideration during training, in order to scale. Instruction tuning is considered of greatest important for LLMs [Chiang et al., 2023b, Taori et al., 2023, Touvron et al., 2023] and VLMs [Dai et al., 2024, Liu et al., 2023, 2024].
The authors divide the training process in three steps. Each stage is described in the subsequent subsections.

### Modality Alignment
As a first step in the LLaMA_VID's architecture the video needs to be aligned within the same latent space as the LLMs, (see Figure 1). In this stage, each frame of the video is transformed into the space of LLMs during a forward pass. In the architecture both the visual encoder and the text decoder are kept frozen, with their original pre-trained weights. The model is trained on the LLaVA filtered dataset CC3M [Sharma et al., 2018] containing 558k image-captions pairs and the WebVid 2.5M [Bain et al., 2021] dataset consisting of 232k video-caption pairs respectively (see Figure 2).
instruction tuning.png

### Instruction Tuning
We mentioned before that this part of training is crucial for allowing LLMs to develop multi-modal comprehension. For the reason in this study the authors adopt the strategy of Video-ChatGPT [Maaz et al., 2023] and LLaVA [Liu et al. 2024] to build instruction pairs.The instruction pairs include different modalities (text, images, video) as depicted in Figure 2, while the input < prompt > answer < answer > prompts vary within the selected dataset. In the training procedure,  the image token < image > is appended at random either at the beginning or at the ending of the user input. Note that during this stage all models are allowed to update their parameters expect for the visual encoder, which is kept frozen.

### Long Video Tuning
To further harness the potential of LLaMA-CIdhour-long videos, it's creators build a dataset comprising 15,000 extended QA pairs, which includes 9,000 conversions from movie scenes and 6,000 samples derived from [Chen et al., 2023] for token expansion. Over 400 feature-length films and their corresponding scripts from [MovieNet Huang et al., 2020] were used to develop the training set. The core components of the instruction generation process are illustrated in Figure 3.

The resulting dataset encompasses QA pairs across three dimensions: video summary, movie plot, and detailed reasoning. For video summaries, extracted movie synopses are utilized along with the use of advanced LLMs such as GPT-4  to produce both brief and extensive  summaries for each film, resulting in approximately 1,000 summary-level instruction pairs. For plot-level data, the entire movie synopsis was used as input to GPT-4, which then generated plot-related and character-related QA pairs, including elements of plot comprehension, description, analysis, character relationships, personalities, and behaviors. Specifically, five plot-related pairs and five character-related pairs per movie were generated respectively, yielding a total of 4,000 plot-level QA pairs. For detail-level data, the full movie script is used as input into Claude-2 to generate five plot-related reasoning pairs and five detailed descriptions for each movie, culminating in another 4,000 pairs.

Utilizing these extended videos and the generated QA pairs, instruction tuning is carried out by concatenating visual and subtitle tokens for each frame, as depicted in Figure 2. This methodology allows LLaMA-VID to effectively manage 64,000 tokens, supporting over three hours of video input.

<table align="center">
  <tr align="center">
      <td><img src="figures/modal_align.png"></td>
  </tr>
  <tr align="center">
      <td><img src="figures/instruction tuning.png"></td>
  </tr>
  <tr align="center">
      <td><img src="figures/long_video_tune.png"></td>
  </tr>
    <td colspan=1><b>Figure 2.</b> < image -0> and < image-i > refer to the token for single image and the i-th video frame, respectively</td>
  </tr>
</table>

<table align="center">
  <tr align="center">
      <td><img src="figures/fig3.png" width=800></td>
  </tr>
  <tr align="center">
    <td colspan=2><b>Figure 3.</b> Here is an example of instruction pairs for the movie Titanic. Using the movie synopsis and script, we employ advanced LLMs such as GPT-4 and Claude-2 to generate comprehensive movie summaries, plot-related QA pairs, and general reasoning QA pairs.</td>
  </tr>
</table>

## Reproducibility: LLaMA-VID

For the reproducibility of LLaMA-VID, we had to run inference for several benchmarks that consisted of image-only and video benchmarks. We attempted to reproduce the results reported in the original LLaMA-VID paper and achieved similar outcomes. This successful replication strengthens the validity of the original findings and provides a solid foundation for our further investigation into the model's ability to capture detailed information in complex video frames.

### Image Only
| LLM | Res. | Model | GQA | MMB | POPE | SQA-Image | VizWiz | VQA v2 |
|----------|----------|-----------|---|---|---|---|---|---|
Vicuna-7B | 336 | [ckpt](https://huggingface.co/YanweiLi/llama-vid-7b-full-336) | 64.38 | 65.63 | 86.1 | 68.22 | 55.18 | 79.25 |

### Video
| LLM | Res. | Model | MSVD-QA \w [Llama3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) as QA expert|
|----------|----------|-----------|---|
Vicuna-7B | 224 | [ckpt](https://huggingface.co/YanweiLi/llama-vid-7b-full-224-video-fps-1) | 59.17 |


## Expected Limitations
In recent advancements of vision-language models, LLaMA-VID represents a significant leap forward by efficiently encoding video frames using only two tokens: a context token and a content token. This innovative approach aims to balance the trade-off between computational efficiency and model performance, particularly for long video sequences. However, this token reduction strategy raises crucial questions about the model's expressiveness, especially in capturing fine-grained details within video frames.

Our primary motivation stems from the trade-off between model efficiency and the amount of tokens used to represent frames. By compressing each frame into just two tokens, LLaMA-VID reduces the available representational capacity per frame, potentially limiting the model's ability to capture intricate details. Detailed visual scenes, rich with subtle movements, small objects, and complex textures, often require a larger representation to preserve and convey the nuanced information effectively.

We are skeptical about whether the model can capture various fine-graded details. One critical aspect is the model’s understanding of the real world, such as identifying animals or objects within a video. For example, what animals are in the video? Another detail involves more abstract comprehension, such as detecting actions or events on a video. For instance, can the model detect what action does the animal or is there a crime in this video? 

Additionally, we aim to evaluate not only the model’s theoretical capabilities but also the practical limits. We believe that there is no possible trade-off between accuracy and memory footage, meaning that users might not have to ability to choose between training on longer videos or achieving higher accuracy. This trade-off can lead to memory limitations, making it challenging to use this architecture in certain setups. Methods to optimize the trainability of LLMs such as Low-Rank Adaptation (LoRA) created by Hu et al. (2021), are not utilized here. For example, LORA freezes the pre-trained model weights and then adds some trainable rank decomposition matrices into every layer of the Transformer architecture.  Thus it significantly reduces the number of trainable parameters.

## Ablation studies:

To rigorously test our hypothesis, we propose evaluating LLaMA-VID on two new datasets specifically designed to assess its ability to capture fine-grained details. In particular, we have focused on video inputs and tested the provided model’s performance in question-answering tasks. In the provided paper, to evaluate these tasks the authors utilize 'gpt-3.5-turbo' to evaluate the predictions made by the LLaMA-VID model. Since 'gpt-3.5-turbo'  is a closed model and we did not have access to an API key, we substituted it with Meta-Llama-3-8B-Instruct, one of the newest open-source LLMs released by Meta that has text-generation capabilities.

### Crime Dataset: 
Surveillance cameras are capturing a plethora of videos of daily activities at specific places. Most of the videos captured contain realistic normal activities, but sometimes some anomalies may arise, such as illegal activities, a crime, or a traffic accident. These anomalous activities are identified through fine-grained information in the videos, such as the color of a traffic light for a specific moment, the item someone is holding in his hand, or an abrupt action such as a punch or a kick.

Thus, such a dataset containing various surveillance videos provides us with an excellent candidate to test the understanding and reasoning abilities of a Visual Language Model. For this, we will use the UCF Crime dataset by [Sultani et. al.2018]

The task for which we will perform inference demands the model to identify subtle and often rapidly occurring actions indicative of criminal activity. As we described, anomalous activities might span a very small amount of time in the video. Thus, we will do an ablation study to show if indeed the model can’t identify what happened in the video, irrespective of the amount of frames per second. Having only one frame per second is something that could easily be argued as the culprit of the model to loss of information on such activities.

In order to be able to use this dataset on our task, we had to create specific prompts with MCQ and answers being the type of anomalous activity observed in the video.

However, it can be quite vague what is an anomalous activity for our task, so we finally finetuned LLaMa-VID on the trainset of the UCF dataset. Then by performing inference again, we show how better the models become for such a task if they have been specifically trained to identify such specific anomalous activities.

### Animal Dataset:
Understanding animals’ behaviors is significant for a wide range of applications. Animal dataset is a large and diverse dataset, created by Xun et al. [2022]  that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. It contains a diverse range of animals with 850 species and includes wild animal footage from different times of the day in an extensive range of environments. The provided videos come with output labels of different types such as the different species captured in each video along with their respective animal classes and the different actions captured. This task necessitates capturing detailed visual features unique to each species. 

As this dataset contains several subtasks we have focused on video data from the provided ‘action detection’ task and also videos provided for the ‘grounding’ task. In both cases, we manipulated output classes to perform inference and perform evaluation according to the LLaMA-VID pipeline. On top of that, we further manipulated the provided data so that we were also eligible to finetune our models by providing different types of conversational interactions. In particular, in both cases, we had to convert existing data to specific prompts with MCQ and answers being the species of animals along with their classes and the performed actions.

## Contribution:

This section is intentionally left blank, as we will fill them out when we have the results on our extensions. That way, it will be easier to introduce them and motivate them.

## Results:
Will be filled when we have them available.

## Conclusion:

## Author's contributions:

- Antonis: Initial setup of the codebase, built the environment in the cluster, executed the inferences for image and video benchmarks. Created scripts for evaluating the zero-shot performance of LLaMA-VId on Crime and animal-kingdom dataset respectively. Further fine-tuning LLaMA-VID on these two new datasets.
- Vasilis: Literature review to spot weaknesses of LLaMA-VID, ablation study to explore details in videos, investigation and selection of animal-kingdom dataset. Pre-processing of the dataset, data-pipeline set-up and implementation of the respective inference and fine-tuning tasks.
- Fanis: Identification of anomaly detection dataset. Ablation study to explore missing detail information. Data pipeline setup for a new dataset. Finetuning on a new dataset. Reviewing literature and related work for potential short-comings of the paper, and VLMs in general.
- Emmanouil: Helping in the initial setup of the codebase and the execution of inferences for images and video benchmarks. Understanding how the fine-tuning is executable and how to set it up. Helping preprocess the dataset and setup the fine-tuning execution. 
- Oliver: Helping with the reproducibility, initial spike regarding zero-shot evaluation on new dataset. Writing blogpost. 



## Bibliography:

- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gon- zalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chat- bot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision- language representation learning with noisy text supervision. In International conference on machine learning, pages 4904–4916. PMLR, 2021.
- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730–19742. PMLR, 2023a.
- KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understand- ing. arXiv preprint arXiv:2305.06355, 2023b.
- Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023c.
- Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.
- Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.
- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language su- pervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.
- Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction- tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023.
- Ng, Xun Long, et al. "Animal kingdom: A large and diverse dataset for animal behavior understanding." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ́ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models (2023). arXiv preprint arXiv:2302.13971, 2023.
- Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.
- Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." arXiv preprint arXiv:2106.09685 (2021).
- Sultani, Waqas, Chen Chen, and Mubarak Shah. "Real-world anomaly detection in surveillance videos." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
